{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_model_Resnet_GradCam",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikmohan1702/EIP_4/blob/master/Final_model_Resnet_GradCam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O8yxNF51ji4",
        "colab_type": "code",
        "outputId": "40762f98-8db5-48a6-aad5-16609354f7a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten,Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 50\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 2\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 40:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 20:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 15:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True,\n",
        "              callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Learning rate:  0.001\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   272         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   1088        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   1040        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1088        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   1040        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 64)   1088        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 64)   0           add_2[0][0]                      \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   4160        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           conv2d_15[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 64)   8256        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 128)  8320        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 64)   8256        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 128)  8320        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 128)  0           add_5[0][0]                      \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 128)    16512       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 128)    147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 128)    512         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 128)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 256)    33024       add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 256)    0           conv2d_25[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 256)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 128)    32896       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 128)    147584      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 128)    512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 128)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 256)    33024       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 8, 256)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 128)    32896       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 128)    147584      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 256)    33024       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 256)    0           add_8[0][0]                      \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 256)    1024        add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 256)    0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8, 8, 256)    0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "ResNet29v2\n",
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 1.8699 - acc: 0.4821 - val_loss: 1.7018 - val_acc: 0.5294\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.52940, saving model to /content/saved_models/cifar10_ResNet29v2_model.001.h5\n",
            "Epoch 2/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 1.3871 - acc: 0.6216 - val_loss: 1.3828 - val_acc: 0.6173\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.52940 to 0.61730, saving model to /content/saved_models/cifar10_ResNet29v2_model.002.h5\n",
            "Epoch 3/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 1.2074 - acc: 0.6766 - val_loss: 1.1434 - val_acc: 0.7053\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.61730 to 0.70530, saving model to /content/saved_models/cifar10_ResNet29v2_model.003.h5\n",
            "Epoch 4/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 1.0815 - acc: 0.7170 - val_loss: 1.5282 - val_acc: 0.5880\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.70530\n",
            "Epoch 5/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 1.0063 - acc: 0.7429 - val_loss: 1.1986 - val_acc: 0.6922\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.70530\n",
            "Epoch 6/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.9499 - acc: 0.7608 - val_loss: 1.0617 - val_acc: 0.7282\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.70530 to 0.72820, saving model to /content/saved_models/cifar10_ResNet29v2_model.006.h5\n",
            "Epoch 7/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.9038 - acc: 0.7754 - val_loss: 1.0281 - val_acc: 0.7422\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.72820 to 0.74220, saving model to /content/saved_models/cifar10_ResNet29v2_model.007.h5\n",
            "Epoch 8/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.8700 - acc: 0.7867 - val_loss: 1.0210 - val_acc: 0.7498\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.74220 to 0.74980, saving model to /content/saved_models/cifar10_ResNet29v2_model.008.h5\n",
            "Epoch 9/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.8427 - acc: 0.7957 - val_loss: 1.0001 - val_acc: 0.7574\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.74980 to 0.75740, saving model to /content/saved_models/cifar10_ResNet29v2_model.009.h5\n",
            "Epoch 10/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.8145 - acc: 0.8021 - val_loss: 1.2042 - val_acc: 0.7118\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.75740\n",
            "Epoch 11/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7975 - acc: 0.8106 - val_loss: 1.1939 - val_acc: 0.7080\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.75740\n",
            "Epoch 12/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.7722 - acc: 0.8167 - val_loss: 1.2934 - val_acc: 0.6634\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.75740\n",
            "Epoch 13/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.7579 - acc: 0.8200 - val_loss: 0.8824 - val_acc: 0.7925\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.75740 to 0.79250, saving model to /content/saved_models/cifar10_ResNet29v2_model.013.h5\n",
            "Epoch 14/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.7427 - acc: 0.8260 - val_loss: 0.7642 - val_acc: 0.8177\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.79250 to 0.81770, saving model to /content/saved_models/cifar10_ResNet29v2_model.014.h5\n",
            "Epoch 15/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.7221 - acc: 0.8327 - val_loss: 0.8911 - val_acc: 0.7860\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.81770\n",
            "Epoch 16/50\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.7150 - acc: 0.8340 - val_loss: 0.8847 - val_acc: 0.7880\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.81770\n",
            "Epoch 17/50\n",
            "Learning rate:  0.0001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.5927 - acc: 0.8770 - val_loss: 0.6092 - val_acc: 0.8708\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.81770 to 0.87080, saving model to /content/saved_models/cifar10_ResNet29v2_model.017.h5\n",
            "Epoch 18/50\n",
            "Learning rate:  0.0001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.5465 - acc: 0.8887 - val_loss: 0.5820 - val_acc: 0.8772\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.87080 to 0.87720, saving model to /content/saved_models/cifar10_ResNet29v2_model.018.h5\n",
            "Epoch 19/50\n",
            "Learning rate:  0.0001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.5287 - acc: 0.8937 - val_loss: 0.5843 - val_acc: 0.8763\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.87720\n",
            "Epoch 20/50\n",
            "Learning rate:  0.0001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.5067 - acc: 0.8983 - val_loss: 0.5584 - val_acc: 0.8835\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.87720 to 0.88350, saving model to /content/saved_models/cifar10_ResNet29v2_model.020.h5\n",
            "Epoch 21/50\n",
            "Learning rate:  0.0001\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4898 - acc: 0.9032 - val_loss: 0.5633 - val_acc: 0.8804\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.88350\n",
            "Epoch 22/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 66s 42ms/step - loss: 0.4714 - acc: 0.9087 - val_loss: 0.5446 - val_acc: 0.8875\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.88350 to 0.88750, saving model to /content/saved_models/cifar10_ResNet29v2_model.022.h5\n",
            "Epoch 23/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4661 - acc: 0.9102 - val_loss: 0.5426 - val_acc: 0.8882\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.88750 to 0.88820, saving model to /content/saved_models/cifar10_ResNet29v2_model.023.h5\n",
            "Epoch 24/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4632 - acc: 0.9116 - val_loss: 0.5429 - val_acc: 0.8873\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.88820\n",
            "Epoch 25/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4627 - acc: 0.9112 - val_loss: 0.5383 - val_acc: 0.8890\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.88820 to 0.88900, saving model to /content/saved_models/cifar10_ResNet29v2_model.025.h5\n",
            "Epoch 26/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4587 - acc: 0.9133 - val_loss: 0.5369 - val_acc: 0.8885\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.88900\n",
            "Epoch 27/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.4605 - acc: 0.9118 - val_loss: 0.5356 - val_acc: 0.8887\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.88900\n",
            "Epoch 28/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.4531 - acc: 0.9137 - val_loss: 0.5347 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.88900 to 0.88980, saving model to /content/saved_models/cifar10_ResNet29v2_model.028.h5\n",
            "Epoch 29/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.4579 - acc: 0.9117 - val_loss: 0.5329 - val_acc: 0.8898\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.88980\n",
            "Epoch 30/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.4503 - acc: 0.9149 - val_loss: 0.5300 - val_acc: 0.8910\n",
            "\n",
            "Epoch 00030: val_acc improved from 0.88980 to 0.89100, saving model to /content/saved_models/cifar10_ResNet29v2_model.030.h5\n",
            "Epoch 31/50\n",
            "Learning rate:  1e-05\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.4509 - acc: 0.9138 - val_loss: 0.5340 - val_acc: 0.8892\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.89100\n",
            "Epoch 32/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.4448 - acc: 0.9163 - val_loss: 0.5317 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.89100\n",
            "Epoch 33/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.4499 - acc: 0.9140 - val_loss: 0.5320 - val_acc: 0.8910\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.89100\n",
            "Epoch 34/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4463 - acc: 0.9152 - val_loss: 0.5320 - val_acc: 0.8918\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.89100 to 0.89180, saving model to /content/saved_models/cifar10_ResNet29v2_model.034.h5\n",
            "Epoch 35/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4483 - acc: 0.9145 - val_loss: 0.5318 - val_acc: 0.8912\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.89180\n",
            "Epoch 36/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4473 - acc: 0.9150 - val_loss: 0.5325 - val_acc: 0.8905\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.89180\n",
            "Epoch 37/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4507 - acc: 0.9153 - val_loss: 0.5307 - val_acc: 0.8904\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.89180\n",
            "Epoch 38/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4484 - acc: 0.9143 - val_loss: 0.5329 - val_acc: 0.8901\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.89180\n",
            "Epoch 39/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4460 - acc: 0.9155 - val_loss: 0.5314 - val_acc: 0.8905\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.89180\n",
            "Epoch 40/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 66s 42ms/step - loss: 0.4432 - acc: 0.9165 - val_loss: 0.5314 - val_acc: 0.8905\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.89180\n",
            "Epoch 41/50\n",
            "Learning rate:  1e-06\n",
            "1563/1563 [==============================] - 66s 43ms/step - loss: 0.4469 - acc: 0.9147 - val_loss: 0.5307 - val_acc: 0.8905\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.89180\n",
            "Epoch 42/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4454 - acc: 0.9157 - val_loss: 0.5338 - val_acc: 0.8899\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.89180\n",
            "Epoch 43/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.4474 - acc: 0.9157 - val_loss: 0.5314 - val_acc: 0.8897\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.89180\n",
            "Epoch 44/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4443 - acc: 0.9157 - val_loss: 0.5315 - val_acc: 0.8902\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.89180\n",
            "Epoch 45/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.4482 - acc: 0.9148 - val_loss: 0.5304 - val_acc: 0.8902\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.89180\n",
            "Epoch 46/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.4458 - acc: 0.9167 - val_loss: 0.5308 - val_acc: 0.8902\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.89180\n",
            "Epoch 47/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.4464 - acc: 0.9169 - val_loss: 0.5295 - val_acc: 0.8902\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.89180\n",
            "Epoch 48/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.4473 - acc: 0.9143 - val_loss: 0.5308 - val_acc: 0.8910\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.89180\n",
            "Epoch 49/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.4438 - acc: 0.9162 - val_loss: 0.5307 - val_acc: 0.8890\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.89180\n",
            "Epoch 50/50\n",
            "Learning rate:  5e-07\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.4467 - acc: 0.9160 - val_loss: 0.5297 - val_acc: 0.8917\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.89180\n",
            "10000/10000 [==============================] - 3s 259us/step\n",
            "Test loss: 0.5297107448577881\n",
            "Test accuracy: 0.8917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdatybpn3Vsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvAXjixS8bgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHCpUg6ID7ze",
        "colab_type": "text"
      },
      "source": [
        "# **Grad** **Cam**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ifQ1UcMEDPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_list = [\"https://www.rspcapetinsurance.org.au/rspca/media/images/hero/dog-insurance-hero.jpg\",\n",
        "               \"http://www.petsworld.in/blog/wp-content/uploads/2014/09/cute-kittens.jpg\",\n",
        "               \"https://ichef.bbci.co.uk/news/660/cpsprodpb/BEEB/production/_108557884_gettyimages-486237421.jpg\",\n",
        "               \"http://www.petsworld.in/blog/wp-content/uploads/2014/09/group-of-cats.jpg\",\n",
        "               \"https://www.washingtonpost.com/resizer/rfjfvGhq2mV_MkSFLGe5BhCuGTI=/150x0/smart/arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/QIXE7EXDPMI6TMFGHUBXEG4F54.jpg\",\n",
        "               \"https://cdn.wccftech.com/wp-content/uploads/2019/07/WCCFplanetzoo1.jpg\",\n",
        "               \"https://cdn.britannica.com/s:900x675/80/140480-131-28E57753/Dromedary-camels.jpg\",\n",
        "               \"http://www.petsworld.in/blog/wp-content/uploads/2014/09/Little_Girl_with_Her_Cat.jpg\",        \n",
        "               \"https://i.pinimg.com/originals/44/9f/f2/449ff2140cc4da56b186b0af58e6654f.jpg\",\n",
        "               \"https://aldf.org/wp-content/uploads/2018/05/lamb-iStock-665494268-16x9-e1559777676675.jpg\",\n",
        "               \"https://cdn.vox-cdn.com/thumbor/Uyn6m3wAQ_rvKSI_N8-UVNZ6YUI=/0x0:3000x2000/920x613/filters:focal(1260x760:1740x1240):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/47597665/185032680.0.jpg\",\n",
        "               \"https://previews.123rf.com/images/mazikab/mazikab1706/mazikab170600268/80608015-baby-tiger-and-baby-lion-are-playing-on-lawn.jpg\"\n",
        "\n",
        "               ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO54JJ_aEE8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef034979-11b8-40ff-e88e-5db23745760f"
      },
      "source": [
        "from skimage import io\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
        "import cv2\n",
        "\n",
        "#fig,axes = plt.subplots(nrows = 4, ncols = 10, figsize=(50,50))\n",
        "\n",
        "for index, i in enumerate(images_list):\n",
        "  img = io.imread(i)\n",
        "  #print(dog)\n",
        "#dog = io.imread(\"http://www.petsworld.in/blog/wp-content/uploads/2014/09/cute-kittens.jpg\")\n",
        "\n",
        "  img = cv2.resize(img, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "\n",
        "  preds = model.predict(x)\n",
        "  class_idx = np.argmax(preds[0])\n",
        "  print(class_idx)\n",
        "  class_output = model.output[:, class_idx]\n",
        "  last_conv_layer = model.get_layer(\"conv2d_31\")\n",
        "\n",
        "  grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
        "  print(grads.shape)\n",
        "  pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "  print(pooled_grads.shape)\n",
        "  iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "\n",
        "  pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "\n",
        "  for i in range(256):\n",
        "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\n",
        "  grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
        "  print(grads.shape)\n",
        "  pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "  print(pooled_grads.shape)\n",
        "  iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "\n",
        "  pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "\n",
        "  for i in range(256):\n",
        "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\n",
        "\n",
        "  heatmap = np.mean(conv_layer_output_value, axis = -1)\n",
        "  print(conv_layer_output_value.shape)\n",
        "  print(heatmap.shape)\n",
        "  heatmap = np.maximum(heatmap, 0)\n",
        "  heatmap /= np.max(heatmap)  \n",
        "\n",
        "\n",
        "  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "  heatmap = np.uint8(255 * heatmap)\n",
        "  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "  superimposed_img = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\n",
        "  from google.colab.patches import cv2_imshow\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  plt.subplot(2 ,6,index+1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(superimposed_img)\n",
        "  #axes[0,index].show(superimposed_img)\n",
        "  #plt.imshow(superimposed_img)\n",
        "  #cv2_imshow( img)\n",
        "  #cv2_imshow(superimposed_img)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n",
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n",
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n",
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n",
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(?, 8, 8, 256)\n",
            "(256,)\n",
            "(8, 8, 256)\n",
            "(8, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAC0CAYAAAA3tP9JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOy9a5Ac15Xf+cubWVlZj66urn6g0Wg0\nGiAIgiAIkSIlURQ1oqjRSB5p5MeMZ+xZx3jt8MZ6vfbHjfV+2PB41xHrcMR+WXs37PHu+rF+zHg9\n2rE8ntFoNBJFSRRFUhRFkSAAgmADaDQa/aiurq5HVlbmzf1wTmU3JUropuSWxs7/FxSys7Ly3rx5\n7nn8zzlOmqbkyJEjR47DgflJ30COHDly/OeEXOjmyJEjxyEiF7o5cuTIcYjIhW6OHDlyHCJyoZsj\nR44chwjvh/3xK3/7b/+xpjZ85G/9LWc/533UcbJxeq6PTS0AJgX0CikwOss6gJX/GMdhRAAxxpDq\nd3EgZfQFcBwnO25H5zuOXhksMDolTS3W2n2P88tpetdx/j9//b9P99wCjjH6OcVh9+vOnm3Y+Z5/\ns//tOZCN6+1nICN65798/9Hv/YXd744ezJ/5X/+Xu47x6V//9T/W6/XJX//1fa3X//Hj70lr4zUA\nqtUy21shAIMwJhrI3EVRgoMcnz3aYGNjC4D1tU3uO7sIwGazyfH5UwBcvPgWx45NyTXHC3hBkv3e\naEl4xsOO1r0x2Hh0js0Wjk0sxhQASKwltU52FZvId//bf/WFu47zf/7VP5+uNpsATI3PMF4UUXXr\n9jq1egmA+Zkp1podAG7eajJZLgJQHyth9Bc2t1oMA1/GRZETx47KOW4EXgBANxrwzPPfkfv34Pz7\nzwBQpEq3dRuA1BtjqyNjXDxSYifsAeC6JRxH7q3kWRrVMQD+9N/9Oz9wjD9U6P7HQJqmjGRcivkB\nr+Hhw6gQStOYtxsAeoc2E6Eid8yuSDIqeFJsdroBjs1OAHDzzvaupE13hSuOA3soe6OP1qbZ/RxE\n+P4weC67s+3s3sP3CrwRnD3nyKBG95nuzs6e+38n4bt7sdGH3bE6FkZbheM42Qbl7DnnHeV2Dnzf\nJxyIQK2NVZiaagBw88YyM0dmANjYaJFEMsEFr8JOW4RHuTTBdmsAwOzM8UwousbDL5blmtUxjN3z\nHPTZGuNCQUSG55pd5QE/e8ZJnJDYIQBxbLN3YziMGbKz7zFOluq0inKds406z3znVQA+eO4sr63L\nBnLt6gptvYn7H1ykty0CuOf0MDYGoDIX8NDsCQCWlle5cf0KAN/qW84tLgJwq73O+Qvn5Z431li6\n0QKgWhlQD0TAj5Vculbm581bSwyQjSXw1pmdmQXANz5rnbuPMXcv5MiRI8ch4seu6W6srQJghz08\nTy6fpoZiuQqIWeuqFheUym+3Z3+CGGmUnnH3HE2znXqv0uWk4Oh9p7DrUtiDOIW6L7vtdjGlHY1U\ngXTXTWFjMLvjHymLnueSJFaPOfw4Elhch8zkckj3+A6cPTvvHn8KezXd3fO/T599Zx/E9+D7n7Fj\ndq0Cd8/47N5zTa7qvhNmpk8TRaLpGnxqNXm3Tp+u0m6L2Xv27Dm2NjYBSBnywLkH5bMtEIjCRmIH\nFDzR2OYXjrKwMA+ATUP8PaIhjmUdYyDWtW5tkr0z1nSwSZyd6+hzs8SZNeP64BDte4xvdZo8ovez\nutOiNi4a/K1mi97GOgAfuXCCb1+XMTbiDkFJ1tFaaBlXefP60iquXQZgvRlTNuJqKI95XF0XWVUr\nB7TDbQCOeBELZfmtcjDGtW05p16qU437AHSMy2xdrh9uR0QdsRya9LDO2F3H9iMLXZumLF97HYBq\npUzgi5/EC6pYKz4fi4PvjczlJDMld9rblCpykyMB/ZNGuuuJxcHJfFjiaZTxGMzuXmEhSXf9u5kp\nlg4peLK6P/2z7+E3P/+snG53x2oTmy3ivUgSmwl1sD8+oZvJs7cLwV35u9dfu+tSMHsEv/N2v8Pb\nrzPyoDi7v+GkiC/h+07edW2kOJnLyU3tXv/LnqeRYwSv0AdHTG/cmJ2OmNUpEFsRxmsbW/Q6IuSC\nwKcYyJpbX2vSXxXzfHJqDMfI/LY7HZqt0Ts6xPO/X0CmNsVVpSSO412lykKsQtcYSBNVVJJh9uzT\nNCa1/r7HOF+rc3P5GgDDco3GVAWAiahLb0bcKalJSVJV4AzUCnLO0vI21XG5tyDxubXcBaAWBET4\n2T376nuu2k1uPfsiAKuxzWTVmYcfozFeB+BWa5liUWTbQrVMS6aZ6kRML5Rn4diE7fbdlcifDjUz\nR44cOf4zwY+sXi5dfoVaTSKpRrY5AKLIZqpV0XUy88O4bhZ0MZ5D1GvLhUpVvML+d8IfN7JAGmQu\nBdFuRRN4+D3388qrl+ScxO7RgPeeTxZYso5HUJadcfn6TS6cngPg4tI6Rd1JXa9Aty+myWAQMlIz\nXddkmmXyYwqkGcMeLZxsu/2Bu+4ehTYlxc007+/VPNM9Xxh9cri1JHN1bOEeHEe1ix8QGZPfcfb8\nZ0/A7acm1PpTBNNnqBptmqSUSxK1FwNDjvu+R0eDOl5hjGbrFgAWy9i4BoHKKXEi509MBkSJmNjG\ngTAW0WCMydwIgVcQlxjgYLLHlNgY1zV6PlirLjrjMHLM2dQ7kCfRri6zuKDvzEqLtCzrYL0z4NHT\nxwH4yteeIa1JEOsKs1x/TYJkJ+uT9PriZpkqFjl/4T4Annv5Ep4GxsrjESV5Pfn2v3yaMBJtdW66\nwgNn7pUxRk2mYmF0rJkqfiADiAd96vr50tU+tXFZ35USLByN7zq2dy10L736kgyqUSfqi3lTHB8j\nKImvYxjvmsiuazIBbIxLkiR63MN1NbrZ7xAN1d9Sqt7FR/jjx16WgNnjZx35eMdNj1/+xV8A4Lf+\n7b/DUWeV44iLZYTRRz/w6bTFjFtYuIcLp2RxrGw8S6AmXZg4NI6KqbS82iSKvt+kM6mDV5SXKtKI\n9buBwWEP4SJjCaSYPeb/7jmecYj1sIuDdfY4XUbsk7e5PdLsuxdffp71qy8DMNF+ga3gAgDH7ruw\n5/S3+493j1vuToD7zxvxENxsI0sx6qfEQhKLsKmWy0w0RAAUPA/PU5M/BU83fc+zWBWKySCiouwF\na2M8R5Ukx1AICnp9B9eI0NrLujHJMHNdpQCJrGPjOdjYZte03F0gjXB6cZ5IVYLpapm1lTsAPHDq\nKKEK1KOz89yKxaUQrjW5Z34BgCiMODMrbgGGbeLehpzT28wYPFW/yGBH3qd2L+K9J+X8sYk6b90Q\nH7BxHR6aF8G/k5RZXRU599EHz9FpC51t+gGP1y7L+eNH78F3gruOLXcv5MiRI8ch4l1pukuXXiFW\n7Tbw54g64iJI4pQolN3D2hijqo/jVUnULCFNcZyR+WEwutMWisVsB0Mjjz95pCQauU3CELcrO6Zj\ndo1ea3ej8Ht1t6pn8T3ZhdvtHXrdSQAenPOolERb8IolQp2WTqdHNJTHsd0dMFcX7fYDD57hqy+L\n2bT6o2i6JiWzBx2za7Y7aaa17zXtE7u7OCwWs3d/Hmn536Op2lhMtNLGC5RS0Xai0CXZ+BIAwelT\nhKay5xsjbTvle5Tmd/78xxDRnSVAglDFoye/7++t9VXq07MHu6jbx7g6dxYiNY0BSmV5t8JhJ9No\nrU2yQJfnedl7GQ0jCr5osX7R3WPhGXxGLJ4UJx4FxBMSRu4FJ0siStMCvd4wu77NAssuNpFr+n6J\naLB7n3dDcyckUP7x65cv86FHJIljrdWl05e1ZT2fR+flXdoOY2bVBWFqZTZ2RBMtOQa/IPf5yLlT\nFF0Zl1Mw/PZv/Kb8mOPy2lviWjk5DPF90fjDfp+VNy4DsD55hokx+a1ry1dZ2REmw1R9jGhMyQNm\nSF9ZED8MBxK6NpFJu33jKgV1I4RhG8+Xy9y++jphX7Ngjh9nYvoIAP3tO3iBqvuej/F2aVnGaAQ0\njvF1ASRxjPsTZDPEukG4eHh6r55jMurMUx9+H99+RQRhe6eTsTRwds2p2UZAHMtczJ04S3NTBPZY\nuUSg44yNoVGWBfr+U2VeuSab15NP3E8YynV6/SHtbv/HMq6RkHTedow9vti9KRCWRAWtNS5uxkzY\nZVM46Z4EB1Je+Nw/AGDKjWnt6FppOxxryMuzeukPqZ79M3r+bsYTe37X4XudDn+8pW5fzVDPK1J8\nh7+n62/CAYVu4Fcw+uqa1NIfjOiFloKv7oIoZKjvVpJakpGP1oEIiSN02iFjdREwLh5ba6JI1Rtl\nBrqmi26ZYSzH/cIY2yM/cbFI4IwSWixOurtyMh9+CjhynTjuH8iuvjU0rL92FYDK2BjXb8s70O6E\n2aYxPl5lU2kEod/lRleONypTVMdkXGXrsbkt4+12dpg7JsrPd1d2MnqiV3R59IzQxK6+tUbjiBw/\ndvwoK0vXASjNPEjr2g0ATp+9l7ZuYpu32wQ1FeRxxJ2tu9PicvdCjhw5chwiDqROPv/FfwdAd6fD\ngqa+eb5Pb0u0uIJf5OnnJV3vV+4/T1CXXcX2A6JQdqooGlCsjssFrUM8ckEYTzIKgCjsUarWfpRx\n/UgYmVnFwKeuxPN4ENLXMdxzfIHxmgbAVlbo9GS3vbZ0g0gJ4FFsqVXF7PBdH6Omj/VK7DQ1zXBy\nhl5XzZE45iMfOAdAtx+j6eK0drYzHuCPOKrsU5pxMtC05BFz4+11GEZsAycKMz6xKQR7/k6miKYp\nHGmI62B9o835hx4AoFYtEm2vAPD7X32DzxyVdNSikt2/F+KiGQX2fnp1gpvf+BzHP/iZu5+oLpdu\nNKT90n8AoDh7P80lMVsnJsYP/NtRBIOezNGRqVl6Awkyua7HrVUJ6tSqZUqBWmyOS3FMrKtBEmZB\n43q9QqqfHTemVB5xWBN8T+7LYMGR7/b6LmNV0SCdtINJZC1YY6iUNeMCy1At4tRa0lHQ1VoK3v7Z\nSdM4HLlPaiAsvbVCq6cuhYQspX58KqK5Kpp0zwyYr8k992yPV67cBOBorc6YurTCpMBLF4XF8foz\nv8e4jqXdjygauU44TJieEcbCicV5tjQRwzElajOSrHFjo8vCpCZBOCWCGbHiP/+FZzh/5sxdx7YP\noSsPZXX5On5B1OiZuTlsLJOws3aHVP08hXKJ//qv/RW9SZeRoWiCMZ7/0lcAWG/3+PlPfBiA4tgU\nKDsgTSIcNWmcdyLTHxKMMXiuTMuRqQbve1gi7tde+BqzumjiZpvZExIprY3v+qpmZxpcufIWAPVy\nwphSxrZ3WkxNyINptZsM1AUxGZRJB+JSML7LsCumaGzH6GyJYC64Dp4J9Pi79+nadNf3TGpw9lrt\nexlv72DOWzcANaewu4LQ7qGVuWnM8pqYnscXT1Gvy6JsrS/RFW46c+M+r3/9dwF478//5R9SWmGX\nnvbTlBzRXn6BzTeW5D9mf/pKOib1N9xhhKnLRu14Lu6YrIeps48feIy3VzfpNMVZMegU8csywRP1\ncXwt4uKYMtWKmr0UiPV9LbpO5prwfY9oIM81iiM8T65pnJTRe+8ZS6IJBWsbhmpZBFilEGPNNACd\nQYIpSDzG81wC9Ymm6e7mbmP42tefAeAv72OM7WRAdEM2k8AL8DXZYWrK0twSM3+n4zHeUDdn2yPs\nye+utrdoNOTetjcMW1bGHg5h5cUvADBeLuOrnGlUytxUZsL4WIFTcxM6J8PMldFpdvFVoQrqVTa6\nIqtSu8PlVyU57MELpzk5fndl8adXlciRI0eO/wSxj+1a5HJr9S18TYPr9kIqqsU1W5scmxMu29bG\nGu01MR/9Sj0LvFUaMzxwTgjK33r5IuGW7mDjMyRqflnAJKoSaQWfnwSMcTj/gNzr2VPHWLl1ffQX\nLl8Swv+58+cY02pLJd+lNi5mzSgHHmBj9SaVuuyYJdewuSrzgnWZPSZa8vbaKpWq8h5xQKPNM+OT\nbNzWeSwUSA+Qs/6DYPeQF763fNdI631bpbA9n1t3lnG7mqd+8r1v/96III/hzFnJ7//9L7zIxz4k\nptjKdplTM/I8Hzx7lJdevaHf/gE83T1353wvq+EnhM2r3wJguLPMRkc0OmNTRlyE3/8X/4w/8Rf+\n4vd978uf+yzve+JxAHwn5c51CQw1ji7w9NdeAeDUhccPHCucnzvG557Ve1qMed97xRpL+jvUAnkv\n7zSv0d5RV4DjZTzaOOpT9DWldsLHKmOmWq8xigeXgiJpqpqr62NTuebzz73Mr/7yz+p4Knzx6YsA\nLC2v8MA5WdMPPjQHjnzXpsOMqZQ6KWt39m+pXb3xBU6dkd+KtlNOzYvJf63TpuRX9ZqW66uiqS/S\n5uyUaOoLvSZLb4jFWb3vfUTqEXvjS98h0vKSgzSipNKvUXHZURrRZLXI7z79PAB//pd+gWioVmnN\no5NZC+BoEhhegXMnpFxkw3gMk7sHve8qdEeRahunhGpGB+WALTV/A9ewcWcNgGQY8e8/L26Ejz50\ngUsr4st7/InHmFwUysdT0zOQqs8nGe5mgiUx6VAG5QWF7GX7YRUDf7yQH5qfO8rslJh+QRBQr6if\ncjDgyLxkwuB5bN0U9gI2pj4rCy6xLgtzwtgoF/2MYL505RJlla22UGJzU3LKXSeho/7wI3MzxJoc\nsXJrhSnN+S7PTJGqH+pHqXX4vf7adzwnTd+xROPMkTnWb8m9Lb36IifPP6LXdPawDgxxUwTqhx47\nQ6kkz/X0ZI20I26TNy9fZqKiNKDnvsR9j30su7vRld7uU/6Pa4qtPvevcdRXHTH2jj5ax0mJu7KO\nU2szWlxsd++zNl7lP/yzfwLAp/7iX8qOb66vUZmQuMb2+m3K+nnj+uu894KUG3w3dTUqvsf7HhWf\n+ZtLy+x0RrTGiPnj8p45XsDkrJi61sbE6tKKQotJxV3gFZoYrX4TDYf4mhGa2ASjWWXDoYfjjmIT\nHi+8IIqHk8asbY6SIMpcuyaf73+gTJr0dO6cLIPNOD6PPnp+32OcnT+J5nMQ1Hssb8jvzlerXHxV\n3plGo8HCUbnP6s1NPvvZ5wA4f24xU+aa2xFl3TR6m3cy5lRiLX1lCG1HfYolOV6oFDhzTOJV260W\noQrdIhbKMsaBtXjKRXELBqNFg4xvcOzdn2fuXsiRI0eOQ8RdNd2R5jN7+kGuvfpNPWZ26w04Hk9/\nRXaYs2dOsd6UwNBzV66yOCnmtVfwsQNxHZhCAaNJA73mKn5VNLqCV8D6Gqhyi6RDUdML/sj8/o8L\nTzXuUwtzjNc0qrndzji1R44dY6hDvr26ypGar/dNFqTYWGvz7NdkLv7kpz/B1qpoSAuLM4QagKo3\njjAIJeC0s7LE9KLkeSe9TXzdJCcnpmheF62xn+xqCz9O7N1td835Xc3LAaJIzcRgjOmaaAupP4dl\nFKCxe/RSSxhJMGJltY2ZPwZAw+8R69x2uhEzY/JrO0uXsc7H5V7SXc6uaORq/diUfSgOPxDX//Af\nAnDi4381O7b+2heItkXzjnZ28IoamR++8w81r72IXxXtPB5EBP6oileLRN+BjbUmkXK4/6+///fx\nNc07tglGzetkOMQOhS+6vLxJfbLxrsdVKwVUa8r7rk3y6tU3ZQytLb72dWHDvO9DH6Toi7ZXq/u8\ndVVYDRNH5ii6sr49r4cZVf5yU7R8AjYJd8tuuj6RFav24x8/zzeee0PGxhjFMdGkC2mJZCBjfuU7\nK1x4rxyPhj08DT6/tXSb48eP7XuMq7duMffQPXI/tkpJC5p3oh6zU/LMltsDerfkefQK08zOy3v4\nxtIyH/3YQ3IPpVNcXlLL0jWYYazXtHj6nI5Oj7O8JXJrdWtAI5H1sToxg68p1mcaFd5sypyfnK0y\nVDfFV2/e4X6tBRHbhDC9exrwviljjakZak98EoBXX/wqsd78IIxoqHB97pWrvP8R8es1JqeZrInA\njDpbtNtCvRg/soApaYGcQpFYBZBTLGUC3k3TH1uhl/3CD8RcCIoeg/4oqy7FL8qkG89jqjF6USJ8\nT10tjWMUlT4265UpKXVmGPW4fk1ehmqQsoNE85utkJOnxQSsTc0wpiZn5DtYzZuPoi5Htar9K2+t\nEPhyb9VqFaMZNWlqGUSDfY/ve10HP2h2957j64ZnUsv2trx4tfFGtljjvYQIpMgPwKlTc7x0RRbu\nBwLLdksEwSA27LSVaO8Z3HR3M0n3SP6REDY2otfr8m6QpglRT+bzyn/4vxlTOlvYauIpHTEteLS2\n5beG7RWGfbnPQmmXxuUFdVAh1bnzAr2eZnY5u5V/m+0OJ04tApCsLrPTUxrTMCLUzKgkDulqCcba\nZJ1KbW9m3sHQ7jaZnpU1sbHhoBUccUyfYSrKyle//iIfeUJ8vZVqk3vvFR/7xSs7NDeFTvXo+0u4\nkTzjvu1ndXALJmGgySs27jDypBT8Mg2dx14YZOs1jFwST96ZrWaPP/j3Mo8TExWslXNqtQrx0d6+\nx2hMCgMZy+otF78q7+Hp6TmG92h9iS40tK3QjatXaN0RN9yD7z3Ly98RAfnguRc5qrLkoulT1hhK\n2Ivoq8tlrd2TFEykDO3pU+KjjX3DQI+/dnOb/kDWd3cqxjOyDh4+McXGjW/IeGee4Mbqd+4+tn3P\nQo4cOXLk+JFxoOQIb1RlfvE0d26Iyr65upKVlit40LkjlbWuXHmT990vGt2wGTClTe/SJMrKPDqu\nRxpqAWY/yMrDWeNlJSIPCyPe8TDqoUX5CYIyjhZ/PvXgeXod0brC7jb+uGi35XoDVzmbpWLA6RNi\nQhVNgflxibIePTbLF74mgYCdZpMn5xflmov3geY9xBRpjwpK11yaRuZ05sgsZ0d+jT1NMF3v4Ptl\n+o7Bqt0AW0qK0aLQQmofVaayVI5J4sbGq09TW9RATLWeJTKkQHlCWCxX37jC8oocf+Kh87RaollV\nPMtAtbJ61WBWpLB735ugMHGv3pFlOApuDQdcu/rSwcao87Py7L+hMCYaa2ligigV7TaYKuKNS7Cz\n8/pzOBX5PLZwhNYdcQdNL45ngT2vXCOOZC16R+7D25Dkn7jbz1Jf+xbevHpN7z5lQhskfuqTT9Br\ni9bnej5dJSy3W00e/sinDjSuvWi2N7LU+9nZYywti2lcqZQpaPr8VmebV167pWMYUKvIGK5cXs8a\nDbS2uxjV3i5d3eDRx8RK7XbeyK7veUVMVrwf3vvwol5nk7UVeVFmGkfoDrTSIAPuOy+WrzEutTF5\nBzw/zFwt+8F2c5vvaqW6D97zYZbWRLu9un6NIuKS3Fp9k1agPN2Vy8xoofPXvrtCoj813KlkSU3z\nR4/Q1/ouG9GA84tiZSZRwpVrYpGYzjBLOb69/Cb3P/xBADZNG40Ns9ppMteQ7y5d+QZzc8pLpod1\n757I9K4KHEzNLrC2LD7HoFwl0IE7lRonVEB8+Nw5Xtdo9k4Y4qyLe+FIsQwFefPcNCVR+ooDDCMR\nxqnpYAtKv0rZ5TQlaUavsWZPCZYfUV8veB5nTon5NRhEu80WXZdBLAKm5u3SbkrVGr1tYWxUp2ZA\n/XmBa7jnmJCyi70eJc1UO+l4/A9/4ikA/sWXf4eFD6h/94kJ/vXXfxuAYWfAL9UlurtwtcR3e3IP\nr8dWGgIidLasWj8pw2j/mWrOng49qbMraKX10K7gHBUmMsbs+vUcGHalLUqzeYuJMw/o+RabtXIx\nTCyIH81cu4F1R2R5n5GnaKJeox2OougR/Z64lir1Mk5XNqW3NmIm9UV1XMOK1uXdL1o3ruj1e5mP\nMux0qZ84DUB56gO0bgvVCb9MoruAY6HTFpM02LhNHGlMoVjK8vfCfg9KSrc6Xs1KlNZKPps76joo\nenz40z8HwLXnn+XEhfdmcxVozGJiaoo0Kw16cKd10S/gaNeDnt2hUtT6INZQVppMq9/K+jNNTt7L\noC/K0KPvX+TyRRHS6+s+335RMuMCf5p/+28kZvOLv/QeHNUGCsUOLpp5NixjXRFOD5w/SkMLwPTC\nEtGm0EAfe/QM1pO5GAwsRfXpDocpRf/u/s4RKqU6FS1puuOndDyhLB6t1blxVa4/HRSpHxMBf+ly\nhK3LJvvIo4u88rJQxoaFKs0d2fiSJKbsy1wdmXAp6GbSizqcOSOb8tJKRDqQtTtMEoqa67BYhpcv\nCpVsbvEBnv/G7wFw4ZGf4YUXRDH46Afvp72T117IkSNHjp8qvCtNN01TOloPICgV6bRFY3nw/jP4\nqokV6zVOy+ZEc20VqwkPYRRTVed0r9vGVxMwNWY3ZdBafNWe42TI33lOdtG/9tgcNTWr/9HTr3Ly\niLgsPnlPA99/910nAs9gNJV5u7VNQfmK0TBhoKUqg1KJ1qZoCzOzM3jjWnvCGLxRFKi3Q0W1i6lq\njYqWqAymA6aeEDP2r/2lX6N7UszMl6Yf41/eEpNuoTzgMe//kOsPY8rflcBbtVRiR0vc28QSK//Q\nMU6mAe8He1MRJN16t7p/mrWHj2CoQcRCEaM596kTY1TrKAfjBOr6GAxjiXQjQS+vKPfcaff4C7/4\nJAD1Wp2wqy4C12VDGyc2W0PmjkkgsBDs4Gmw8N7ZGmtrKzpGn9lGfd9jBNi4+EcAtHsp1apW+g/h\njaclBfVnf+1BvKrUffCrM2xpI9UgsThaEau7s0UB0XRjx2QshfJ4jVoo99zevM3Sc58D4Oc/9QT/\n7F+J5jNzdJbOumjMPWuytbTdXCccyHfDsPeuNNwRvIJLoaDlB3shJ0/IWrz25rUskWH2yGxWYvHy\npTucOSMWWNXZZqjV795aglpd3Drh4DaxMmy+8vQbPPnRRQDctEFBA2ypn2BVZLRbawSaXTCILE88\nLlaOjZpZbQfXMbgaXLWUiKP9s3DO3fs4NzbkWW5u3MGL5V1K+hEl7dpQXZjl4jPyXD/xc4+ihgcr\ny2sEyrZZvnEnq4TneWSt5X3PY2SCFUsVajWZn6nOMpeuibyZqVeYOiLa/EuvXQR1vfV6PdY0Dfur\nX3mR6WlZoxevfp1B5+5pwO9K6Pb7/YzG1Oz0KGs+drvVZHJKBCGJ1b4wUKvXUI6xsBJ0IXpFn3ZT\nzNapo/M4WrneSwz/01eEmr3Zy9UAACAASURBVPKBY6/z+KwM9jtLu/dw/zy4VjJ8/vlLD/OrF+RF\nKpWCdyT4/zDshEM2WiIMfM9Q1a6iNk2yer/GcXnmG9K8Lrbwi5+RbBlSZ5c+Z52MklIPPOoPi8ui\nOxuyUpXMtvjULF/oyHdv9J/iZwbywDr9G3zxnLzoPzfzj6iGIvDGxydZXZc5ipM4I3d7npvV+t0P\njI0Y+WacdE+zS+Ox2w/YZrUwtq5fpjEnOVfWQGFU96HocfP1pwGYO/EA3c62Ht7Ns6+VDJdeFt/n\nG9c3uO+oPO9nv7tJTYuqxKbAym0xVe+plDJXiSGl12ll9zMcHiwbb1R8qE+VWMP6tfo4c8c0L//1\nZ7h5WXyd/uQYkyfOArBx5xYzMxK1jjpbpOjmExcy8vt2s8mGZijWiyluJPP25sUrfOYXnpTvRkO2\ntuR5BUGBcDAytUPa22LWT01N0dkSdxuux4tfFoH95K//+r7G6DpO9uxnZ+v0+3Ld8w/Mcv2GbOh3\nru1QUepjq2WxVgS+ZwxzcyIYbtzq4xTlfJ8CjvrPojDgymsy/jNnDW5Zy0XaiMAX5aFeD4m1zu4g\nciGV813jEatrIgi8LFbimJjhO3RG+UEoVzrUWo8BcP/CAm8q7csfRFS0kM/KrSZW5zd1LGjRmtnZ\nEqfPSTwpMnUufeu7AIS9HaxuOI5j2GrJ2KdnJ3DcUb1hh6PzMj9JOOTSdaGATk3O43kyDzevr3Dm\nPlkrGxubXF+SZ3nmTJFu++7dgHP3Qo4cOXIcIt6VphuFPcK+aGWeX8hM3jCEbU0P9guFrAmcV64w\noUkQqYWhaoND62XRxGE0pODJ+b9zI+TR0+LgdyKXaU92lbVkGSfVoBIpWpSehcmX+HvPSnDnbz4x\nTxCMysztD7/wyad49RXZDW+vb3Hv8SM6zpCy8m591+Xsgmiuva0WpTUJpC1MTVFTTahQKONPaiCt\nUaIjPn7Wum8yNyGshqWe5fJtTSe8fJnpZTHvvLDG1XslvfbJ+89S+46cc2ZQpqU6ZMsx7Cj31JbL\nOOzfvWDDHka1EdI0i0hjitiRmyIOs/5WjXqACWWMqZNmwbBSwfLdb4sWOzczQ3GUox9ZYnVglKtF\ntjQBYaJoiNRs7cYGtfiJ4pS6Vl5zINNoh8OAdTXPxycqbG539j1GgMacpGRvvbFBeUKW9ytXVzit\nKbHba5vMHhNTdfP2GpHV8qOOi1HXmOMZrAbG2h2oTIr1Nug0GSvLsx6rFtjUNHcmFqlPiqW1dWeZ\nSPnTXqFA2Bt10BjQUJ53HA24c03m8Itffp6FowcrY+r7xawvWhT2cAu7FcGOHhk1FyhwZ1M0uXJQ\nJAq1nGglYPHkKD04oL0j7pXYbdAL5VmW/ZRVTexx3WnuuVe0tyjp829+87cA+JVf/iTG22W6dLRE\naTUo73asIAZ1TQzjHl6wf3Gz/SbMVMWE/9I3XuRnHpG06WFvjHIk4ypGKW199zbWdxirjBp0emzc\nkCC+8Va457isy/a2S6KBvX4nzFKF425IpDKpXC4x6p+w2upyoS6lGt1agzuT8t1vfP1VhqG8k0Gx\nyokT8rv9nZh67fZdx/bu2vVcvZKR9JMkIVCfY7/XoapFX3a2t7OMpqmZOfqhlkXz/cx8HPZDKjV5\n8YY2JYllsf7C+Tpfvrr74NZjMQc/dPrnePbqFwH46OnPkCDX/9LVL/D4KYlIr+7MMe8dLIMrGQ75\n0GPvA+CZZ79BrKYYNiXWFjnD9jZ/4WefBKDz6mUmFmWDmFqM6UbiAyrO1PHHZYG6RRdvWzYm97LD\n8leENpXcF1FWeTc49zjJDTUB26u0VmXMN97zMe75JSkXN/18AFdlrq/02yyp4IyGMV5x/9Fgequk\n6iIwJDhmRBPbLWVpkxirJqB1LN2u+Or9oEi3K3PdmKzz0ENikkf9HXrKQKhWqzi6oK/d6jPXkLEM\nE4/vXpffLZccbqzK9as1j56axWHPZE0w07TNieOLALy19CYHMcZSJ2VEWZg4UgYtmzk/ljLQrtOv\nXl3nT/3qr8jnV3+PcVfJ/r2QqWPiTmlv3aG2KD7Ka1//KgWtu9rrhMyp8C5VG1TUtcJ4nVe/JfSm\nmaNTTE2JIG+3d7JYQavVyjbwSqVMWwus2NRCfDAXirWWRM3kOEpJRu10vCjzs1bKEawpq6ZYoVoe\ncahK1KryebKeUFFBeHOlhacshUKhyaMfFGHz+nfX+eZzsoFeeOi+7Nn3By3iUL4bJw2Mmt5h1Kan\nrbxsEmeNXaNBhDnAa/nNr32en3nq5wE4lhbZWZPxtlabHLtXNtaZqSLLL8k9vHlrk/sXhbIYVEr4\nVaU7RkOGsQjmM8cbXL0tcZk0tYz6ZHbaA/yaln9MDHNaH/fRTz9BqyWuA3fQZnZCFKrPPPVJXnhN\n4hTXrt3g0UfvB6C53mbh2OJdx5a7F3LkyJHjEPGuNN0kHhIUdwsej9JUy+WAhhbxDcMB1ZrY10G5\nykBT+mJrGavLjhGXQ3BGHFRDSxkOr77x+SwYVjAeiZFd6JtvfjHra/RH1/4drnKCXdfgxvL5n15u\n8jcfnT7QeC5ducKCVhaaPzJFszUKXFlCNTt2Ntaoz8o5c5++j+6cBnvO+PzB56QwstP0+OVPSqUq\nExui72h01Pqkb8rn9kqTkx+RoMBbhR18bWkdxUPqBXGjXCq6XDgrWtR8tcVUqF0WvtCn0xGzZsNA\ntXIAN0rUzKK11omzbhEOZrcueUoWOCQ1BCNzzcKUsgiM4zEcMRxsOeMNx3FIYcSLpZC5Gu7sRNTG\n9JwoxisV9JrJbh8812R8336vT037xk3MjNNRF8d+cO3pf0WqBatNYlndEi18ZS1hRG5xrKWlpnO5\nVubmNQmM/fyf/y8IVWvfWFmlMSWupNMPvYfVm8s6byn+mKyBtGDoa2r3N595jp6yGspvLHHulLgR\nirXJzJKbX4RnvyI8z/GKnzECkneR7e6kMdbK3JXLRawG/ZIYHLXyGpMRvZ48g7BnJeEI8EyXsCdr\na3OtzbmH5V43u4ZBTzTUR95/GpvI+n7f+8fp9WSNLi29zANnJLXYL7fpx8I8ur5aYD7j4HoE6vqw\nicXTRTGM+3T7+09bf+ihx1m5Ii6CI1PzhG15rov3LRAnsi6vXbzJU09JQ4Red4PBqMtr6oEWXg8q\nPlEkk7y8vkmrrTVgwmhEr2enbxkfcZ2LhawQ/I2lG7SjEdumxUxNnveZsw/wxPkPAFANYqo17a+2\n2aF36e688ndHGcNS0HqyY9UaU1q8Y/X2LbyyPIipcUO7KS+M5wcUlALmGrIGj36pkpWcc4zht1+U\nl+HsgvwKjBoWyCRPlCdp9+SaT5z+FD1tmPfNpWeytj+PTZosyWK/cBzD2poIWmuHWT1Jz3Vx1Tzq\nNZvc3BTi/VRjlumPS5GLa8lRvlUTc2dqZo6Xxx4GYN7dYLMrNU+T1pC2lpGt1yd46l5ZEM+Hy3Q9\nbaBXG8N0ZU6X27M8U34CgI/d923G1Y/bubZB+aJmRxV8Or39m6WOjfaUyzQZrccxKWZP5tmIguA6\nHlvrYlY2xiskmqHU2lxna1NM9etLdzilNQdC22NmUhbrTpjQ0Rd+aD0irVdQq/p7uoN49PTZTzke\nsQrLRqNGouZpr9VjsrZ/f+ewv8OOvtj1Wo1uU7vSGoj1hXzyqYew6qIxvTUeuyAbnUktsRakmZoo\n0VmXjbFoehxVQs5WagjVpTAMu9xYEbdSHA9JdLPa7oWcf588u16/z5bWl7YYPPWX7/SgqrZ2CkQH\nXK8pDkky+r5HqsURCr6XuRq8QoG5BTm+uuxTMKIApeZ25s2479wpjJG1NT4W0botYxgMelm97DCM\nMnbLqdPHwYjyNIhiLCKMtzZuYBfKOtdplnGKY2jvaGcUYzHO/g3rja3b3NSaHe9/7CGCo7J5rd1Z\nYmlVE5Nsm9/7baGMnbnvFN10dP99PH0eUX+H08dFCVtZ2yDR9ZdGDh6jrshx1kzTSYZsaGGba9eH\nnD4r7//E0Um6G7Ip37x2iTQQ4Xpjpcda/G0ALn59m6ONuytCuXshR44cOQ4RB9J00yxF1MlyvH3f\nz4Jqs0fndhn4rke1ISyAsN8mUpbCWONItjMb7ChTEeN61MsaPU4dYq298Ikzn2YrEi30xRvfAN0t\n/+jK72blGdyCYVQ368ZOxOPmYMTzcrnC7duap+5CORAT2CZJVmQdx2BcOf7yN5/lQx8Srq0zeYK/\n/jf+Kx1nRKQF2uk1YVuDJRsxm65WMesXiC6KpvXko9d46TPvl+9eGsJpDTL56wQjTShNKZVUiwhq\nWA1YpIUClv2nAVub7PKX9/Zdt+luxbGUjFCPibIgWblqcLtyvFQuAXK8PjZBUQMxJTNOU9uND2PL\nSAm3mMy03QmTrFaDH3gUNfLcD3uEmWkIPbVCQ2uol/cfLHRJSEeV17yA93xAOoDYqJs9O5sa2hsS\nsW9UfaJRpalLz3Hxslha9XHDhHYDKSRh5n4pVutEW5JemiQJ71Me9kfHqrRuyzXDfkh7Uz7fWl7m\nrYtSaW528RhW1+7qTshiZZQ0IPN1EETRgGvqFjl58iSjtV/wfAoFWWee52LULdfprPDNb8o9Pfpw\nNau9EMXbxANZBzMzPhsr8t1K2ct4+K4H/d4oASalq+62ytgEL78k78zpU3VSrSbW7mzja6qt7xcz\niziKeiTx/t/LT/3Kp/h7f/d/B6C+MMnllyWwfOzcHLYl9/DiC19l/qSwGmzBp6CMk9sbW1nDzUrR\n5cZqS+ch4ueekEDg8y9c4kZX56paZtAXt0MSpzhawW3hhKF2VFhHhVLAoCbP7423bvLQorhZFom4\n8ISk+HdXP0thePdneSChu9MWU6HgedQbYnMFQUBHi3qUgiJGI+pewccrKa2lv0NR3Qu+H+ypGZBi\nXDVtbcIvnhYT4mutNDNFvnz18/QdeehPnf55XrsmUeL2oEPktXdvTs2DlRDMARX4lN1CMlGUEKgp\nTQqRUpnWo5i1KxLJLB0rs/SPhTR94dcmCZXC4hgYqIka3exTuanNGVc3aKtJWySlJp1aePj45+GI\n7BztJ05yKpDI6mz/dU6pDei96cHrMh5vzSN2Rz6yBLPP5oggwtQ1I0YIe3r0GNLR7pWkGC3X3+v2\nODIrm+bOTo94KAv3yNFJXM0867bDrBzgpavXOTUnUfuaDy0VuiWbZqUHC27K3DH53c2NISYdlRI0\nWC1KUi5Y1jTpoIylF+55xndBJ4Jxjcx3tlYx2qXCYAi1WBHOgKr6WQc7azhaxtNzHWZGyW82oVTQ\nBBm/jNVMskG3S6Smc6dnsbEIsk6xSE1ZK81OzGBZ6I43rqxnSUHL11bw96yrIKNSWVJ7MC+fcRzm\ntENJoWAwyj7xPZ9QfcvWpiSRjOH8e6a5dGmQ/V4xkGfsBwnGyoY+SNYJNe7imDokck4Sx/hFub7r\nFDKqWj+EZlPW6wPnnMwN53lJlsATDnpZ4arExgfKoPzOM7/Hn/z4hwD4/Ff+kJ/7oPhQf+dz/5po\nRZWTqQI7LXE1tDaGlOtTOvYk6xhsU1i+JfLJxh6JirxP/OKTvPjsa3L/Z+cwulH8xj9/kU98TFyH\nt+4sc2dDFL4HH54njjSuYQd8/cviXpienaIZLwEQh20ePv/4XceWuxdy5MiR4xBxoC12+cYSAOP1\neubIX735Fqfuk+pYg14722HiMMIokdzaFLRg9XAYMQhF63A9H09NHSe1WNXiTtQ/yc0tYQQMnAg/\nleu8tfEmfS2cfOH+C7x89QW5PilBItf5K4vmwHUYomHEQIN7wyjBL8guHxS9bFdKPI8lDWhVvrnD\nXEWSMTZ/8w6hdsUYJgllNYeDuEj8ljbvDIIsuNcx4F+RXbU2Nc7PvkeCbZXj10lleuktrTGIxLy1\nzRhvTR5Tp1xhJxXtIokTHP8AHFZsluAQ9SJ8Tcc1jpcF1VKbQCzXDPwSkRHtqDE5xo2bYkpuru9Q\n0Mpw4/WUUHm9D184y9UrkpZdLhtGSyt2IoIdGfuphQaxmqpTNYdCcdSHKyTSYGlqLT29n9A1tNea\n+x5jpxeh+TX4hYCoK1ry5aVtrCbgV3wHvyBshJnpKk5Bq3V1Q8bV5B/GluamaPb1MY80HhW4LlAN\n9Nn5LokVi63djhiohkkccWtL5qQXDnHVf2Zcw8xxYT7cuXyLUF1sDpnHbN+IbUK5PKoP0qfXEg11\nb5F7YwwFff+60R3W7sg9zR9t4HrKJIp3aLdkPQXVhKIW8m93upSUkWQ8w1BdNsaDMJa1njo+jz0u\n7hXXWwMtnRlGHUpGE4qKJXrqK3JdkzU+2A+mjxyl39deaDbiN/75vwRgasrgjGJVJqWgrkTHxJk1\nE4VplrJ7ZyvBK8icTDWKFNXaWF+6wQP3C+vAFlJ85D7ni5b1G+JmWrxvkTHNO1i7tcpEXcb49B9u\nsDAj7/aTH/sE/+e/+KcAPHL6fvx9JCwdSOiOyN2GmFCTBuqT0/KyAoWgSNSRhV6emCYeNZos7ApB\n13EpasaY8X0yZdvGDDXyOhmvsKOm4WZ/g0RLD95ovZnlh7946Ru7DeFweGZFSvedf7iW0Zj2C7/g\nUdVyghvrLXoDzcMuuBhty9FzUt7Qeqjzc9N8/XUxa8aW+2LOII3y2k15YDPz83h1MUdCB4y6Zky1\nypoeZ7PG1Ff0RbcQKn9obbPAlpYZrE5UccfUTTFWYaBuAd/3KHj7N9f6/ZCS5uIXg4BIiwa3o3Wm\nq+p7jyO6etzzy5S1s2wSJ9ihljGc9+nfEp9uL4xwlY7TWmozOyNR4vWeR6SJD+ubZC1PLr3Z5NSk\nPL+NHpT0BbBRm452W0hNg6srEvE/1SgxOz2z7zHOTPoY9RN1uj3qDXlJTsRD7Ki2g+dl9SWs2d1w\nSmWfoY7FGot1Ze22QzcrEekYF6sO3jC2mFF5xmRAogWgelFCfUzGu7mVkiiVMUoSrr0pwn7Ms4ze\nCGNckgPyxsJ+L2ukGA0iKhXZadI0zsbjF4uEQ3kGtWqd+eOjzL4qvVDcIg6Wmq4JTIyr/lfXCzKf\nbhrHOGaUvRgRRXLN1Bp8X7P22n08dZf4viHUIlHDYYyn1xyEPdIDGNbF8SITmgn4+1/8GvNaL6Ld\nijha1aJavpeFkG7esQRFubchhm4ov7XZSbKEmW63TymRJKo+lkA3qK1ul/FZkTfzZydonJR1/MXP\nv061KutpYW6Co2OLAEw2qjzymCTPfPfFl5hXdsTmrSa98t3fydy9kCNHjhyHiAOphKGayBurNzh+\nTKJ6vl8kUq23WPQJVaMI4iRr6ewYw1CLQnvFMlajCy5upiXHcZzVA/CCGhMtubUH73mCL135rBw3\nu3tE6rmcrUvjut+5aPir50V7rFVqu4yDfaJWr9NUTqBf8OhpJL0cJNlvWseho1rmpWYza0df3Goz\nURcteaFe4/hDsgNurK8TXRFn+333nGZef6tcKTPU3b+dOmj1SwakxBpY6nhFnIbM3bde/Q6LZyUl\nMxhvqHUAWEs03D97wVjoaY+ualClp7nj48E4q6uigVXrDYravy5JevQ1AlYoFahpT6+NzTaOaoqD\nCKZVZzNln6Ga2x7wppqzZ45Osa7pxA/NHmN1Q7TYR0+fyoJ5yxtd4lCDL72YI5pzb1OydOX9oFqt\n0R9ZWuWASFPPxytF3LFREkqCa0ZNRZ0scAhkqciBXyAeacCBj6usj+ZOHy9jB3gZxzwoVTIySD9s\nc2tVW5ADoSqJZQ+8QJ9dv5eVUfRQfvQBMeLjFgouA7VOCgWPggZXe/0dRvRfG1mOn5TzX35uiQce\nlt8umIB41CbFc7Nyqr1+iO/IGIyXMGpvMog8ktF9mzKJVvjyTEkyaIBWewPPaEnTYpntVkvny2EQ\nH6QbjE+nK66lUg2OaeWv7rVVJuqinY9NzRDpmv7oqSN88WlhdGzhYZXRkaQOlaI8nDiyLK2JHDrz\n3mN0bioPfWaeey/IG/rKxaucqktA+E99usi3nlsCYPnSFu0VSf390KP30l6TcR2dneKPPi990c6O\nN3jwg++568gOJHRH678cBFkCgXFNVrKt3+tki3IwiLKF6JgCifqw0kGY0TkSG2UXdeRiAPjGcEQn\ndmd1nZOVJwH4zbd6wukCzjjbeEql+rULNapVbQtyQNcCQBxFHJsTkjw2YUP9eVFkUTcXnufhqena\nGB/H6ov78qVrmdAdr09Q0Ey9o8dPcPKYREHv9C5S+4h2Ri7G+DdkofudMlfUjIuc3RcmjSIGWlBo\n9vgiF5+Xhz02M4Pva83aQUThAL7rJImJtPZCZDxKvj4DYHpKfI2bnRaB+okbtXKWybPWbGdFTFLr\n4GrXgkH3DkEglJ2d1gozc5L7fnIuod8TIX3qTAOzPDLVu8xoY8OpeoyjZt/iwizrG1okxjecmJEx\nrnZCquW7l8obodXq8pLS8Z542MemuzGFUclgl13qY2Q9fBUEjnEY1WMxBvwRDcuJs66ZM7VC1kUi\ntRGOrrXEWgYq4AtBwL0n5fjKnQQ0cdHalKg/qn3hZJ1BY+NkjT73i6DogTOi2BmK6hs3e9gFxsTE\nuin7pYgk0uamtVVMKs8bxwV/lCRTp90WofXAuVnUQ0AUpbvJFx6UfXUNegV64ci56rB6WzbTyalJ\nEt3QB4MER+vpOo5LOdj/Bvrvf+sL/Lm/KFSsM2en2EmUeoahsyM3N37ERy/PnZUW/+Vf/hkA/vFv\nfR1HXh8+eP80n7sorsCP3D/J5SvCRihPkbVw6nRj/vE/FTbSmVNlmtoFwxtWcTTGUWtMMlNTt9TA\n4fLLolCFVY9UXRnV02OsaqGdH4bcvZAjR44ch4gDqYVzc6KCX93eyApWx8MhVrmp5eoYiQae4jga\n1TVmbMwnGWpAqt2ioAGaGAej7oVytZJVlt+7E9SnxhnTa/539WIWKDCmRkEjz57nHdilsBdBUNwT\nDJvItOWNjU18tQ+rpRIlDQDOTh1hfFpMkMtLt3CMjsemtLSA9fF6g/F75Pj4hx7mlfKoFGbAqRUt\nmP4tqK/LOW3ADnfrBnhaBj9OLHP3iHuh3Q9Z15KJlVL5QFHvXj/E0cj7Tq+bJSk0GkdoKd+y4geZ\nNbPZbFPR4GKt6pOoS6jVCbVChqSaXtPi0rNzM6xqqcOxWsCRGZmrdnfAjHJYe3EBG6kVEXvEmrIb\nFF36ymiJbZ2dgYw96nQZlPe/RHu9LjPT2pVk6GepxWE3wilJoCTtbFNWlkLZOIzMsSRJd7taOiZL\n+rDxMAsEWxtjtJIarrcbpx5EpFr1q1YNGOUANBrjbDTleQUGOrqOjePQjLRuSAGG8f4DogAFL8iY\nE8Z16fXV7be+mpnVvm8y7vpw6GUa5z2nF0FZKd1uD7RbRj+8yb33iebXa+8Q6Xgm6o2sb15GfwE6\nnS4DfU6VSoX5Y5IKHycDEpS3jqGg/Ndur3uggOE9H7mHW7pWbt9oMr2oXRv6Do5WEFtfWeHqkriT\nHn7PNJ/9/6TR6a986sNs6EPo3dikGsgzOHtyjKl7JDjneGNMaUv4C3MLLC4sAXDp4lUK+mSXrl3D\naAeR9z40Q5zIGnr52Zdoq+vt7JkZgqrw96eq0/SSu7+UB6OM3RLV2XGcrARgUG+gyWZsbzVxdYEG\npVJWMnA4CLOc/oLnk6jf1zNO5qO0xs86sCZJvOu+MF5m2hbVdP9xo9PZYawiZu/kxDie3pPnGVqt\nkZCAGa0DYIpF3KL4CMdrY4SayLC+0aShzITEtth5XATP9uICn239WQDCTpu/sijlKc90oXxHI+aJ\nZdCTBxzudEhVAPvFgJ7aeqViMbu3dqeDYf/zUa/XiZSo3ut2KanfdGVtBV8FSZrETIzaJ/UdNkeZ\nW2Pj2FHLE9/QUQF53+l5Vu+IKdbe2qanCQ5T0zUmNHsn7EWUqzJXphdSVdqN73kk2l51vb/Dwqxs\nYr1+zHhFxPrRqXsY2u19jzExPkcbIx98QFGz+tqJR9QSNogxHhU7WmcRW8r06tgqU4VRcZOQ+bpS\nshKDp8kBOIZ4tHb9ArH6s/uDHoH6a3v9IaPFbpOUaORGSFI8XdPbseWYJtrdTlwKB/SIpTh4WYYd\nFFz57cXFUxkrJYq6WQskm8b4Ss0sF4tZcZ7a2BjBqMZCv0OkWlJgqpR1bMNBSKz00HKlnMndwA8o\naeamFLMadZ0YZPVoMWmmwJRKPruVle6O+SOT3FwSYXny/H3cvC6ZfVMThp2OrOOxmTqpNrmt16cJ\nAokdDIdrPP0lqZPScCL+mz8nRXG+9uzrdLQ40D3nXLY1g3K87nBL62OfPDlHX2t/lKpFHnlQ2mk5\npRqvf0MSsx56+AL/5A/E5dd8ZZmHzolS9NCTj/DyM9+969hy90KOHDlyHCIOtMeeOi0S/bVvfQOr\nZllzYzUzO4zn4WlwJImHhLFoiaVKjYGqw35QwlV+rev5jLSCsNfNjmOdXVPvEFBwDcWSaAWddoe6\n8mLLQZHFhUUAemGfrbZocp3eHZpd2THPnz+Lo/fqFopsNDXPOyqzrVWSv3A55v/9I8kdXzg6yVsf\nFzfNQuFVTCQadhgN6WyJVtfebjE1IYG3OEko6JyGiaWj2qTr+RlrZD+wJibUzgDtdoeCBuQKBZ+y\nNjnEg/aWnBP3dhjXdNntHtSUFzo3W6bd0pTrW2vMHNFeUa0VxjX46SQOfnHEFw0yUnyjUaWrWnvY\n61PQIM6YKYCasH4AW0qoTwY7uHb/Juljf/pv8Nof/hMA0qjDQDW0iu9graytWtXFVa0vjkM2m6Ip\nnWikrLZEix0vGVodTUuujvGFV0Xzeeq0z4aa8uU4Za0lxxdmqsSaQNAbRFzRNNX3nqmRpuo2cRx6\nAxnL0YpD1q/OphlbZr8YDrtZw8t+v5/VN8CxmZbpmFJm2sexpa9lGz3H4Hm70fxR37ZSYGBUedGL\nM56u7/kExRELCXz96uMzdgAAIABJREFUHKYxLV2vQdnL2EmlwM86mnR6O8RaBjQIilhn/26UV154\nkV/6zF8H4B/8w/+NI7OyFgtVj1eviVb6p88Vabbk/Vlaa+I4WuUuGeJrRbfp+2r8zpektbxTLTOl\nFlin3+PSJbHkyr7Pg/cLE8qkLn/wu1KCc9IrEJ8XC2zz0jVWtaPJzNwsaihSBHrq8rv16jL1iZHz\n7QfjQEL36iXJVXYLuy16qtUxyto5gjRlQzu5Vmv1jJmQWpvVTrU2wR1F6fccd5x0tyCLTRiFd+M4\n/pE6/e4Hnh/Q00wpxzOZXzPwA1ZvC51q7tgxauqCcByy7B1wiDXbzo1jxkZdBno9mk1Zxem85cHz\nUl3+pZdfZnNDHlJkyTopb7R3svoDR47MkAzUv2vcrLBNEscURhtWoUA03L+h4hqoeuKjpRbhjOoe\neB6xznVnu50lI6xF2xLGBwI/xiqRfBgOpeo+sHDqWFY/Ybo8S0EpVEND1iUgtsmemgOWotZXjUzM\nQDP8/HIZVwVEGFsc9XEWgojkAMaYQ0qkHSIKrkeiboSCsUxVd8sfjgSKTQ2BRv5jG2cby3bkMFvT\nTshxxKPzOj9hD5soO8Am1LTj7B+9sMy5RXHLdPsxi1Py3Xg4yOoWl4ylpu4Ux4FU17pjU/wDtLEB\n6PXbVBx5lsZNiGJZQ5gSBX3GjvWyjSCxMQVvt2hyqOssiT2qVVnTNh5ih+pSYJDVPomTmFjXIoY9\ndR4C6kojjGyXqkqhxFpC3TRLQZDFYIDdZqj7wOpKIjswsNG0BCozHnvoIbanpUHsys2bpFbu+dXX\n1jlxn5zfGSZ88s9KDYQojXj9prgFFhszTM1ookQv4qmPSbGpdnuNpSVphHv0yDE+/DFRLqvuFC0t\nLDRRH6dWFtfYN1+6yHgwapdkmTsmStRG7y0K3qm7ji13L+TIkSPHIcI5yO6TI0eOHDl+NOSabo4c\nOXIcInKhmyNHjhyHiFzo5siRI8chIhe6OXLkyHGIyIVujhw5chwicqGbI0eOHIeIXOjmyJEjxyEi\nF7o5cuTIcYjIhW6OHDlyHCJyoZsjR44ch4hc6ObIkSPHISIXujly5MhxiMiFbo4cOXIcInKhmyNH\njhyHiFzo5siRI8chIhe6OXLkyHGIyIVujhw5chwicqGbI0eOHIeIXOjmyJEjxyEiF7o5cuTIcYjI\nhW6OHDlyHCJyoZsjR44ch4hc6ObIkSPHISIXujly5MhxiMiFbo4cOXIcInKhmyNHjhyHiFzo5siR\nI8chIhe6OXLkyHGIyIVujhw5chwicqGbI0eOHIeIXOjmyJEjxyEiF7o5cuTIcYjIhW6OHDlyHCJy\noZsjR44ch4hc6ObIkSPHISIXujly5MhxiMiFbo4cOXIcInKhmyNHjhyHiFzo5siRI8chIhe6OXLk\nyHGIyIVujhz/P3vv9iRHft35ffKXWVlZ1dXV1dXVFzQajUYDaFwGxIDgcDgcjsghNeLNXJK7S5HS\nyivJu9KG7LUV6wiH/egn/wGOsCP8IHsdCl12Ja1WErlc3pbiZTgcDeeCwWAADC6NRqPR6Gt1dXVd\nsrLy4odzKhsKe4FGOAJPeV6mppCdlb9f/n7nd873fM85mWTyFCVTuplkkkkmT1EypZtJJplk8hQl\nU7qZZJJJJk9RMqWbSSaZZPIUJVO6mWSSSSZPUTKlm0kmmWTyFCVTuplkkkkmT1EypZtJJplk8hQl\nU7qZZJJJJk9RMqWbSSaZZPIUJVO6mWSSSSZPUTKlm0kmmWTyFCVTuplkkkkmT1EypZtJJplk8hQl\nU7qZZJJJJk9RMqWbSSaZZPIUJVO6mWSSSSZPUTKlm0kmmWTyFCVTuplkkkkmT1EypZtJJplk8hQl\nU7qZZJJJJk9RMqWbSSaZZPIUJVO6mWSSSSZPUTKlm0kmmWTyFCVTuplkkkkmT1EypZtJJplk8hQl\nU7qZZJJJJk9RMqWbSSaZZPIUJVO6mWSSSSZPUZxH/eOnj1nJkek5AO4vPiDn2gAE/R5JFAHg2h5J\nPwTgH/6LL/G9b38fgMZym9JUBYCW1eMbv/wZAG6v3Kfe2QBg9YMG7pDo/VK5jGv2Py9vLALgUU5P\nhnZQ58jwBADFfg+3VALgaO0YwzUfgM7WGiuhfP8//7v3rINMwnf+u19LbCcHgHEc4nYTACtnaHt9\nAJb896i3VgAIm1McLZ4G4MjYDG5efg/HEJpA72roJg0AoqiHcfIA9GmDlcjnZA+LWK62CkTIPNZ7\n1ynZMk6T5KmaEZlrM0ESd+We4TbofP0X/8t3HzvO3z10KPm135Znc7w6f/F/uAB85Z8W+f4vygB8\n9swqljwm7fYw3/z38jxf+dosnZ4886XvL1MteQCsdCBG1oSJbcK4A8C5zy7w2l+/AUBlZILZi3L9\nVqvF1IhcY+w+cUF+99Z3LD71Ffnd/MhL/OSbV+R//A0++XlZov/gf9957Bh/YzyfWJbMSWJcPKco\nc4VPbkjWouuU8TuyVkqlEnFYByCIfbxiFYCw26Xf1+c0CUbn2cIQyzQQx2DbMocJMXFP5iqIQuJI\nLoqjgMSWCc2bHL1eW54nDgjDUO9vYdmyl/7NVv9A6/V//cqJJMbS8bhAXx8KYhIdDxS9IX1ui0TX\nGQnEsf62ZbAs+T5n52j7sj7COMJzZT9YSYLROXW9HL7fk2vC/fvkvCL9wNexhRDL9WESk7NVxSQx\nJPI+/vtvvvPYcX7j108lg/dUBHyj+9MkFPV52nFMLief446PlZPfsixDTj93oxDjGP3bHMaSuY4T\nsHVKelGMXoJjxQSWo59t4kjm1mDR1zm3LEMY677V+ZU5DyjqvP2bP/7gPzvGRypdNyhw7+qS3Nw3\nRFPyQ4mVUJmQRez7hpIjt/nxN39GY10W1ov/6DmCNRlgsWrx1z/+kXx2cvj6q5WqQ7Mn9xyuTTGi\n32/sPMBpyUYN4x6OK9fkyx6z4/L99bUWnz12CIDt9fe5eV0Ww+FqjVK4+6hh/b8kDiP6nRYA3vAI\nZnhSxmwlDOtLOus9z6pO18TEJ3EcUbSOyWNZongiq4WjCyuyAvqhKF2TFMkhz93HJ0IWE5ZFkMgm\nSZIGBtnELiViPdSqziFcCgDYuHSNHAgFZxrrCRyV8pBLHE0BcP1SlU5PDrUbD85SPSq/dXU3oujv\nAHDfTGGzDsC//ZMlfuufyLPNX/RYviwLy7Ha9BNRKp5nYXvyeTK+Tqkg80Boc/v+MgAXn32W3eY1\nAGpj83QSmXM/3uG9W7KBZyZ+TtQflunBp9U8euAxxnGCsWU+TQJWTp7TjiPiUNZQbMUYVUCWbSCW\nd2oiQ9Bt6Y1ETQFYBixVusQW+jUkMdFAcVoQ6T3jOCFOdGObJP0trP17yv/qnjQRibX//UEkCqEX\nioKMc2AS+Y2IOH3WfhjS0p82xuAaW8djpUoiTvoYM9ANCXlVVE5k0VOF1wl6GFsVXttKDx0v7xIE\nMv4w7hCrEoqJsU06RcQ6tjCK6ff9A49xwivSSeRGnaQPHXnO0pBDEvb1OWNGHNlXXTch0rHbNul7\nco1JD40vF4/z7dYNuacVQzJYKxFGD9AAh3jwXh0b48i8JYmFpYeMY3TtAHZigb5j2yqAXvMoyeCF\nTDLJJJOnKI+0dKvjNVp1sXxanRbxqprUxsKZFlcseLBKqzawdnLYnpwYa7e2cNRm31iEmXG5fqW1\nB025z+mPnmdtTVz2YqXE8pUPAOh0uszNzwJw/e3reJ485t5Wj1uRuH3V/BCvvfVAnseN+OwFuX7b\nhJil+IkmYX31PtXaOABhP8AbktMtbG1gu2KxxZ0W0yMfl0nL1VJ3zbJyDE66ftwhp+52lPgQy1xU\nnJn0VG1b2wPbh1a0RckcBiDBZTdYkrlIigzbM/LZmsCPxbp1iQhTV7eCk7gHHmMuP8TNnSMy3o0P\ncDyx1NfWNrHH5InOHv4Yri/P6Tdvs6fGaitM6HASgMNHA5auyHMSeoRBR6/xePEFsTrWg3lqM3cB\n2NmIGbj8XX8RNSLY2ItJ+mIR9vEpjss1Q7VDDNliES18uMKh+fUDjzGOxZIDSEhSTyAxNgMTrd/v\nk6hlaBccklivDyxSgzNOsMxgaySktolJSPQ+cUJqxYZJMni9kCQksYzLMmD0bw2AGVjDcWrpWhZY\n5gkt3djCMbLnwn6EGZiWlk2kFn3XD1lZlnUzO18hr3sxAQK1kmennqfti+XX7AXEkVpylkNXrclO\npweOerhRTC4v82KbfRe71WwSJ2qJFgoMps5Y0GjupfPi5IYPPMa9fkxO141JbE4fmQZgaf02jiOe\nXy7n0I1kLDk3j6vvoxX4ODrvRa/MJyPxytduf8BXjn8EgH/XfBOjcKnlONj6W0EcEscDOCJJoaLA\nssg5st/iJCSyBpZuTKjvu+aO0Eqajx3bI5XuXqdHV12CYm2Yjk5g3Epw1PR3kgT/nnwfxhbWjExI\nvdGisbQlf1susfDCWblmawu3JQ95Y32RYlsxmb06sT787OwsflPggr4fQSyTk68UWN4UDO7ls89R\nNaKwK8UZblzXgc9PMTYbPXbgD0vt8JHUBenUN3Byom0ct4xR98Uuu4Q9nVA3TJVuP+nhoIeONU6c\nyGKt91aYyM8DsvESZDwmSehEAjvU+vPsuTJHO8EyJWRxTOU/hGcJdEICnuXpfUJsxXQDWpCUDjzG\n8apLFMg8OtURjlVlA1x7/yofm/80AO3tJa7fvwrAXLnM9aK8y6QZsNnYBGC0YhEZ+d0g8nF1h43N\nzlCd+mUAKq7FzOyHZd6CLt977V8D0Gz32N2Vec4H2zTWRGHPHK3Q0cXadwJCV8Y7e+YV4v5fHHiM\nxBZJqMosZ2NSxdkjVAXvOAZb32mx6LHny5x849f/eeqa/+kf/0GK4xqTEA8O2IhU6ZJY6fdJEqcu\nO5YoWxDFHCk8Zex9N9fEKcIKlijtJ5FWp0c8eA5jiNGx2TaOLc/RD2Mmp2Sc58/8Cm+9820AbMdm\neuKjACzefZPySDG9T6cjSrc05GIpZJgYi6SvuG/OwWYAX0T4gSi8ThDiJArTFJxUGXd8n25XYxwJ\neNbBjaF+z6enuHps2zS2V+UzORwFY1v9Ho7R+XVcAn2e6eo0jh7uW0Gd1rbojFv3mkwfEiMydhxC\nhUdi41B0ZN6cwNDzdJ8Y6yE0KZbDG3DIEeqhVC3WGCA09W6dkNxjx5bBC5lkkkkmT1Eeaem2lzYx\nI2LtOEcschraHlk4RP3+GgD5SoVKUU7U5s4afkNPp11wQ3V/ixb+dyWa/eLwEKsL4jqPHTpE+75Y\nekPDZYKunGZrW2vUKmJNjU2NsrUipxN9C/Jyyj0/VOLyspxy3//5Db7w8RMA3Lm6ysqQWJK/ecBJ\nKJVrqZVtY2ENDOWcQzKwYJwyjlvVsS0SjIhVmsQhlloaxspjJQP3MyBnyRiipEuMBBhLdoWWshou\n/ce/wx0VWKN9eonJyqdkTu0xLA3wdHsdrr7/EwAufuQVhpG568VrBKZ1wBHCxvoupaoEtJy2YemO\nuJV532KjIbBOoxdgV+cAcEvLBIovHJ4xjJWPAxBGm+QcgXUcx+H8hy4CcOTceaJQ3r0Vezi2/K3x\nXD7/8r8E4D++/gdpULTT3INY1tbEbImhGVlD99ZuEvblt67ff51T1YNDKCLyviJSRAEsJ2WM5HI5\nLI14u26OkZoE6gI/wFY33ct5aTAsoo/RG0UJ+xZmYu0HwCwrvb9jQajWJvFDATPLSq1nkkT+Rh83\nORBn4SHJ5fYhEsvgDFgUcUygAdjIOGh8l3/7F39F3pf1N390hveWfgbA8IzLnjIWLEg9gyCM6AV6\nf+OSpPCFlTJmOr0+rY54b05i0dcAUjcM6anFGZMwMip7pt8LeBLQL7EC6l3xso9NzXFjbQmA8lAZ\nuvq7lgVqkRPHlFMosMd2qCwLYiwNhk3PzOAolOPGFn7KRPGxNNBd8kaJaej3TvrMnuMOwt8YK6aU\nkzlfqa9RURZVElhEpvfYsT0a033mGOHKPXmAvTwzp44BsLO0Dequ7a2s01WlWygP4yTyo37QwxkT\nStCxjS0+cWpMn9imtimb9o1OH+PIRl25tsgz5wWCuHltiViVTrlWxpRlVTaWdvEmRPFv7GxR1Gj5\nyx8/wo1VgThurd2jNv1kmG6c2BjdNIXSCH1fnolwP9pqEYG6paY8z15XaE2WMbTVbSz0i9jqggTJ\nLr1IqHGb/k0ST8ZQMTMsfWcJgOLQEO8oPvq7n/wfU/wv6sf4fVGoP3/9h3zm5c+nT2EZUVQmKRIN\nqEIHkGa7w8zwM/K5tUHflfFO5iY5XlC2htngVlNpZZMBdiJUtZnjFn/4h68B8PyvFOiqS94P+0yd\nlnfmd7bJDRRJ30/d0whIbJm3L37sv+L29l8BcPPWOt0dGePO0BAtS+bq6OFZbut8Njt59qpfOfAY\nJXauyi+JQN1ZYzsMVLft5nnprCj1KLIoTAmm3txrktfN+dWvfp3ma9+TPxge59WVO3JN8yFWTBKm\nOIJFQjxgodhCLwLRUYm62pYx2LbcX2DkwTV/n9VwoFFaYKuCtG07hTYiYlxlbHiWxcDvPfNsgXBJ\n300UcOy0YPu+aT0EU0AYKusiTtL3V3Q9+g+NwRnEFJwcxaJQ0hLLwlFqmJ0zDA2YDCGpMRMndoqb\nHkS6YczZQwLPre2tUczLGnKtiE5fdEzeGaKtCr5oQhKlCIYmpKd7t2ZGuLsq763tB0yUZLyfLU/h\njcj6/vnSNexZoWgudx9Qyote6YUBieLfbSeHN4AODMR66DmeS0fpcsZ1OAiCksELmWSSSSZPUR5p\n6Z6dm+LBhJwwh6YrrLwpgauFlz7Ge3/1A7moaKjM1ADoNLpc+AcXAHjtWz8jeCDQwadfOkynIadT\nN/AxCrqf7eyycljcu1Mzz/HW0iW5pVtIXbFdv8nJUTmFFss7hOoTdMKQ5Q1xA95cbnHiGbFlDs2P\nUVFr+KBirIiwK5ayXSxj6bTEYYJhkOxgp98nVsikI9H8leB1yMuJ2WyuEmlcwirY3GmIdegMFbH1\nb4vM0FJUwIw45Jz9qMsgINLt7OJopHTjwRYoMdyKbQnRA3biYWuA7SCSK7q8/eOfAzB7YZ7ZZ88B\nsPvuCpW8BM/CfEylIfPw9rsJdk4s6YUjPm8W5P25VplEXbTP/erXsDsCM3V3m1jjEvyL4z7Rtliu\nw9UZQl8+hwxzuPgyANf5M5K2vKfd7i4jCuls9R/A8HkADnk1hp0nsQL3OagGK7XicjmLRH3tOLEJ\n67Iu/XvX+elPZT4/+49/DU8j8z/593/GC3PyIoORUT5UFijp57/4aZrUEMc8xMFN2CfwRhi1+BNj\nEyUDlouFre8UY2GSAavBsI9nHXCU5qFoHRCqa29hUhZFP4oHSABhGNH05TlmJyrEmuAQFy0se8BD\n3b/e5HI4A5gsTvZ5ymnoEIxjpV6dMYaEAR83ZEDGiK0ojew7OSv9fKAxRobFLdE3ru1SVg7xPz39\nX/LXl/4MgF0nYUgt8vnoMKuJvNfV3l5qlU40bRa3RE+cfmYWX2GEvY1NgkQs4CP5Ua7sSqDYyecJ\n9fnz3hDNpuwB1zKE+i49y8HXRKaSA5u+fB4ilzIoHiWPVLrvXLlFe1k2zO0QqMgmWbnxHwjjASYV\ns31FJidx4LW/+SkAX5+rUR0WrOPWzSVcV37KdYsUxzWrzO8y19MNcH2Z33nhWQB+vLPDW2+JIjg1\nWmFrReCIMSvHV175LADN+9s0WqIQX7o4iqME7nrOwuo82SKOujuEfSVcBw7RAOeyDYPMH2MskgHx\nOewQ9cSlmBn7FPfDvwOgNDFNR9kFjskRa7aH51RBEaHm+horW/KyXT9EoTO+972/5uI5cdXt4jAt\nXXCvfPYjxGFbn6EAsbIgMBiKBx5juVDk3LTgwfdGXSaHBfrZ9m/x5h2557kjVc6dVWK4Cz9tyvu7\ncb2DicSVvH1nkw9f+AQAQaPOxqrgwd/64Q3+2//pXwFCqfm3f/g3AOwFPX79txUiaN7DUtpULnAw\njtxzci6CvCYj7A6x8UAU+eFnS6zf/eDAY8REWGbgwsfpAWVZLtHAi45j3liXTVg5tMDzF2b0Hwwo\nFnvyhU/gTAjkkjPgdOSdnjxxhqvX3pN7kpBCBBgcVdgJCUEwyGZzBmxCjGWIdY06Zj+zzTIoyfBJ\nxEox3Tgx+1jxw1cYUqqe63opjLe2tsW5Z+VQW27fIYnkPkE/2MeZSaFbwKSHiGWZ9PrYSlLQPDEm\nhTKiOEoNhiiM02f7ewyPA0gz8BnRyWs7DidGBRrzA5+vXvxVAP526Rd8dFpYMv5ek2mFBXdbD3gj\nWQKg22rz3GckG3Z3bYlaSeE/t4KnsOitOys8r0o6bsH46Kg889AR6kXZq7/wlwmNUurcHG40UK4O\nFVf0TS8OHzqg/vOSwQuZZJJJJk9RHs1euLXBxPPiRm/cvokZRJKnhqmKgUq5WmZpS4jwv//ho3R3\nhW/Z6vbY3hY/ulAq0NsVSy/sd2BILYHI0KmLhVkqVbj0HbEYP3JugWuSscp46OAW5Jrl+z0e3JD0\n1b3tBrOHxO0zDqwqlFE7OoOz/niC8sPS333A4l2xLKdr03glYRTYjsFW1z6OIqxYxpCELdp7Mrby\niM9wLCej5VaJQ3mOVtQg5+6fabVIYJf37r1OTvOz4zBgakqs1e3tJlcXbwPg5QoMj8gpXG80aOYl\nQeCw+wyoS2TZFQydA4+x3avz9nV5tumJ4+RHBRKJ+iHNvUHgoI9TVNcTi3trwhrJ1wqYvIw96OQp\nD+IJYZfDp18A4PcufIb335Xg4tmFw/zqP/sncn1ji4BBIsZ15qoy3k9eeIXvrF+WMa6uk9fgp79l\nSNQaHqHB2GTtwGMktkCtEYOBRDyWhBzGDCL8UZqLH7S7lMpibTuOm76vsbHRlL3gJBauBlYOTx7i\nxu3rAPR7+27kxOEjTE8LTOaQsLopa2n53mJqMcZJjBm48paVuuCJFfGENF3EyJQ/sg0Pm6WpsWob\nJ03IsR2HurpUM9Uytz54H4DKydH0mgIu0cD9fyjJJIpjAvX83LyzD5cA8eDH4jhNs8aCtnL77Ydq\nFJDEqcV8EBkv5Nhuy31G+zFzOVEIXb+TphP/0sxHuHpHdE9va4WxcfFaRkszjO+Ihz49W6PVEJ7u\nh+bneO21HwFwfuEkLb3/2WPT3L51S353YpLtzTWdqylavlzzUTNLpMkjw6bMD0JZB+K1KH/XDsF+\nvPf5SKXrLVSxlXrh5PKYB5qwYLdpaFKD74b8y09KlkdnYxW3JNCBzS55T3bn6tJ9Ds9JxPT+yhrt\nuiisnBvjKibTCVwczRBZvHqVb5ySgjKNpMVRIxN+Yf4ZltdFcYwdGqe6KwooaPtMTEoUemOnSd15\nPG3jYWntNXDMoPCMnbrwUd9KoTOTkFKi/PYuey1x+YfHfcpDQldrRGvEKQa8y54yEIxjYVR5L5w8\ny+0Veak3l+q0fHFdT8yNsaTUuM+8eDTNGjo0/SHux0Lxue3/kIK6aG3HYCy5568cYIy5oSKWLogP\nT0wSRjKP/SimpG7cj6+FfE6z6PomoqYK6eaVNSKNlm/cCQiekfkJOmtEChsN5ef47g8k4l+wP8vs\ncckgcsoOjmb4nDp+lM7OtlzTCnBzmmQRrzAeyX1u3tiiUhO8OQpttuq3DjA6kThJsElxhBQuAIuc\nJlwk2AyKG1kk5AqSJOI3W7ixXOMUCmm9AYIujifzNjU3zzdmhMHz7R/8BwaI+sn50wwrlJYkpDUM\nWs06dcWPsdh3tUmIBjTDOMZ6Qs5YEIQoikJAghMOoA1w9eAIw36K+4a9Hm2ld60EK3zsvOytnWAP\nRw8Cg4VjBtmUD2XeRcm+UrdsQsUyrQSS6OE6A6qkoyil3oVJQqQFY5LE4gCedyrr223GSjLve8B7\nW+8A8KHiKeo7sk+mpo6w/I7ETZq9IFW6XsHjU6OvAPDuj/+KKzeFilp+aSGtNVGuThBvCjNrdXWD\nnYa8j/yIzaV35fvZ46dp7MoeHqoYLGVK7DZDno9Fz70TNwkLCikmDu4BmCgZvJBJJplk8hTlkZZu\nuNRg3VFXPTSEAyuu5lHQ08wlwtHTw6rOsHRLzP3yaJmuL5Hw06dP0GzLaTB38jgPlKcbhjnaLTmB\nR9yY6WNiRbTqWywuy+nU7wRsuHKyFbwi5WGxTH76k3f46isfA6A316V9T06kEyOH+L+S+080CcVS\nmZ2WuuqJNYi/YDsJcaRWs5UwiIrcuPw+82ckGBHuNXEHlZQKPmGsqdJWhGcEphhrnWGzeROA4eoh\nFo7PAeDHDkVFbOrNFhM1gUveX7zL0QkZZ7MRc3z6JQBWeZPtjriuJfcIrW77wGMcOlajcUXmKAxa\nvHlVklVMboTVRRnj9PnDfKCMkChp4ftipUR+HndaXOzibkwcannJlk8YyfN0fIdn5oRX2Wo3aLXk\n+Ttbt/GGxctJem0itfw24jDlmiZ5i9yQ8I89q4+la6WXwLFDzxx4jEkcC1EWSbdOA8kxqXUbxYYw\nEKtmpOTR92VOWp0WQ3mZfzvvQU7Ga7kFkoH/Hyapi/zFV75IXxMR2rs7eMojTZKIWEuLjo2OUq+L\nV5MQwoBtYqyUvxsnyRPzdK2HeL5gCJRREYYBQTCotOWk0XapJaHP/YVXqN+TPdrcaeO4g/oGCXmF\nUbpBL2Um+EFvv3Rh6BAnod7fTpkc1kOhQMeYtOYKkAbVZG4OPsayl0fp4JgkYDUQiGB45RpTc2Kp\n+36bFz/3ZQC+982/3C/zGARY6oEt3lujWpMcgZLncu4ZCdZvb9fZ68j87O40mZtbkOtXN5iekrUb\nkEvvQxRT1LnK5R3aXU3Sciq8qxZ/wwno9rqPHdsjlW5cdjFKLylfnKGmSmFuao4ffe9H8tnvsroo\n5HEKJYpD4hJdKowpAAAgAElEQVRcev8mHzo9J4PqdOlrYQonNByuCVzQbPvUlcrj90N6PXVbg5DD\nY5LJ0hlJKGoNAGcoDz1RBF/9ta9w5TVhSlSPT9GpySTc4j6ty1uPHfjD0tprpXUlnKqDpS5UEoM1\nyPZJYizduGc/9jwo3b4ftKAhuE/FHWNX60HEccB8LAVymq11SiOCTd784DLjCoXMt3w2d0TJOQYi\nhRSSIKDe0oSLYo5AsafS6DgtX3PQoxjvCZhxG7evEviCPV9ZWuRjz8nC+v5SxIgti3I23+DOpvzW\nM2crXPs7wcWGTIlDowI1RMWA0ZpAB3cbt6jo/vrzP/kLRkfk+/bmFj/64asAfP6lOTz1QvdaDfpt\ngYSW3YRuS+azTI5WRw69kfE5vEGmWu0WkX1weCGKpLopQGSslMifRPFDGz4h0jVtykWMrsv61nU6\nHdkOCwsvpqU+c56X4pXGcdI60mEYkPdEifaMkcWCYMZ5VcCj5RpaCoJ8wn7tBeOkBkxC8sQFb/r9\ngLw3YAjs1wFxHCdlbPi9Hp4qURwI9KBp7e7RVAPDrbcZqcg+a7dbTE4Kzm+7w9z0ZQ/l8zk8hUsk\nSUIZGE6OaADBWA8RHxIDD9GmwsHcGQijg4+z7UTYA2aF49LvyrqpHjpBrHWbnXyNq+9LHMFv75Eo\nhl8qT1FSZkJtdJgXPi4Mh97qLXBl3fdyJSrjsqZHhlyuLaoxsLHKlz4tWZavX79LrSRjzw85dFty\nzeZai9qErHXLODxvBGr4IQ/kwH6MZPBCJplkkslTlEdaunPzsyy+ITn69at3mXhBrcm3+gRa5Ph2\no8+cGFCMFfM0WgIdXG/ETKu1MOQVMaHo971Wi6ZmB/i9gNzAWrAME+NicRWG9ot4b25sc/LUKQDC\nAF599V0ATn25ydCLApxHxqHb0zTEjRZnT0092STkXHpKgo7DNq4ziEDm0mBEEodpKcL8yGEsTXus\nL11hUyOls+UXaOfkNDyX/zWadYE5EkMa9T18ZI61Nfl+enKcbk9O7XbLsK1g/kfP1+i2BNb58ev3\nObsg7vnpi5OUPC0eH4QcHn7+wGM8fOoF7r4ttRfCrsEMkkx6uxQ1dfT08ZizC2IFvn8/SpMLPvG5\nizQfiIX90a8/T68vFtShsy9x75LUhXjh2XnmjzwHQG2sxLvvSlBttDZOfV3+Nul3mTwvFc1aK+/w\nQJketZJLosyQKOzR1yL0eS9P98HBC18/nKJgYdIUVAwMiLoJ+/xdiGjrPFcmFjA9gbE6rWZa9DuK\nQpycQgdhlEIBxtiEGljJeUW6TXF/XW8IR62dQzOneF651FfefT01Bx3bScOtmDgNPB1ULENawjFO\n9rmwxjLEmkySM4ZILb8oisi7YhlPTU7y/hXZQ88eP87Wpqzd2ugoGxvy+cSpE1woChvjZm89ZXLY\njpUasUkc4+pzx/KFfL9v9Os8yX/j2GA/QbH26vAEJOJ9Nuo7BBqM3Qr3WP6ZQGNf/MZvc+OyMJ5e\n+eI/ZHpmDoDQb6UV0Lq9iHu3hRV08vg8K3dl7/3Vq5f5+i+LJzpTG+H0WbH4N7fWCLUTzMsXZmgo\n86Fe36Svnngh7zCUH1j/pJ7GBXOE99TTfZQ8Uuku3r9DTik+SRhy/UeyaUsLNdiRzdCLYKYiC7Tl\nb7Hbku9j4NoVeYmTVY9AGQH5kQr/4GvfAKC5sc2f/omQ6EPgyITgIZ1Wky112fuxxU9/9gsAotDj\nS7/3sjx41CfUROfFlTqHtF7vyOhp+o0no4y1djcoawsa47gPJUSQVsTvB7tp4ZOw+wBvXBR7bf4C\n8c03dTwrnDn6dQD2mpt09NDxO600scLv+bTbgsU+WFuloMUy5k9Mc6YsCSHvboyRKwgcMVtcTl3J\n8t4QW0VJahj3Jslx8NKOO6ur5GJZiKsfNPmuFuyYfmaG9SsyX0EnZLOl7vlOn4lJcfOHrBJT5wRX\n7vlbVCblsNv6xXcZm5EDcWfrDvdW3pYfK51gRAsW7W1tU6zKWGIzArY8/4cWPsl7b/8b+dvdCNvS\n+dndoVyQeV5fn2HSO/i7tPc5+lixvU/kN2HaVcCC1L9LTEJL6ykMVcdSPNhxPTrKTik4eVDlHUVh\nquCcXI6eMk8KxRK7daWYmVxah6BQcDg6K8rL8wpcuy6JFY7jpjVwHdekNVsOKokVE6pCxXrImU9C\nwmjQfsdOMWfHNnz0osQgVrd2OHtOPu+u3WX6iNShTqIQX5MLOnt71HdkXhaOzGJp4OFK/VbqGlsx\n9B+CEVL+RSJMBUATOPZLZO5XIHq8bD7YYKwoEzNanWJzTZRfM9ll5rjgr7s7Db72678FQLftC9QH\nFDwPX9snPXPuDLFCWkGnzdwxGe/vz06ma8Vv7VCZktjBqfkb6RiToMWIp7GriSkCjVHlvCLRIDM0\njBkUEJ4t17jcWHvs2DJ4IZNMMsnkKcojz9j5yZOsduSEmazVWFsV0zx8v86/+GWJVD+4vwEFsYhm\nKuOYgvAwuX+fPXUZxxKfphosX//iR7j6M+Gd7u3W+dTLwskcPTTP5KhYR9ff/DlXB8B2B/7V7/xD\nAEywy7K6ud5wmY6j7t10lWtNOeWSxMN0Dp40AGIdVdQyMw85qTGGnU2x7vu9DnktHTdS8Qg0VxvP\nwfPEkvPKHpZabH2/TbutfdfyHm19pl63RaEglkO7DUNa6vDBRp2N2ssArG7dYO6oWJOzz36CiZqk\npK5cf4v8sxK5tSkSJwc3kTr1XaqalGFqJeKagP/NeoNYEwr6sU+7K/csjRzGG5Hnr9ZqPNgQ17s2\nUaHVlNO89MzLdJvyfbhxjygUS+DBrUXcnFiNxdEJzLB4IYXiSJrz2jMOsVZJ63XA0SLSbmGUhRdk\n7IXKVQjnDzxGY0GSdnnYTzqNE+VfA0kUpN8nYUy7Lc9crI7T0/fe2FhnpDquz9Yh8bSYOzFmUG/A\nkqQDgDDwibWWQpyQFtO2LJMmwkxUqzinxZp66/Iulpb9zOXj1PU/qPTDkKGiQDwxSRow7IdRyhxI\nYvDyg0BXRKClHUvTkwxqru2suoRq3YdBJw0MYtm09mTvvnv5GieOa6p0EjPIfnUcK+2SYhnSSmTG\nSvYtOdvGDwasoP0qaweTmIayI9xGi7ipDI2yTb0huuGE7XD9PanXcvrcRfodmfe7H9xIreHv/eCn\n/M6vCcPhyuXLnL8gVn7U9bm5LOv43NlzKfNhZnYeR/cDlps2uyw4btr/zBgnraPh08XR6oOx3+HM\n3uO7YzwaXrh8FSeSJbq8vMmgO8zv/+qL+Bu68YbLtDpaS6BaoVqTCOg//1TMH//tEgCNvRyf+6wM\n1m/Wuf6+4MQbTZvmNVHSv/5pn/W+TNStxQeo58ZvfvUlQq1AUe90iVU5BqHF1duiEK9sNCiURSFa\nVp9XFs48duAPS6cXUR3SSK/tpM3oAt9nRIvtvH3lBmdPCOvAIsRSVG5ns05eq9z08jFRU2tJ+D6h\nunc79c10Qbv5IlcvSybWzLEZXH3ZY9UK9+7KfFUqJbQUBLWxQ5Q04aQzdZy6r+UFky5DQ4+npwxk\n6nie1Xcls821xwjW5PC6cOIsr74tB+vGbpH5Q7rg3E2U2cbm8hKOdmGOCbGUTbHm39NDCvx2i3JV\nCt54XpFYK/qHcZ/SoLMsVtq1IQn9tENycbhPbUzcvuW1gClXYBaPNpb77oHHKF0b7PS3Ynvg2j50\niTEMHLwwDFPsL+cW6epmDkyU5tA7OXe/PGEYgpbE7IcWtvqnYRITDxIR+v2/RwFLegN331Auy4Yc\nL1foaRdsy46JkxThPZCMVodSVz2Kw1T5G9vQHyQjEBJHsmFtx6ZS0c7Lt24xMyXvst5sMrMgey70\nmxRKEs3vdNr4WuaxOFRkbV326NmFo1xvieHV7vgpuGDFcZpEZBsnxTjDON5vmRSHxNFDYO9jxMl7\nJINGmUHAUFXWzereDhOaFPTDb/41Z5+Tg8xYcPOSKODxuXkCzSSzjcMPfibfTxRCLJ23oXKVwxNy\nTbezTqTZncYtkyQaX4gCTDSg43UZWAw5r5Ti1q5x0rovRD7Hph7fSDWDFzLJJJNMnqI80tKtnV9g\n622xSulGfP154a/5ayus1wUv2K43eWZBikLvrW0wrAGpoB/yX39VoIM//av3WN8SK2K8lGdTOaie\nk7CmVuz/+cNFfu9L4oaWii6vfEwCVau3btAaFkYEx/J0ta900bWZPiPW0U4b9mJxc48cmuDa0tIT\nTcL99T2cMXmm0sgY1uCkDlvY6opVijaOds5od7fTfk/t7SZd7eBQv99m+Lichg/WHlCriNXfardS\nHqqxYsoVcVdDv8XOrgQYi+XSfnQ38BmryKvJOTbGlt81+SKTlnz/5+/9lH/y4tcPPMYLJ09TVKbE\nrRsuz9fkPlcWFwlDeeb1DZ+ZUbGUrGSYnYaM6633PuDsWXm2ZjdHwWiB6JFptps6rnyZglZwGhop\n7wergg5+T/7WM1ZqBRlj4fTl+vmZLnfuK7/Z1GgEczInDy7juU9iF1gYtUYk5UCBhMQQqddhhQGD\n0FPYD0gGHSJig9F32uvUUYcFtzic1m0wJqGnwVFcj0RhAcdxUrZNGPbT8H2cJCSDwuB2TN6V9zg1\nO0t9S/jHIcET8VdBGre6mryBFRMPqt9ZJrW+XW8o5RdHccTspLBert+4RaUs3uLc/DH8lvDE+1HE\n8LB4bM3mHrWa7MVWc49Aeev3Fu+zWxik8LuEWmnPdfJpsocf9ckrtGRbhmgQzHyyOBrDCewqvLDn\nd8iHsmfcSkxgaeHycoHGoGLf+TzTyp1tBz7Gl3Xz3HMXma6JBW8Ha+QHpSadPIN1EMcxzR154RPj\nh4gVFo3jIVAYwTF9Yt17cRRhqXXrOm7K8klMgsPjrflHKt2td27tRyXzhmRIHqC51uXSFXFJTy8U\nabRl4/3de1u89CF5ubvtFreuiLL83X/2OWxHXvT7b7/GoHHHgwjyujt7xPia4/7il7/Mf/i//wiA\nk184w9UtxQ131rl4UojOmxub3L8hEEfOqlPsyLONlEdZub742IE/LLutiKYWepmxDYm6a4VSha5i\nfvNzJ7FUAfd6Lt2uvDx/uUGsDQC3dvbw9XDZ2tphT/Pdw6BLX5Xu4ZkazmBT9nrsadnAocooGxuC\nEx85MonfESggCoepN8Wlmzp0mmZD8sK/fv4zuPbBscCllUWUB8/MRJWfrwqz5GRpiyXFVt3RGnuB\nzLXj5PE182x3N6Kri7I0NcleV95ZsRhTKMg7K5dy5BWC6JMwPKxFgPwigSqtfMljUHQziSImjswB\ncPWDq8zMyLopVyv4Wp/4zuYsiXaX+MIBxmjMPks/ieOUdZDYpDUZpk4HrH6gmXBhSF/pYL1uC18L\nuwwVShQrtfSeO82Gfj9MvyfXeMVSyk+L44BEM7WCoDtoDQEYUKWexBGOwh2lYommpxSrMCEyT1aK\nFGws7dAb9DTbDamfMIBFgl4fV2lNOeNhlFExUhlKEyJGywVa67KedvaaaamKnMmR07opDgmriudP\nTJQY0ueOoxhb61lEcUJOaYcmiVMGBcl+/V2sCCd38BhEcL/B/KwYVT1cclV55vsbKzQLctdz48c5\nMimKNsaiOCPrb7xY4o/+8A8AmJ89xp1lgUdaPYvqjNzH6tSpzUp8ZPPuDWxbX6ZtYVyJocRhL+1E\nQhykNTiIEyLF/+y8Q94Tpb67toJdfPyezOCFTDLJJJOnKI8+eqwYZ1hOs8Pn5vn2t18H4NNnCnzq\nBXH/jVvi0ntiWVYcaO3JyTBSqbK9JoGu9m6DoK+c3YlxGrEEcarHRynfEffg9Gd+ibGqnFTBXotP\nfFkKD19uNijkBMoYrY6yuayl3EyOaU+sqUuxjaP9yCpOhb2zR55oErZaEaWiWDBhGJIfVKQyUBwW\nq4vQpH2jck6OrWWx4ksLE9Q1lbfZt6hoIKdYKjCsHNzuXsLylljMrVurjJTlNBybmGCsJG5i328y\nMyG/1WlsMqHR107Qx3HFtdrrbOJ6mhe+28cPDl57YbXj8tycFEl//UcfYOY0COQ5eK6Mfe1mh/JJ\nsRxqUy5LWxpQCPoUNK1yZ3WNyiG5ZvnGdc6ckwSN1cYmtSmJchuTEGkkNJez6GxqckQnh1OSd5bL\n5zlxSgJvLRNw5S15r3k/of3uktynUiUOKwceI5bZDzBZCVastSP6Ycq3Xrnm4A7Sp62QoKlp1b3D\neAXt9Veq4mvSRGm0RrsulpJdDlLudS7vUixJcMrOFUlQLmivQ6Q8VTdXhHAA14TED8MRap326RHx\ncLWux0sYJYQtHVsUURmSOe32OgT6e8YytLWxY7FY5O1FSShgKGRNEyIOTZxlS9urD5WGU+5wzrVx\nFCLodFpE2gOs7HmsWjIvbt7QC7TJ40NaxLGcNJmiH8Q4qV1np23LDyKtbotbNyWgmtRmcD54S/5h\ndIxYizLEns/Ornhgk4dmGSnL/nnn0rvkPIUCkh5rWvbVjwJW74mOKRUMe9oIt76zzuwhYav0ez7u\n0GD/u0TaBt5xS2kdiSDY92zq9Tql0sDL6ZMvPv5dPlLpGmPoa0GapZ9f5aJmnpnAsL2lDdt6K8we\nlo2xvlpnta595R+EFDUx//YH1xipSrbZH766X4ymubeHeU7M/XOnjxF2RXmxt0lZN3niNdjWlxt0\nRpgcETdm4+YtvHFRRnO4XNK8+Vdvv80z1pNRcEqezWxNFOSD1SWOHD+t43cw4aCLhEuim9jv7FGs\nynP4fpOG0mt826anC3SiNp4qgJW9B5S1TKLtOtxRqspe4KT1X4NOh7Iq4Fw+R127YpRMyPKi5JfP\njteYOiSKLYz72I2DR4PdvMdlxU2HRwvYLTkEKse6fOQFLVP3VoPRlsyjv7GFhSjXsdom1iDAHrRY\nviO1Nqanx0k0D97EAX0lj1uOh6vNA0NngoJ2hPV3tjGRlnYcHWPqiMQCvv/ad5kdnQNg9W6HojJJ\nmusNKsdOHHiMyX4zB4moD6Yn6RMptpqEPRJ1tU0e0NoLfr+Fm5P1tLyxhKeshlPVceJBSc+eh1Eo\noL79IFU2rldIm1H2e72HOsgW8XU9xP2QodIgmSVJE3Dg/7vzw6Mkn7cxWgMhwtDu7VMkPVU2ju0Q\n9PSZgpDYKEvDlNnVbi13l5e5uSQw1tGJYVpaKnVm9hhrK2IwnTx5ghlVzN/8zmscflGMLYv9ehZD\nFEgUygkIyenEWMRps8sEQ88/OIwyPF6jo3SzeG2NsKMK3sQsHBWjqr3aTDPqZo8sUNZSkM89d5Hl\nFTEEp4/McuEjkilZLFW48tr35dnaebYVz/ZMDzMqrINeex2jhkE3cckPycFq9Vspnh8bh77izZ5X\nwA/kIKo3mniPsWMhgxcyySSTTJ6qPLrK2DCYXTktYyuhoskL+D73N8S19SqH2FsSV3u7Bf/4H0mH\nhB99+w0WxWNkhV2G1P0686XzXPuW8FTDEFrqdv/kJ6+jTRSoFl2KZYUpsLBHxJLe6IWYvKYkXt7j\nv/kfvgrAqz//a07Oi3uQrNgcn36yxpQvf2Ka/K6cwrl+AD2NUA9XGVRVisKYMFCytptjZVUs+rsb\nbZKKnIzNboP1TY2CxlHqrl780DP8p59JjvhUbZRnTmuVrpUtSppmWJ0oMTgDt+o+8wraVys1qhWx\nLrYaLYoKqWys3CV6gqh3fXOTXCDWZ2/bIdJGhdPPRTQ8dUmThHxJrIutlWnymhZ6YeEY4aDltzfD\n3NAgEcBhbVMCqkcufordu5Lmut3oMTkmrp6VGBLljj5YvMvheUl2sBOXjloatp1jbUUsFuNUKeVk\nng+fDNjzHk82H4gFxOrax4AZfLYTIg0KxkEXK9LnNy6aM4GTL9BsaDlRP2BIm1FGQQ8z6O9XyBMp\nb31r7R6VMVlzQwhfFsD36zhG3l1cDNnVuhx5t5RaOGFspYyDxIoEhngC6Qd9qSgmT0he11Ac8/cS\nQgZlG5M4wdJgpuWEKavhz//mO3xcGUmTcwtEHa090dkj0spn99fX8HWtHF84hD/oRmE79HU/DArx\n64/RVavUyVsoqUNSsq0nSI6wA3LiHOJaHh1du5M5j7Wr4mnNHJ6l+0DWX9hr0diS8a5tbaQp4F5x\nhBvXhYH1zLln03e2u3Y3TXAolit0tkWHNYMONUus/9CuYNRjs7w8lgZd/U6LfF7hF38v7Y5RbwfM\nHSo/dmiPtoV345R4HQcxZ06KG5p3DduvygJ9d2uDjkaMJxyLd9/QdiZOkS1XNu3nzs/zt2/LRN29\nehNP7+kXE+JA/nZl8TanX5b6uNfefJOTZ8T1fPtvb7OmCTHf+OJL3FZXL3/W8K1XpSPxJ5//JV67\nLJjP+IzDrfaTVeLvjeSojiue6kARcYevvv0m86ckqcOJE2JdZGtrW3zrXYEIjhwepaLjaXb7lHqD\nzddNW6qE/n0+/bzc58HWOsv3ZaH0g4hQN4xXKNLUuhVDQy79QSnUOMHVyPCRySnCaICLWfQOULtz\nIGH7AXPHZHO/f6PDzIRiq16BcS37Z/k2NUez60oxM3MaGfY80M4OhXyRel1YFkPlEaoVxVzjHq6y\nF45MljE6J1tL13GLclgPVypUZ8SNMwaMumjsnKXeEPxuzGtx/oTMQ96Dd+++f+AxWoAzqDHoOGlt\njkoppqnvJQ7DlN4UJU7qIs/MnuEnlwX3LA47WIrX+p0mG6qMm60thiuCQzt5h7CvzTSx6IdamCdu\nEStcEIVN2i2tBT0xQpzWIdjvJGLimP4TJke4D9Ho4jgh6u/TslxlCHQ6vZS0HwR9TE6e9bPPf5nX\n3vghAJ+4+AyTRT3oV5Ypa4JRYbjEiLIF3r/8Hi2tq7zyYIuxUVkfTs4iyQ2aThpaWi6y6LpSZwHo\n96K0BVK/30+hhoPI3kqTQWuOqYuf58vPC5PByzlcf0cMmHJtjNqovKcrb7/BueekdZTnurzwnEAK\nl997j+Pzokt26rvMHBca69rKEn2FZQqFHIkj+79slWnpPtzzl5mc0vhQp0+kjVR3djdoafsj1zIs\n3ZeDdf7EaUqFxx+gGbyQSSaZZPIU5dGBtGS/hBwGhkti1fzpX75DVTm7rYca/a2RYFY16uvC6IRY\nBWNei7Ao5vhYZYwjlpwkqzWHvLYvPhONsb0hFsXibkDnmpRjW7g4zosvyGlzZ3WV5bsCkH/q2XP8\n5HUJMP3R5n6/Kto2rZknaGYIxI5DW0PaQzjsatTo2PmPsnpPXI21lVWaSrj2EziqcMb01GHuLmtN\niiCk05PxbNWbTEzJHE2MT+23b09sOlqIPUygVhX3eWu7xVBJAjnGRJTV0jLGZdBCK0pCXHXzV9ZW\nCDoHt5Be+vhnePNt8QyOnStRvys3vfT2HJEtp/zsXIOtDXErTp2pEG0vyfyECX5H3muv2yEMZa52\nG/tc5Kobp5WXSsWyVGUHRqYO8+CeslXGxugHMvacazCaDNKPY2ampGuIFzd5/Q2xsF/66B3mhg48\nRHKujasMAbdg0nfRbLmQMgQCbM1Zja0Qp6D1E8KIEQ3mTRybZkDbTJIIvyN/e+LDLxE2xaoZO3JO\n2pYD7U5LUoSRRNG81tYI4h2cknojnkk5tH7QwtUK9GGvRd56gkECnU5ASb2HvOthbOX8JjHNpuyt\n4eHCfonFOE55uvduXGemJlbjjy69yslPiXW4tb4GylTa3fXR5cfCwnG2twQGmp+d4lJDgrGtTphW\nU/N7LVwtZxmEIaGuy3K1nHKfozimVHp8ge+BnLwwh6365vr3vsNHviSJQC7DnNb+ibc+uEmgiQzn\nnnueVl08yJHyEB31bMqjo3z/h7Luv/q1X8cobJc/tIC/IrCDW65iNIja21lm4oTcvxLCxobAhZXJ\nGXq6H3ZbPXY7Aq9OVCucPice+ujEITzn/2dyRAycWBB38Llyj5/+QJrD3SOh396/eUmjsr/54gn+\nt+9Ieb9PjnuUhLDAT5bbOCOyyddXVjk1Jq7tyrU1ps8rY79TZ6gq2OWxL5zjrBFltNZa5y+/Kfcc\nikgzxO5dvsExZQTcLRjGI1Hq5w9X+eHbS48d+MPy3sbbnBvVZoi5HJWSusDV0+S3ZNPUjhSZ0A39\n+vvXmZ0QAnWrvZfCApaxCJSxsLrVSYvoNLa28DuCXQ+VR1G2CXkLQnUNAwx+XV7kzNwModYP7QV9\nckrQzns5jCrdvVaXvb2DwwtXr/2EmXFZcO2V4+QSwbBagceII/c5fX4mvX9vb4/uIKOpH6X1g+PQ\n0NDasbMnztDckENpZXWViSOisK04SN9TdfoYkWa89TqdtCtHLj9MsyWfPzxbJDLa2LHt4o3KOvjR\nd29SmpV5+8oBxlgq53HU5c0VHOKB5uzlaOt7GS4WCNo9vSZHLifrcrP5gPyQrMVuvEsxlvXn5AuU\ntWvGav0GBe3eNHf8Wd5++zsAjFU+wdF5oePdfH8VtOhRZHWgrBFvZ4d6Rw6ctr9KGMq7DpNOmul4\nUHFMjk53QGXq7WenxTFasZN+P0yTN4wjFEaA27cWaRZlvT5//gS+0rjGpqfxVZHcWFzh3Fk5BIsj\n03z3W9IF5OUL8ykdzM05aSZWHO+XQMWOKSpM4fsdov6APmcTBAdvGLu95jN3Wu45e7bCd7/3LQB+\n+aUvMKntd8596Fw6DwaLm4sD7PYCr78uDStzBS81Cj+48T6jH5MSpUdn51hsCUzmxX3u3hFj7uih\nibT7sVMo0uqKsWF2NijpIdDZbNFVpf7O1ft8/FMSx9rc2ODSosBhj0rmyeCFTDLJJJOnKI+0dG3j\n8FJFLI1Ll7e4pAdV5VCBjftiHVmIewXw9juXKR8Wq+DEySmu3JeTc9Xt8dUvie7/yz/4Jg1b+5HV\nHLxDYuGUtkos3xC44HYJui1xRU5cPMo/+pQ0k7t/ZY2j5+X+N26ssdqQ+5+tTtFU62uz43Dh+YUn\nmoSbN8xfD8AAABQZSURBVC38WeElnq6Ocl0Lrn+ydAG7IjBCvLPD934mxco//dHz3L4vgbTllU2q\nagnNvvgJigUZz+s/+QGLmn7ouQ1eUTfOy+cZGZUT82dvXMJR67mYWDQVnA9CmBgVb6AT9LE159tY\nhqCvVrVlURo6uIVUygeYpljwhyeHKC9I1DoMQ/xNGUuRhOtXJKA1OTNB3pOx+IFDUeMD66ubTM+e\nBKDV2Eb7IBLEEc22WgW2Q9GWMVpBi3FN57z8+ptUpzRZYG+bnJY33F7bxFGWwnCpTEcLi5+ev8DV\n+PbBx1grDhpE4NgGJ42WRyS7Ypk0Ax9P2Qim0Eu7hPQ6uwwqLMZBD08ru/UiKGrTzCDZJuce0Xnr\nEzVlD2zXt5ialu99u0mo1qAfP0BJA/jhHfa0W0ev18T0tYpZsU83XjrwGOW399uiR5GFn7ZCD/cT\ncnw/tT49r5QWKA8rlvJx4FawwY3/JIHvatHil5TJ8OHzZyjaMgavVOT5U7JuSqVhPO36UqnWaO3J\nniu6edQRIudYdLUHnZdziTVQ7JgcveDgntmzF4/xi0vSzPXs/ALzZ2Uif/HT/8jGvUHx+BzGVc/J\nc9Lmm/d+ukxHufMJhr42oDwRhlgKLe3WV2loQHh0fIHzH5E1vbe7RqBB0TgyGJ3DWrlCY09glvX1\nVTaVQ7zT9zn6gTRZcAoVRstjjx3bI5VuYcQltGVR9ucnmFkUPGt7JCEUr5LKRJmF81JK0bn2C2pH\nxO2+3OwzUZAB1oZrNLUwytRHRulqBk14ZZGO4kVXbtYZ9sS9e+5UkbGOLPqc38RLlMY0HTKu0fXw\ndInTNyUyfG8v4t4lcQl++TcWWLl/77EDf1i6jQRrQca5abn0Q1EejZ0VfHWJ3LzD174orWauXv2A\nkhKxj8xMsa11CaYnp8kZeUmffuXzLN5bAmDx/StMHRaqlGXb5Dy5/uiRFe5vyW/Zxko7RMzNnebG\nXWF7nF84i6/5/nESp80Nm83OE7kpXsGmc1eLg0wW8PQ5W0FCUUsvbqztMDUnkd6w36XbVDerCH3F\nNQulEQLFd4OgT3NNYIpCPkdjTdZHyfGIFRTsdPoUbFFa7127ysnTOg99yCkbbPzEM7S0hVFjt8nE\nmCiO2AxzxD924DE6xTzGUqUegqNF/B2vRaCaxsQdjK6/2AmJtY5vcXgSY2QtNvwVcppZuLZ5G6P1\nBhzjUarK828s38bWOtLNZp2ZaTlYyId0EnFV3WCcQLtyxgT0Y3nvbqFMkBODoUuXIW1seFBJEosk\nUZpfnEtxU4uIMBSl6LjOoPQEYRgShfuMnkGzVdcxuHoAlafKfHBrCYBOq8tnPyM4ZbfXZWZWSpqu\n3V9mdErea5S0KWnmVtfvYruaJdZPyOnp1Wg209ZIRc/lSThFm0FES+uYvLG5RUFZEMPjbtqtd/1e\nk8Mjgr9u2A84PSsKb/VBC7bl2Ro7DVBiSWW8RlNroCTBFh89L++yUK4SBtoVIl9Jy7b2YwenoL+1\nssiwGkuL9TqBJn59cuocnjeYE9jTut6PkgxeyCSTTDJ5ivJIS/fEQomlGxKt3H2mwjNb2ozt4rM0\nJsR1XotbbGxfA6Bx13B4WE6SG0GD42MSEJkaGyVcldO/tGtR7ouldGYTzFk5Jb72G59n6S2p7bDW\n28FotH8vDlMO4PGZw+S0weWR0Srrh+T7F2sznDwq96n3WuRqo080CUnXYmxYTufj42ewB5HeO2sM\nD8t9F1eXOKXWzNZeKz3lh8oj/NIvSZ0IY5FyQHOWRVE5e4fnjjOgrVuWTagmyEuf/CL/+k/+EICx\ncoGNuljVQ4UiE1VJf12r16kNa5CJJO2A4Do2xcLAUXy8lPNDXPiCVNC/cekdfG0senh2hl21tp0g\n4MYHkrhy6OgpaekABB0fU9WOEsNlFu8sAeC3Iq4vCyzzsQvPMzEiMMurr73Biy8JnFIaG8PqiVX3\nm7/1WzQ2xDJ2is6A4MDEzBHcvriMfujT7u0Xly6PHLz2gglD8AY1JWOSKbE6wjzM6vetvIOr/F3j\n5nC1YP3Y0AhdJcKXvdn9LgFem0ETM0OL9fqPAKgmpylrQNhxx9MOCR1rMe172Y0t8hpQzFllXFvr\nZsTbaYq47dgEwcaBxwjQ7XYplvS+jk03lvkdKhVSb6nfjxkZkbXbC3wsDTC6OZMWbifh/2nv6p7c\ntq/rwQcBEARBEuSS3F3uitpdbaRVJFtObcvfTuwknaZu85CZTif/Q57z97QPnclDxpPJJHGbVI5r\n1x5ZVlpZlhRpJUv7ZS6X4pIgCAIgPvpwLyE/SbsznX3CfeRIS/6Ai4t7z733nJRPoLU2j08/ILLv\nn//8n2GwMsXm7ja27hINZatWws4hPfeFgoaaRVWtN/URpNWYiByLY5ZLJibc6CpbVUSzuewjWCGO\nYZSZ+N/Q07lYebmNyV3yue+9toEu++i7le/hD3/6MwBgZa0B54ChLh2IY27kDocoLNC1jlUVLq8l\na1YePkMiSRxgyGKUnu2gvUYNUnvQhcbx7JXvXITI1UIUS1ho0nU4HI8R5Z69APLUoBtKIhbOEKb5\n0O7jZocu5kJnF3WmSDslFDA/5Bv0dzvwGEs5kzQRscKA8ngHOcbF1tZrUBhV2vMdtA+5XN7ZTMfT\nNvd2UW6Rg05GCjyXsBf/3h4aF2jKwBtP4fTpggs5AfGY+UwND5Z2zHJtkqBZZi7OZAAlpsty4dyb\n6LHQXE5WMWV6ynOnGsjxw3q740BmTjxZlFMcLUkSdB5RN/XHr7+KkLd9HNeB1aSy5tHuX9Fepu8d\nHto43aYybuT08b/36UFaKKvQeZ9eFRSoMl07dzJFGB29GzyXewcPbt1KfycYUng8sFNCk0N7gNMb\nBBVVkgTXNwnicEIJr7RJObVU1PHvfyK5pb49Rshl65VPr+Gf/p46w+++/RIUnkRJVA0xy/iISPDF\nNXqwX37jMsSY/GYyGuHGPSaekQQWNKTKdI+x+qPYJBhD4nJZhoIyjymO4EMpECxlVl3o8uxlNYHI\nW0bD3gMoDIMMJl8hHDGWb4SQQYE/xghRjn7PpG9jTyUl5BX5ZynnbhSZ6XaaLEwxi2+KFCBKmKAo\nLmJWZAbBAHnxmOrV8pPNM0GMUbVmwTWE7z8RrJypSIjf8kuCGjjYqBo0BuuHzmOsXqKXSNE08Ovf\nkmCsuSAiz+NpYZxA4/GInCSmvRzPm87cCbKEVNJHkQGJg/00iOAfY3rhcDJEdZlizKJRRsD9hcDp\nY1Cje7Z1+xvMnaIX/fUv/oLvt0lFIq9omF9jpRNTQymkM9aNKkY2+aJWWkj9IAhjiKxoIgoK6vM0\nQhnWo/SF+9l/X8VbbxO8WC1VkGOO67HnwWOV4IIkQqo8e4MygxcyyyyzzE7QnprprjZMWMyNsCxr\nCMpUAi4VDYjcGBuHAXSTsoLFvRibszXYyMNygz7f87qY4w58FSXc36SmiSYpT5QBIsCR6f/+6OU3\nMeDpACMe4BRrb5VrdXQeUUb02O/hbIuysmQyxqLBGaM/QlU+3i77C++1UVQp29ga3MFrDRrEjuMY\ngUdv89X2uXTH2qwvY8JzwS8tltMh9AhxWkK5Ewf/+O6P6AzFMlyWIjDMuTQTKuYUdDv0uSTKGPGK\nh6aZuHx2ll2JcFjvScspiBiaUBXgvTcvHfmMopZLB+oVGXC5odDpdlBg4cXvPv8iZlsBo8NBqnCx\nXLZw/w51kmVFx8V1gj4UQUCJJzcqtSoOOgRF3d3pYP0SXc9h/wCrp6k550YeZBbivPHJR9jnRp0U\nxqiyrHscigALGEpxjDA+OvG1pSg45GZetGAgYYWLM9YZTPbo7F3hG2i8KhvGVDEBQCwdQCjxymci\nQTXI5w6FzxGDIBchbAJMw14s1ZB4vODguXCZAlBXW/ADyoaDKAA4y/IQQeIGzSRCSmSvSEAyPZ6Q\nqpyTUqFTQcjD5ZGNvKZA58zV9d0Uigqn01SdotmowXa4ekgEbJyhSZ9rf7mG5iprqokiWmvkf53e\nITp8XxfPn4XVo+81lDJGzNC2prTRe8wS6U6cNicDy8PI5uy+LKNgHH0JxPanYD59PLD30d+iKQvD\nVKCz3mA4X0ePaVWXQhP2Pqtg5POwWB2jUWhBzlMlMPEc6Do126I4gsaLEmPXTbksJEXBhJcsNLMI\ncFXwt//wU1z9L1qfvvzWu7i1SX6m5Z8odDh+ADV+dux5+vSCIGLMrPFW1cAD1pWv5wz86/s0PvXe\nyy384RrxHijrBl57ifhVBw93EDMn6YpVxZ27dNHeWjdTpYXWIWDwJstoMsLtaxRor3/0AC9epIC9\nvv4dXL9JIxl7tx/i4nP0cK4V1rDVZYWIno/1DXIeQ9ERh8fbZT+/tJEyxBfzBuwRBcIwCuBNWBG3\naiLGTOG2hv6IPldkARGXcXEsIOG/s9xswbGZOCPwUpb9aejjjx/QoHdOFREwn4NpFfGTVwgHtX0H\ngxFNGpgFFc0KOYoXTNKRsfPLJjTt6Jju5zcOEPPvFKZhygcqykX0PXpIdj+7hylDAZoyhT+ml6xe\nzuM08zbs7mzDLFAJFcVA0WKVh807GHEXXdRLsIp0HWpGHl98QksEFauG82fbAGihZrYMcnfnEM7s\nlkkq8jI5+iQOMOrtHPmMB/I+RJOCJQYunDkWY7QfQeIlhXBqYKoyXaS4j8AlPC6yqrg/uAIAyMkV\nSBrd64Z8GYOQRhm73l2YGv390cRGo0hjRp4rwmYIrCIvYI831cz8A/RGdN0MsYgpjyIi6UEB+XES\n5TAMHh75jACNHVoluge246X48HQq494d8l17EKDGwoumaeL2LcJBZfkg5WqAEOGXvyCcv9Uo4F/e\n/z19/ibw4RXCcZ//mxUMVPKDbdfBme/S+Ji9uw9rgSldD3oQW+RbZiykmHazsQh7SL/TqrZw48ur\nRz7janMeX7n0G5yogIv8vf/z9XUYy20AQEMpQHF5WUPMwWEsx3cm1GABUBkNoarMKaEZqWSQ9K3+\ni6rpaX/BdcfQTIJZRDFBwlMZ0yCAxqrWnc2bMAyCCCdeAIknZhRFhSo9GzzI4IXMMssssxO0p2a6\nK1EJHw8oI/rB6y+h8+XvAAC6kMc7S7xH7orgrB7BVQeXFW48eQHkPL1tyoGGN1Zo8DrwArygU7bQ\nDwdwWCOsVavhuRXKTP7juo1PrlOXe+P8eTx/jjLAZaOLGhMMC4qKFY339VcKgFLgA4WI3eOVa5Oo\nj7JM5XAYduHFnBWFAQ4e03laSxsY2Ky1FCeoM7uWKMqYzFQS5FxapkwDB5o+gwhi+Dyc/p8f/zmF\nCDTFwHyD3p5rF15Hnsv8MPERMRQwcoeIYhbiyxupaOap5Tpu36cM7J0jnLG/exO1OZaHTiLInAnE\nciVde4SSIAFdx9XVOXS3aTGhVi6ljRKzVMOES8bVtTX89sqHAABDEaBzt7xp5vC731DWVDUVTDiN\nyAkBPO7mFw76OH+J7utnf/0UnR7d7wQeBO5UqzkB/jHygnAiQczT/TLHIpxDzuaLCnIMHyn7IYoj\nqjrsagVylaoxSHmIPBkzJ6wgx/7Ucf8NqkQMcWV1HYUZ656kQOHVZb1cRgD2jaADNaIM00jWkbBO\n2WTgIDLoXEWlCCUhOOz2p5/D2piRmx/NKuUaHKbF3O90cZrpMvP5HFqLVCFu7+yj32XOiPUW1llN\nZdj3keeJhbpVw5WrVIX48RCvvk0VwPt//BVWeG5dyYc4u0FTOzt7HUy2Zs2zBGsh+a7t+tB18mmr\nPI8rH1JlOlrxIPBa+eP+rWOdMS9oOMeE/dODHAJmI7xQvQRjQL+tqRfh5+m+SoaEmvDEV/o847u1\ntYs15rnIl+pPCOwlKV1jFuMwXX9XVR05boz3ex1U69TclmUZZzc2+N8HsHkQY7vn4HSdmrTe2EF3\n+OzY89Sgu9XZhc8PZ+fBJhiCg71/gOUNGkr2+j289wLdFGuujohLK6vSxDeMBRVUFwmTYAi5PL7Z\noa54paQjYHxTy2uomBR0fvrqEqwKOc/u9hYqCjmlqITo8hbJYnM+xVg924cKCnyuoiA/WwM6ovXd\nhxBlcuKzxZ9hPKQHV5JkFJgT1x0PUlpFVZTS7aty0UqDJQAE/mwrK4eYg2vXHkFjhYVmpZRuEGky\nsHiOOv7Tb0ml2u4Q8myAXSlCFOlsjtuDyt1jOUwwd+Hykc8YxREe7BDEUy7oGLHcS860UVO54yqK\nqOuzUimHRpvuq+MHmGNegvZ6C599SAQiH934HD/+yQ8BAH63C2uBHpLrN67h8os0ZTI6GCBiDuRh\n5xECf0bbWIXboZfGTvdrxFMOkBJSBQBNVfDcG6eOfEZFKuDggOCRRX0BBvPaDg96UJZ586/RhMhc\nqE5XgcTlYEf5AAWZHqovex/jdJPgKs24hAFLwtjh1zAbrwIAPP0hmjpNdNi9PkKD/MdIQrghC7gG\nfSxpbQDArlzEhPkWEjmAGtALyjSW0MwfL+h+9dU9bJynAHl6JQd3PBvI17G9TTBCu30Kp5bJL2/e\nvJGq+w4OHdRYzPHx1j48HrkaHtootMkP5hplSIyrB8EkhdXKFTHltgiCEUJ+uSy15uAzTLbb2YNV\no+89s76KXo+eVwghgunRX6DmgQkzoetizpVgD3mMSxOgzrYgvQg+P0tCPIXMUKUs5WCysGhv7xAB\nJ0WyNIDI+C4SBfGMBjSO4XGi1uvto9mkQKvrJkZ9wm5Nqw6R+1KBZ8MqMywVdNHt072vV0w4+51n\nni2DFzLLLLPMTtCEJEme/a8yyyyzzDL7f7Es080ss8wyO0HLgm5mmWWW2QlaFnQzyyyzzE7QsqCb\nWWaZZXaClgXdzDLLLLMTtCzoZpZZZpmdoP0f40so86pzuEoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyp2aY9AMMf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}