{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v2___PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikmohan1702/EIP_4/blob/master/v2___PersonAttrubutes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b6cf2555-81f2-4ec7-ed32-21c6d0caef8a"
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWnvAJ5rlx3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "#def get_random_eraser(p=0.1, s_l=0.01, s_h=0.1, r_1=0.1, r_2=1/0.1, v_l=0, v_h=20, pixel_level=False):  \n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16hPcQ3ny8SJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "42be9063-e75f-4d24-c9bd-f287fc486ba4"
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "b7cfcc40-5de4-466c-8704-db47d1b67014"
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        if self.augmentation is not None:\n",
        "          image = self.augmentation.flow(image,shuffle=False).next()\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16f8a391-ca71-4830-d67b-833de692b9d6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "8c2efdd6-0ca8-4d98-9a88-6aafa195afae"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td>resized/916.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6382</th>\n",
              "      <td>resized/6383.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11661</th>\n",
              "      <td>resized/11663.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2194</th>\n",
              "      <td>resized/2195.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9824</th>\n",
              "      <td>resized/9825.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "915      resized/916.jpg              0  ...                        1              0\n",
              "6382    resized/6383.jpg              0  ...                        0              0\n",
              "11661  resized/11663.jpg              1  ...                        0              1\n",
              "2194    resized/2195.jpg              1  ...                        0              0\n",
              "9824    resized/9825.jpg              1  ...                        0              1\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32,augmentation = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rescale=1./255,preprocessing_function=get_random_eraser()) )\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32,augmentation = ImageDataGenerator(rescale=1./255))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4ec94532-7964-4905-ae7e-5ff9f1f8edde"
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, Activation, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
        "\"\"\"backbone = VGG16(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\"\"\"\n",
        "from keras.layers import Activation\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import AveragePooling2D, Input, Flatten,Dropout\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers, optimizers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#model = Sequential()\n",
        "inp = Input(shape = (224, 224, 3))\n",
        "x = Convolution2D(32, (3, 3), padding = 'valid')(inp) #222\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(64, (3, 3))(x) #220\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = MaxPooling2D(pool_size = (2, 2))(x) #110\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3), padding = 'valid')(x) #108\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(64, (1, 1), padding = 'valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = MaxPooling2D(pool_size = (2, 2))(x)#54\n",
        "x = Dropout(0.25)(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3))(x)#52\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = MaxPooling2D(pool_size = (2, 2))(x)#26\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Conv2D(256, (3, 3))(x)#24\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = MaxPooling2D(pool_size = (2, 2))(x)#12\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Conv2D(512, (3, 3))(x)#10\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = Conv2D(128, (1, 1), padding = 'valid')(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "#x = MaxPooling2D(pool_size = (2, 2))(x)#8\n",
        "#x = Dropout(0.2)(x)\n",
        "\n",
        "#x = keras.layers.GlobalMaxPooling2D()(x)\n",
        "x = AveragePooling2D(pool_size=8)(x)\n",
        "\n",
        "\n",
        "#neck = backbone.output\n",
        "#neck = Flatten(name=\"flatten\")(neck)\n",
        "#neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "#def build_tower(in_layer):\n",
        "\n",
        "    #neck = Dropout(0.2)(in_layer)\n",
        "    #neck = Dense(128, activation=\"relu\")(neck)\n",
        "    #neck = Dropout(0.3)(in_layer)\n",
        "    #neck = Dense(128, activation=\"relu\")(neck)\n",
        " #   return neck\n",
        "\n",
        "\n",
        "#def build_head(name, in_layer):\n",
        "   # return \n",
        "\n",
        "x = Flatten(name=\"flatten\")(x)\n",
        "gender = Dense(num_units[\"gender\"], activation=\"sigmoid\", name=f\"{'gender'}_output\")(x)\n",
        "image_quality = Dense(num_units[\"image_quality\"], activation=\"softmax\", name=f\"{'image_quality'}_output\")(x)\n",
        "age = Dense(num_units[\"age\"], activation=\"softmax\", name=f\"{'age'}_output\")(x)\n",
        "weight = Dense(num_units[\"weight\"], activation=\"softmax\", name=f\"{'weight'}_output\")(x)\n",
        "bag = Dense(num_units[\"bag\"], activation=\"softmax\", name=f\"{'bag'}_output\")(x)\n",
        "footwear = Dense(num_units[\"footwear\"], activation=\"softmax\", name=f\"{'footwear'}_output\")(x)\n",
        "emotion = Dense(num_units[\"emotion\"], activation=\"softmax\", name=f\"{'emotion'}_output\")(x)\n",
        "pose = Dense(num_units[\"pose\"], activation=\"softmax\", name=f\"{'pose'}_output\")(x)\n",
        "\n",
        "\n",
        "# heads\n",
        "\"\"\"gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\"\"\"\n",
        "\n",
        "\n",
        "model = Model(inputs=inp, outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion])\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 70:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 60:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 50:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 40:\n",
        "        lr *= 1e-1\n",
        "\n",
        "        ###edited above learning rate\n",
        "    \"\"\"lr = 1e-3\n",
        "    if epoch > 80:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 70:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 60:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 50:\n",
        "        lr *= 1e-1\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"lr = 1e-3\n",
        "    if epoch > 10:\n",
        "        lr *= 1e-1\n",
        "    elif epoch > 25:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 35:\n",
        "        lr *= 1e-1\"\"\"\n",
        "\n",
        "    \n",
        "    \"\"\"lr = 3e-3\n",
        "    if epoch > 80:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 70:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 45:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 35:\n",
        "        lr *= 1e-1\"\"\"\n",
        "\n",
        "\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK8LIkkULWNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1c03148-7f5e-4eeb-f794-8937b8da3ddd"
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UgKVVPqLLR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e88e42c-c16a-489d-f1be-89cd3f3871e5"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "save_dir = os.path.join(os.getcwd(), 'gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models')\n",
        "print(save_dir)\n",
        "model_name = 'v2_person_attr%s_model.{epoch:03d}.h5' \n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I0ezoBzLLOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtb4oQEEH0fx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be6920dd-4964-4108-a599-bd88022e4188"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 222, 222, 32) 896         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 222, 222, 32) 128         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 222, 222, 32) 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 220, 220, 64) 18496       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 220, 220, 64) 256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 220, 220, 64) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 110, 110, 64) 0           activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 110, 110, 64) 0           max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 108, 108, 128 73856       dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 108, 108, 128 512         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 108, 108, 128 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 108, 108, 64) 8256        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 108, 108, 64) 0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 54, 54, 64)   0           activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 54, 54, 64)   0           max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 52, 52, 128)  73856       dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 52, 52, 128)  512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 52, 52, 128)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 26, 26, 128)  0           activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 26, 26, 128)  0           max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 24, 24, 256)  295168      dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 24, 24, 256)  1024        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 24, 24, 256)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 12, 12, 256)  0           activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 12, 12, 256)  0           max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 10, 10, 512)  1180160     dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 10, 10, 512)  2048        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 10, 10, 512)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 10, 10, 128)  65664       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 10, 10, 128)  0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 1, 1, 128)    0           activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 128)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,724,315\n",
            "Trainable params: 1,722,075\n",
            "Non-trainable params: 2,240\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk94kvRqHaAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.utils import plot_model\n",
        "#plot_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84d548da-7ba6-40b1-e31c-7e1f01dfee19"
      },
      "source": [
        "\"\"\"# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False\"\"\""
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# freeze backbone\\nfor layer in backbone.layers:\\n\\tlayer.trainable = False'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40292a8e-5d79-4986-95a9-43051c992cfc"
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "#opt = SGD(lr=0.001, momentum=0.9)\n",
        "#model.compile(\n",
        " #   optimizer=opt,\n",
        "  #  loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "   # metrics=[\"accuracy\"]\n",
        "#)\n",
        "\n",
        "##########################\n",
        "from keras.optimizers import Adam\n",
        "losses = {\n",
        " \t\"gender_output\": \"binary_crossentropy\",\n",
        " \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        " \t\"age_output\": \"categorical_crossentropy\",\n",
        " \t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\",\n",
        "    }\n",
        "\"\"\"loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0,\"weight_output\":1.0,\n",
        "                \"bag_output\":1.0,\"pose_output\":1.0,\"footwear_output\":1.0,\"emotion_output\":1.0 }\"\"\"\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "#Adam(lr=lr_schedule(0))\n",
        "model.compile(optimizer=Adam(lr=lr_schedule(0)),loss=losses, metrics=[\"accuracy\"])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGYaqtzAyzEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights(\"weights.best.hdf5\")\n",
        "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f296c21-fe94-4d09-c0d0-b4201d5ff0d1"
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=2, \n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "360/360 [==============================] - 66s 182ms/step - loss: 7.8394 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9551 - age_output_loss: 1.4393 - weight_output_loss: 0.9893 - bag_output_loss: 0.9305 - footwear_output_loss: 0.9866 - pose_output_loss: 0.9392 - emotion_output_loss: 0.9180 - gender_output_acc: 0.5709 - image_quality_output_acc: 0.5464 - age_output_acc: 0.3947 - weight_output_acc: 0.6356 - bag_output_acc: 0.5529 - footwear_output_acc: 0.5306 - pose_output_acc: 0.6124 - emotion_output_acc: 0.7095 - val_loss: 7.7171 - val_gender_output_loss: 0.6713 - val_image_quality_output_loss: 0.9093 - val_age_output_loss: 1.4300 - val_weight_output_loss: 1.0004 - val_bag_output_loss: 0.9188 - val_footwear_output_loss: 0.9679 - val_pose_output_loss: 0.9193 - val_emotion_output_loss: 0.9001 - val_gender_output_acc: 0.5871 - val_image_quality_output_acc: 0.5486 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5625 - val_footwear_output_acc: 0.5600 - val_pose_output_acc: 0.6314 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.71707, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.001.h5\n",
            "Epoch 2/100\n",
            "360/360 [==============================] - 61s 169ms/step - loss: 7.7087 - gender_output_loss: 0.6699 - image_quality_output_loss: 0.9198 - age_output_loss: 1.4270 - weight_output_loss: 0.9807 - bag_output_loss: 0.9181 - footwear_output_loss: 0.9566 - pose_output_loss: 0.9255 - emotion_output_loss: 0.9111 - gender_output_acc: 0.5911 - image_quality_output_acc: 0.5552 - age_output_acc: 0.3977 - weight_output_acc: 0.6378 - bag_output_acc: 0.5582 - footwear_output_acc: 0.5490 - pose_output_acc: 0.6156 - emotion_output_acc: 0.7111 - val_loss: 7.7339 - val_gender_output_loss: 0.6592 - val_image_quality_output_loss: 0.9400 - val_age_output_loss: 1.4501 - val_weight_output_loss: 1.0026 - val_bag_output_loss: 0.9151 - val_footwear_output_loss: 0.9379 - val_pose_output_loss: 0.9311 - val_emotion_output_loss: 0.8979 - val_gender_output_acc: 0.6104 - val_image_quality_output_acc: 0.5233 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5620 - val_footwear_output_acc: 0.5704 - val_pose_output_acc: 0.6280 - val_emotion_output_acc: 0.7138\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 7.71707\n",
            "Epoch 3/100\n",
            "360/360 [==============================] - 61s 171ms/step - loss: 7.6671 - gender_output_loss: 0.6602 - image_quality_output_loss: 0.9102 - age_output_loss: 1.4226 - weight_output_loss: 0.9783 - bag_output_loss: 0.9145 - footwear_output_loss: 0.9500 - pose_output_loss: 0.9226 - emotion_output_loss: 0.9087 - gender_output_acc: 0.6043 - image_quality_output_acc: 0.5539 - age_output_acc: 0.3998 - weight_output_acc: 0.6376 - bag_output_acc: 0.5609 - footwear_output_acc: 0.5546 - pose_output_acc: 0.6158 - emotion_output_acc: 0.7115 - val_loss: 7.5748 - val_gender_output_loss: 0.6566 - val_image_quality_output_loss: 0.8801 - val_age_output_loss: 1.4115 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.9057 - val_footwear_output_loss: 0.9395 - val_pose_output_loss: 0.9029 - val_emotion_output_loss: 0.8905 - val_gender_output_acc: 0.6101 - val_image_quality_output_acc: 0.5719 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5625 - val_footwear_output_acc: 0.5744 - val_pose_output_acc: 0.6285 - val_emotion_output_acc: 0.7143\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.71707 to 7.57485, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.003.h5\n",
            "Epoch 4/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.6264 - gender_output_loss: 0.6514 - image_quality_output_loss: 0.9019 - age_output_loss: 1.4196 - weight_output_loss: 0.9754 - bag_output_loss: 0.9097 - footwear_output_loss: 0.9440 - pose_output_loss: 0.9192 - emotion_output_loss: 0.9052 - gender_output_acc: 0.6120 - image_quality_output_acc: 0.5615 - age_output_acc: 0.3979 - weight_output_acc: 0.6380 - bag_output_acc: 0.5639 - footwear_output_acc: 0.5586 - pose_output_acc: 0.6149 - emotion_output_acc: 0.7113\n",
            "360/360 [==============================] - 61s 169ms/step - loss: 7.6284 - gender_output_loss: 0.6514 - image_quality_output_loss: 0.9022 - age_output_loss: 1.4200 - weight_output_loss: 0.9761 - bag_output_loss: 0.9096 - footwear_output_loss: 0.9442 - pose_output_loss: 0.9192 - emotion_output_loss: 0.9057 - gender_output_acc: 0.6122 - image_quality_output_acc: 0.5612 - age_output_acc: 0.3977 - weight_output_acc: 0.6374 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5585 - pose_output_acc: 0.6151 - emotion_output_acc: 0.7111 - val_loss: 7.6910 - val_gender_output_loss: 0.6972 - val_image_quality_output_loss: 0.8893 - val_age_output_loss: 1.4223 - val_weight_output_loss: 0.9934 - val_bag_output_loss: 0.9134 - val_footwear_output_loss: 0.9678 - val_pose_output_loss: 0.9095 - val_emotion_output_loss: 0.8981 - val_gender_output_acc: 0.5280 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5615 - val_footwear_output_acc: 0.5511 - val_pose_output_acc: 0.6290 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 7.57485\n",
            "Epoch 5/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.5947 - gender_output_loss: 0.6423 - image_quality_output_loss: 0.8981 - age_output_loss: 1.4195 - weight_output_loss: 0.9751 - bag_output_loss: 0.9059 - footwear_output_loss: 0.9384 - pose_output_loss: 0.9113 - emotion_output_loss: 0.9042 - gender_output_acc: 0.6233 - image_quality_output_acc: 0.5623 - age_output_acc: 0.3995 - weight_output_acc: 0.6376 - bag_output_acc: 0.5650 - footwear_output_acc: 0.5646 - pose_output_acc: 0.6161 - emotion_output_acc: 0.7110\n",
            "Epoch 00004: val_loss did not improve from 7.57485\n",
            "Epoch 5/100\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 7.5946 - gender_output_loss: 0.6423 - image_quality_output_loss: 0.8980 - age_output_loss: 1.4194 - weight_output_loss: 0.9750 - bag_output_loss: 0.9056 - footwear_output_loss: 0.9389 - pose_output_loss: 0.9117 - emotion_output_loss: 0.9036 - gender_output_acc: 0.6232 - image_quality_output_acc: 0.5622 - age_output_acc: 0.3996 - weight_output_acc: 0.6377 - bag_output_acc: 0.5651 - footwear_output_acc: 0.5642 - pose_output_acc: 0.6157 - emotion_output_acc: 0.7113 - val_loss: 7.9526 - val_gender_output_loss: 0.8070 - val_image_quality_output_loss: 0.8758 - val_age_output_loss: 1.4832 - val_weight_output_loss: 1.0006 - val_bag_output_loss: 0.9730 - val_footwear_output_loss: 0.9803 - val_pose_output_loss: 0.9182 - val_emotion_output_loss: 0.9144 - val_gender_output_acc: 0.5890 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3438 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.5600 - val_footwear_output_acc: 0.5719 - val_pose_output_acc: 0.6280 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 7.57485\n",
            "Epoch 6/100\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 7.5569 - gender_output_loss: 0.6339 - image_quality_output_loss: 0.8945 - age_output_loss: 1.4151 - weight_output_loss: 0.9709 - bag_output_loss: 0.9013 - footwear_output_loss: 0.9283 - pose_output_loss: 0.9100 - emotion_output_loss: 0.9029 - gender_output_acc: 0.6327 - image_quality_output_acc: 0.5650 - age_output_acc: 0.3997 - weight_output_acc: 0.6379 - bag_output_acc: 0.5693 - footwear_output_acc: 0.5706 - pose_output_acc: 0.6156 - emotion_output_acc: 0.7115 - val_loss: 7.5833 - val_gender_output_loss: 0.6596 - val_image_quality_output_loss: 0.8835 - val_age_output_loss: 1.4088 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.9022 - val_footwear_output_loss: 0.9346 - val_pose_output_loss: 0.9125 - val_emotion_output_loss: 0.8994 - val_gender_output_acc: 0.6250 - val_image_quality_output_acc: 0.5794 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6275 - val_bag_output_acc: 0.5645 - val_footwear_output_acc: 0.5605 - val_pose_output_acc: 0.6285 - val_emotion_output_acc: 0.7158\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.57485\n",
            "Epoch 7/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 7.5223 - gender_output_loss: 0.6229 - image_quality_output_loss: 0.8951 - age_output_loss: 1.4152 - weight_output_loss: 0.9702 - bag_output_loss: 0.8941 - footwear_output_loss: 0.9189 - pose_output_loss: 0.9043 - emotion_output_loss: 0.9016 - gender_output_acc: 0.6513 - image_quality_output_acc: 0.5653 - age_output_acc: 0.3981 - weight_output_acc: 0.6373 - bag_output_acc: 0.5700 - footwear_output_acc: 0.5748 - pose_output_acc: 0.6148 - emotion_output_acc: 0.7113 - val_loss: 7.5526 - val_gender_output_loss: 0.6142 - val_image_quality_output_loss: 0.9397 - val_age_output_loss: 1.4097 - val_weight_output_loss: 0.9869 - val_bag_output_loss: 0.8948 - val_footwear_output_loss: 0.9229 - val_pose_output_loss: 0.8885 - val_emotion_output_loss: 0.8959 - val_gender_output_acc: 0.6558 - val_image_quality_output_acc: 0.5407 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.5680 - val_footwear_output_acc: 0.5655 - val_pose_output_acc: 0.6280 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.57485 to 7.55259, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.007.h5\n",
            "Epoch 8/100\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 7.4944 - gender_output_loss: 0.6210 - image_quality_output_loss: 0.8875 - age_output_loss: 1.4122 - weight_output_loss: 0.9677 - bag_output_loss: 0.8902 - footwear_output_loss: 0.9151 - pose_output_loss: 0.8989 - emotion_output_loss: 0.9018 - gender_output_acc: 0.6494 - image_quality_output_acc: 0.5679 - age_output_acc: 0.3998 - weight_output_acc: 0.6373 - bag_output_acc: 0.5744 - footwear_output_acc: 0.5727 - pose_output_acc: 0.6156 - emotion_output_acc: 0.7112 - val_loss: 7.4220 - val_gender_output_loss: 0.6007 - val_image_quality_output_loss: 0.8789 - val_age_output_loss: 1.4017 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.8884 - val_footwear_output_loss: 0.9032 - val_pose_output_loss: 0.8754 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.6691 - val_image_quality_output_acc: 0.5888 - val_age_output_acc: 0.3963 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5670 - val_footwear_output_acc: 0.5828 - val_pose_output_acc: 0.6290 - val_emotion_output_acc: 0.7128\n",
            "\n",
            "Epoch 00008: val_loss improved from 7.55259 to 7.42198, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.008.h5\n",
            "Epoch 9/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.4637 - gender_output_loss: 0.6085 - image_quality_output_loss: 0.8909 - age_output_loss: 1.4112 - weight_output_loss: 0.9662 - bag_output_loss: 0.8893 - footwear_output_loss: 0.9073 - pose_output_loss: 0.8912 - emotion_output_loss: 0.8992 - gender_output_acc: 0.6574 - image_quality_output_acc: 0.5642 - age_output_acc: 0.3983 - weight_output_acc: 0.6374 - bag_output_acc: 0.5789 - footwear_output_acc: 0.5766 - pose_output_acc: 0.6159 - emotion_output_acc: 0.7109\n",
            "Epoch 00008: val_loss improved from 7.55259 to 7.42198, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.008.h5\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 7.4625 - gender_output_loss: 0.6085 - image_quality_output_loss: 0.8906 - age_output_loss: 1.4108 - weight_output_loss: 0.9661 - bag_output_loss: 0.8892 - footwear_output_loss: 0.9071 - pose_output_loss: 0.8914 - emotion_output_loss: 0.8988 - gender_output_acc: 0.6573 - image_quality_output_acc: 0.5645 - age_output_acc: 0.3985 - weight_output_acc: 0.6376 - bag_output_acc: 0.5789 - footwear_output_acc: 0.5766 - pose_output_acc: 0.6158 - emotion_output_acc: 0.7111 - val_loss: 7.5960 - val_gender_output_loss: 0.6819 - val_image_quality_output_loss: 0.9154 - val_age_output_loss: 1.4025 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.9235 - val_footwear_output_loss: 0.9249 - val_pose_output_loss: 0.8649 - val_emotion_output_loss: 0.8953 - val_gender_output_acc: 0.6245 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5630 - val_footwear_output_acc: 0.5774 - val_pose_output_acc: 0.6290 - val_emotion_output_acc: 0.7143\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 7.42198\n",
            "Epoch 10/100\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 7.4377 - gender_output_loss: 0.6066 - image_quality_output_loss: 0.8860 - age_output_loss: 1.4092 - weight_output_loss: 0.9638 - bag_output_loss: 0.8854 - footwear_output_loss: 0.9023 - pose_output_loss: 0.8854 - emotion_output_loss: 0.8990 - gender_output_acc: 0.6627 - image_quality_output_acc: 0.5678 - age_output_acc: 0.4002 - weight_output_acc: 0.6373 - bag_output_acc: 0.5818 - footwear_output_acc: 0.5832 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7115 - val_loss: 7.3850 - val_gender_output_loss: 0.5813 - val_image_quality_output_loss: 0.8852 - val_age_output_loss: 1.4209 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.8760 - val_footwear_output_loss: 0.8828 - val_pose_output_loss: 0.8677 - val_emotion_output_loss: 0.8883 - val_gender_output_acc: 0.6838 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.5947 - val_footwear_output_acc: 0.5992 - val_pose_output_acc: 0.6290 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00010: val_loss improved from 7.42198 to 7.38504, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.010.h5\n",
            "Epoch 11/100\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 7.4058 - gender_output_loss: 0.5993 - image_quality_output_loss: 0.8851 - age_output_loss: 1.4071 - weight_output_loss: 0.9610 - bag_output_loss: 0.8825 - footwear_output_loss: 0.8984 - pose_output_loss: 0.8758 - emotion_output_loss: 0.8968 - gender_output_acc: 0.6692 - image_quality_output_acc: 0.5677 - age_output_acc: 0.3998 - weight_output_acc: 0.6368 - bag_output_acc: 0.5825 - footwear_output_acc: 0.5853 - pose_output_acc: 0.6218 - emotion_output_acc: 0.7109 - val_loss: 7.4380 - val_gender_output_loss: 0.5806 - val_image_quality_output_loss: 0.9123 - val_age_output_loss: 1.4048 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8867 - val_footwear_output_loss: 0.9322 - val_pose_output_loss: 0.8507 - val_emotion_output_loss: 0.8867 - val_gender_output_acc: 0.6783 - val_image_quality_output_acc: 0.5615 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5709 - val_footwear_output_acc: 0.5680 - val_pose_output_acc: 0.6429 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 7.38504\n",
            "Epoch 12/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 7.3615 - gender_output_loss: 0.5884 - image_quality_output_loss: 0.8841 - age_output_loss: 1.4046 - weight_output_loss: 0.9621 - bag_output_loss: 0.8771 - footwear_output_loss: 0.8916 - pose_output_loss: 0.8604 - emotion_output_loss: 0.8932 - gender_output_acc: 0.6826 - image_quality_output_acc: 0.5680 - age_output_acc: 0.3996 - weight_output_acc: 0.6386 - bag_output_acc: 0.5909 - footwear_output_acc: 0.5905 - pose_output_acc: 0.6221 - emotion_output_acc: 0.7110 - val_loss: 7.5740 - val_gender_output_loss: 0.6158 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.4152 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 0.8801 - val_footwear_output_loss: 0.9605 - val_pose_output_loss: 0.8364 - val_emotion_output_loss: 0.8952 - val_gender_output_acc: 0.6592 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6071 - val_footwear_output_acc: 0.5605 - val_pose_output_acc: 0.6334 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 7.38504\n",
            "Epoch 13/100\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 7.4058 - gender_output_loss: 0.5993 - image_quality_output_loss: 0.8851 - age_output_loss: 1.4071 - weight_output_loss: 0.9610 - bag_output_loss: 0.8825 - footwear_output_loss: 0.8984 - pose_output_loss: 0.8758 - emotion_output_loss: 0.8968 - gender_output_acc: 0.6692 - image_quality_output_acc: 0.5677 - age_output_acc: 0.3998 - weight_output_acc: 0.6368 - bag_output_acc: 0.5825 - footwear_output_acc: 0.5853 - pose_output_acc: 0.6218 - emotion_output_acc: 0.7109 - val_loss: 7.4380 - val_gender_output_loss: 0.5806 - val_image_quality_output_loss: 0.9123 - val_age_output_loss: 1.4048 - val_weight_output_loss: 0.9842 - val_bag_output_loss: 0.8867 - val_footwear_output_loss: 0.9322 - val_pose_output_loss: 0.8507 - val_emotion_output_loss: 0.8867 - val_gender_output_acc: 0.6783 - val_image_quality_output_acc: 0.5615 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.5709 - val_footwear_output_acc: 0.5680 - val_pose_output_acc: 0.6429 - val_emotion_output_acc: 0.7148\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 7.3205 - gender_output_loss: 0.5833 - image_quality_output_loss: 0.8793 - age_output_loss: 1.3997 - weight_output_loss: 0.9574 - bag_output_loss: 0.8737 - footwear_output_loss: 0.8871 - pose_output_loss: 0.8467 - emotion_output_loss: 0.8933 - gender_output_acc: 0.6843 - image_quality_output_acc: 0.5743 - age_output_acc: 0.3986 - weight_output_acc: 0.6372 - bag_output_acc: 0.5901 - footwear_output_acc: 0.5860 - pose_output_acc: 0.6282 - emotion_output_acc: 0.7112 - val_loss: 7.3658 - val_gender_output_loss: 0.5864 - val_image_quality_output_loss: 0.9156 - val_age_output_loss: 1.3995 - val_weight_output_loss: 0.9802 - val_bag_output_loss: 0.8741 - val_footwear_output_loss: 0.8881 - val_pose_output_loss: 0.8423 - val_emotion_output_loss: 0.8796 - val_gender_output_acc: 0.6959 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.5898 - val_footwear_output_acc: 0.5883 - val_pose_output_acc: 0.6468 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00013: val_loss improved from 7.38504 to 7.36577, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.013.h5\n",
            "Epoch 14/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 7.2882 - gender_output_loss: 0.5704 - image_quality_output_loss: 0.8819 - age_output_loss: 1.3996 - weight_output_loss: 0.9600 - bag_output_loss: 0.8717 - footwear_output_loss: 0.8816 - pose_output_loss: 0.8309 - emotion_output_loss: 0.8919 - gender_output_acc: 0.6989 - image_quality_output_acc: 0.5717 - age_output_acc: 0.4020 - weight_output_acc: 0.6376 - bag_output_acc: 0.5911 - footwear_output_acc: 0.5906 - pose_output_acc: 0.6383 - emotion_output_acc: 0.7114 - val_loss: 7.4372 - val_gender_output_loss: 0.5936 - val_image_quality_output_loss: 0.8795 - val_age_output_loss: 1.4297 - val_weight_output_loss: 0.9990 - val_bag_output_loss: 0.9032 - val_footwear_output_loss: 0.9366 - val_pose_output_loss: 0.8112 - val_emotion_output_loss: 0.8843 - val_gender_output_acc: 0.6699 - val_image_quality_output_acc: 0.5794 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.5685 - val_footwear_output_acc: 0.5630 - val_pose_output_acc: 0.6761 - val_emotion_output_acc: 0.7158\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 7.36577\n",
            "Epoch 15/100\n",
            "360/360 [==============================] - 61s 171ms/step - loss: 7.2465 - gender_output_loss: 0.5646 - image_quality_output_loss: 0.8781 - age_output_loss: 1.3980 - weight_output_loss: 0.9572 - bag_output_loss: 0.8687 - footwear_output_loss: 0.8756 - pose_output_loss: 0.8136 - emotion_output_loss: 0.8908 - gender_output_acc: 0.6982 - image_quality_output_acc: 0.5732 - age_output_acc: 0.4023 - weight_output_acc: 0.6372 - bag_output_acc: 0.5943 - footwear_output_acc: 0.5958 - pose_output_acc: 0.6398 - emotion_output_acc: 0.7111 - val_loss: 7.1807 - val_gender_output_loss: 0.5566 - val_image_quality_output_loss: 0.8842 - val_age_output_loss: 1.3866 - val_weight_output_loss: 0.9871 - val_bag_output_loss: 0.8574 - val_footwear_output_loss: 0.8934 - val_pose_output_loss: 0.7466 - val_emotion_output_loss: 0.8688 - val_gender_output_acc: 0.6972 - val_image_quality_output_acc: 0.5729 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6111 - val_footwear_output_acc: 0.5833 - val_pose_output_acc: 0.6716 - val_emotion_output_acc: 0.7153\n",
            "\n",
            "Epoch 00015: val_loss improved from 7.36577 to 7.18074, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.015.h5\n",
            "Epoch 16/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 7.2032 - gender_output_loss: 0.5571 - image_quality_output_loss: 0.8743 - age_output_loss: 1.3949 - weight_output_loss: 0.9563 - bag_output_loss: 0.8674 - footwear_output_loss: 0.8705 - pose_output_loss: 0.7920 - emotion_output_loss: 0.8907 - gender_output_acc: 0.7030 - image_quality_output_acc: 0.5814 - age_output_acc: 0.4051 - weight_output_acc: 0.6382 - bag_output_acc: 0.5981 - footwear_output_acc: 0.6023 - pose_output_acc: 0.6551 - emotion_output_acc: 0.7109 - val_loss: 7.1409 - val_gender_output_loss: 0.5394 - val_image_quality_output_loss: 0.8751 - val_age_output_loss: 1.4089 - val_weight_output_loss: 0.9921 - val_bag_output_loss: 0.8586 - val_footwear_output_loss: 0.8683 - val_pose_output_loss: 0.7248 - val_emotion_output_loss: 0.8737 - val_gender_output_acc: 0.7230 - val_image_quality_output_acc: 0.5913 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6091 - val_footwear_output_acc: 0.5997 - val_pose_output_acc: 0.6706 - val_emotion_output_acc: 0.7143\n",
            "\n",
            "Epoch 00016: val_loss improved from 7.18074 to 7.14089, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.016.h5\n",
            "Epoch 17/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 7.1664 - gender_output_loss: 0.5463 - image_quality_output_loss: 0.8768 - age_output_loss: 1.3954 - weight_output_loss: 0.9559 - bag_output_loss: 0.8646 - footwear_output_loss: 0.8610 - pose_output_loss: 0.7778 - emotion_output_loss: 0.8887 - gender_output_acc: 0.7142 - image_quality_output_acc: 0.5797 - age_output_acc: 0.4042 - weight_output_acc: 0.6380 - bag_output_acc: 0.5952 - footwear_output_acc: 0.6076 - pose_output_acc: 0.6598 - emotion_output_acc: 0.7112 - val_loss: 7.2767 - val_gender_output_loss: 0.5811 - val_image_quality_output_loss: 0.9322 - val_age_output_loss: 1.3879 - val_weight_output_loss: 0.9955 - val_bag_output_loss: 0.9492 - val_footwear_output_loss: 0.8776 - val_pose_output_loss: 0.6794 - val_emotion_output_loss: 0.8738 - val_gender_output_acc: 0.6964 - val_image_quality_output_acc: 0.5685 - val_age_output_acc: 0.3785 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5650 - val_footwear_output_acc: 0.6042 - val_pose_output_acc: 0.7004 - val_emotion_output_acc: 0.7143\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 7.14089\n",
            "Epoch 18/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 7.1363 - gender_output_loss: 0.5450 - image_quality_output_loss: 0.8755 - age_output_loss: 1.3904 - weight_output_loss: 0.9538 - bag_output_loss: 0.8608 - footwear_output_loss: 0.8625 - pose_output_loss: 0.7622 - emotion_output_loss: 0.8861 - gender_output_acc: 0.7149 - image_quality_output_acc: 0.5750 - age_output_acc: 0.4054 - weight_output_acc: 0.6380 - bag_output_acc: 0.5990 - footwear_output_acc: 0.6070 - pose_output_acc: 0.6662 - emotion_output_acc: 0.7114 - val_loss: 7.1901 - val_gender_output_loss: 0.5621 - val_image_quality_output_loss: 0.9000 - val_age_output_loss: 1.4018 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9044 - val_footwear_output_loss: 0.9007 - val_pose_output_loss: 0.6605 - val_emotion_output_loss: 0.8821 - val_gender_output_acc: 0.7098 - val_image_quality_output_acc: 0.5848 - val_age_output_acc: 0.3864 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.5809 - val_footwear_output_acc: 0.6141 - val_pose_output_acc: 0.7257 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 7.14089\n",
            "Epoch 19/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 7.0768 - gender_output_loss: 0.5324 - image_quality_output_loss: 0.8713 - age_output_loss: 1.3865 - weight_output_loss: 0.9483 - bag_output_loss: 0.8555 - footwear_output_loss: 0.8515 - pose_output_loss: 0.7478 - emotion_output_loss: 0.8835 - gender_output_acc: 0.7260 - image_quality_output_acc: 0.5813 - age_output_acc: 0.4030 - weight_output_acc: 0.6369 - bag_output_acc: 0.6043 - footwear_output_acc: 0.6101 - pose_output_acc: 0.6778 - emotion_output_acc: 0.7109 - val_loss: 7.1701 - val_gender_output_loss: 0.5413 - val_image_quality_output_loss: 0.8613 - val_age_output_loss: 1.3961 - val_weight_output_loss: 0.9864 - val_bag_output_loss: 0.9282 - val_footwear_output_loss: 0.8982 - val_pose_output_loss: 0.6722 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.7143 - val_image_quality_output_acc: 0.5878 - val_age_output_acc: 0.3829 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.5650 - val_footwear_output_acc: 0.6027 - val_pose_output_acc: 0.7068 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 7.14089\n",
            "Epoch 20/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 7.0429 - gender_output_loss: 0.5232 - image_quality_output_loss: 0.8724 - age_output_loss: 1.3849 - weight_output_loss: 0.9488 - bag_output_loss: 0.8557 - footwear_output_loss: 0.8475 - pose_output_loss: 0.7285 - emotion_output_loss: 0.8819 - gender_output_acc: 0.7327 - image_quality_output_acc: 0.5781 - age_output_acc: 0.4074 - weight_output_acc: 0.6363 - bag_output_acc: 0.6056 - footwear_output_acc: 0.6146 - pose_output_acc: 0.6832 - emotion_output_acc: 0.7112 - val_loss: 6.9037 - val_gender_output_loss: 0.4767 - val_image_quality_output_loss: 0.9113 - val_age_output_loss: 1.3867 - val_weight_output_loss: 0.9682 - val_bag_output_loss: 0.8446 - val_footwear_output_loss: 0.8408 - val_pose_output_loss: 0.6128 - val_emotion_output_loss: 0.8625 - val_gender_output_acc: 0.7609 - val_image_quality_output_acc: 0.5689 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6225 - val_footwear_output_acc: 0.6146 - val_pose_output_acc: 0.7302 - val_emotion_output_acc: 0.7128\n",
            "\n",
            "Epoch 00020: val_loss improved from 7.14089 to 6.90366, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.020.h5\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 7.14089\n",
            "Epoch 20/100\n",
            "Epoch 21/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 7.0038 - gender_output_loss: 0.5136 - image_quality_output_loss: 0.8681 - age_output_loss: 1.3824 - weight_output_loss: 0.9476 - bag_output_loss: 0.8505 - footwear_output_loss: 0.8449 - pose_output_loss: 0.7165 - emotion_output_loss: 0.8802 - gender_output_acc: 0.7408 - image_quality_output_acc: 0.5835 - age_output_acc: 0.4037 - weight_output_acc: 0.6376 - bag_output_acc: 0.6115 - footwear_output_acc: 0.6164 - pose_output_acc: 0.6894 - emotion_output_acc: 0.7110 - val_loss: 6.9724 - val_gender_output_loss: 0.4619 - val_image_quality_output_loss: 0.8841 - val_age_output_loss: 1.3842 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.8516 - val_footwear_output_loss: 0.8690 - val_pose_output_loss: 0.6644 - val_emotion_output_loss: 0.8719 - val_gender_output_acc: 0.7686 - val_image_quality_output_acc: 0.5699 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6171 - val_bag_output_acc: 0.6101 - val_footwear_output_acc: 0.6121 - val_pose_output_acc: 0.7282 - val_emotion_output_acc: 0.7138\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 6.90366\n",
            "Epoch 22/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.9744 - gender_output_loss: 0.5088 - image_quality_output_loss: 0.8680 - age_output_loss: 1.3805 - weight_output_loss: 0.9462 - bag_output_loss: 0.8472 - footwear_output_loss: 0.8400 - pose_output_loss: 0.7055 - emotion_output_loss: 0.8781 - gender_output_acc: 0.7429 - image_quality_output_acc: 0.5834 - age_output_acc: 0.4064 - weight_output_acc: 0.6389 - bag_output_acc: 0.6148 - footwear_output_acc: 0.6206 - pose_output_acc: 0.6997 - emotion_output_acc: 0.7108 - val_loss: 6.9960 - val_gender_output_loss: 0.4776 - val_image_quality_output_loss: 0.9052 - val_age_output_loss: 1.3949 - val_weight_output_loss: 0.9927 - val_bag_output_loss: 0.8395 - val_footwear_output_loss: 0.9052 - val_pose_output_loss: 0.5888 - val_emotion_output_loss: 0.8922 - val_gender_output_acc: 0.7592 - val_image_quality_output_acc: 0.5685 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6324 - val_footwear_output_acc: 0.5858 - val_pose_output_acc: 0.7391 - val_emotion_output_acc: 0.7138\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 6.90366\n",
            "Epoch 23/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.9364 - gender_output_loss: 0.4997 - image_quality_output_loss: 0.8670 - age_output_loss: 1.3751 - weight_output_loss: 0.9460 - bag_output_loss: 0.8424 - footwear_output_loss: 0.8379 - pose_output_loss: 0.6905 - emotion_output_loss: 0.8778 - gender_output_acc: 0.7505 - image_quality_output_acc: 0.5808 - age_output_acc: 0.4084 - weight_output_acc: 0.6388 - bag_output_acc: 0.6155 - footwear_output_acc: 0.6187 - pose_output_acc: 0.7034 - emotion_output_acc: 0.7109 - val_loss: 6.9933 - val_gender_output_loss: 0.5609 - val_image_quality_output_loss: 0.8894 - val_age_output_loss: 1.3837 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.8758 - val_footwear_output_loss: 0.8456 - val_pose_output_loss: 0.5997 - val_emotion_output_loss: 0.8621 - val_gender_output_acc: 0.7200 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3844 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.5972 - val_footwear_output_acc: 0.6042 - val_pose_output_acc: 0.7356 - val_emotion_output_acc: 0.7118\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 6.90366\n",
            "Epoch 24/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.9018 - gender_output_loss: 0.4900 - image_quality_output_loss: 0.8637 - age_output_loss: 1.3732 - weight_output_loss: 0.9435 - bag_output_loss: 0.8364 - footwear_output_loss: 0.8330 - pose_output_loss: 0.6836 - emotion_output_loss: 0.8785 - gender_output_acc: 0.7586 - image_quality_output_acc: 0.5822 - age_output_acc: 0.4068 - weight_output_acc: 0.6362 - bag_output_acc: 0.6210 - footwear_output_acc: 0.6285 - pose_output_acc: 0.7085 - emotion_output_acc: 0.7111 - val_loss: 6.8672 - val_gender_output_loss: 0.4693 - val_image_quality_output_loss: 0.9050 - val_age_output_loss: 1.3709 - val_weight_output_loss: 0.9684 - val_bag_output_loss: 0.8927 - val_footwear_output_loss: 0.8308 - val_pose_output_loss: 0.5620 - val_emotion_output_loss: 0.8681 - val_gender_output_acc: 0.7510 - val_image_quality_output_acc: 0.5660 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.5858 - val_footwear_output_acc: 0.6151 - val_pose_output_acc: 0.7654 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00024: val_loss improved from 6.90366 to 6.86722, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.024.h5\n",
            "Epoch 25/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.8826 - gender_output_loss: 0.4906 - image_quality_output_loss: 0.8605 - age_output_loss: 1.3729 - weight_output_loss: 0.9404 - bag_output_loss: 0.8401 - footwear_output_loss: 0.8284 - pose_output_loss: 0.6713 - emotion_output_loss: 0.8783 - gender_output_acc: 0.7547 - image_quality_output_acc: 0.5838 - age_output_acc: 0.4072 - weight_output_acc: 0.6391 - bag_output_acc: 0.6187 - footwear_output_acc: 0.6231 - pose_output_acc: 0.7184 - emotion_output_acc: 0.7109 - val_loss: 6.7497 - val_gender_output_loss: 0.4260 - val_image_quality_output_loss: 0.8577 - val_age_output_loss: 1.3670 - val_weight_output_loss: 0.9721 - val_bag_output_loss: 0.8417 - val_footwear_output_loss: 0.8753 - val_pose_output_loss: 0.5535 - val_emotion_output_loss: 0.8564 - val_gender_output_acc: 0.8026 - val_image_quality_output_acc: 0.5888 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6220 - val_bag_output_acc: 0.6176 - val_footwear_output_acc: 0.5823 - val_pose_output_acc: 0.7718 - val_emotion_output_acc: 0.7153\n",
            "\n",
            "Epoch 00025: val_loss improved from 6.86722 to 6.74968, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.025.h5\n",
            "Epoch 26/100\n",
            "360/360 [==============================] - 61s 169ms/step - loss: 6.8538 - gender_output_loss: 0.4784 - image_quality_output_loss: 0.8609 - age_output_loss: 1.3724 - weight_output_loss: 0.9416 - bag_output_loss: 0.8328 - footwear_output_loss: 0.8286 - pose_output_loss: 0.6648 - emotion_output_loss: 0.8743 - gender_output_acc: 0.7624 - image_quality_output_acc: 0.5889 - age_output_acc: 0.4085 - weight_output_acc: 0.6373 - bag_output_acc: 0.6241 - footwear_output_acc: 0.6205 - pose_output_acc: 0.7185 - emotion_output_acc: 0.7116 - val_loss: 6.7744 - val_gender_output_loss: 0.4214 - val_image_quality_output_loss: 0.9131 - val_age_output_loss: 1.3671 - val_weight_output_loss: 0.9576 - val_bag_output_loss: 0.8234 - val_footwear_output_loss: 0.8756 - val_pose_output_loss: 0.5523 - val_emotion_output_loss: 0.8639 - val_gender_output_acc: 0.8013 - val_image_quality_output_acc: 0.5650 - val_age_output_acc: 0.3938 - val_weight_output_acc: 0.6305 - val_bag_output_acc: 0.6344 - val_footwear_output_acc: 0.6096 - val_pose_output_acc: 0.7698 - val_emotion_output_acc: 0.7173\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 6.74968\n",
            "Epoch 27/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.8400 - gender_output_loss: 0.4763 - image_quality_output_loss: 0.8629 - age_output_loss: 1.3715 - weight_output_loss: 0.9370 - bag_output_loss: 0.8325 - footwear_output_loss: 0.8259 - pose_output_loss: 0.6585 - emotion_output_loss: 0.8753 - gender_output_acc: 0.7709 - image_quality_output_acc: 0.5889 - age_output_acc: 0.4087 - weight_output_acc: 0.6394 - bag_output_acc: 0.6279 - footwear_output_acc: 0.6243 - pose_output_acc: 0.7226 - emotion_output_acc: 0.7109 - val_loss: 6.8398 - val_gender_output_loss: 0.4510 - val_image_quality_output_loss: 0.9179 - val_age_output_loss: 1.3771 - val_weight_output_loss: 0.9663 - val_bag_output_loss: 0.8831 - val_footwear_output_loss: 0.8364 - val_pose_output_loss: 0.5390 - val_emotion_output_loss: 0.8690 - val_gender_output_acc: 0.7870 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3889 - val_weight_output_acc: 0.6275 - val_bag_output_acc: 0.5923 - val_footwear_output_acc: 0.6146 - val_pose_output_acc: 0.7803 - val_emotion_output_acc: 0.7138\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 6.74968\n",
            "Epoch 28/100\n",
            "360/360 [==============================] - 61s 169ms/step - loss: 6.8056 - gender_output_loss: 0.4637 - image_quality_output_loss: 0.8628 - age_output_loss: 1.3670 - weight_output_loss: 0.9389 - bag_output_loss: 0.8302 - footwear_output_loss: 0.8227 - pose_output_loss: 0.6464 - emotion_output_loss: 0.8740 - gender_output_acc: 0.7766 - image_quality_output_acc: 0.5827 - age_output_acc: 0.4073 - weight_output_acc: 0.6396 - bag_output_acc: 0.6298 - footwear_output_acc: 0.6286 - pose_output_acc: 0.7260 - emotion_output_acc: 0.7102 - val_loss: 6.8066 - val_gender_output_loss: 0.4734 - val_image_quality_output_loss: 0.8778 - val_age_output_loss: 1.3630 - val_weight_output_loss: 0.9663 - val_bag_output_loss: 0.8571 - val_footwear_output_loss: 0.8516 - val_pose_output_loss: 0.5490 - val_emotion_output_loss: 0.8682 - val_gender_output_acc: 0.7775 - val_image_quality_output_acc: 0.5823 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.6062 - val_footwear_output_acc: 0.6126 - val_pose_output_acc: 0.7862 - val_emotion_output_acc: 0.7158\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 6.74968\n",
            "Epoch 29/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.7778 - gender_output_loss: 0.4611 - image_quality_output_loss: 0.8610 - age_output_loss: 1.3639 - weight_output_loss: 0.9335 - bag_output_loss: 0.8259 - footwear_output_loss: 0.8188 - pose_output_loss: 0.6390 - emotion_output_loss: 0.8746 - gender_output_acc: 0.7721 - image_quality_output_acc: 0.5862 - age_output_acc: 0.4089 - weight_output_acc: 0.6402 - bag_output_acc: 0.6297 - footwear_output_acc: 0.6303 - pose_output_acc: 0.7303 - emotion_output_acc: 0.7102 - val_loss: 6.6595 - val_gender_output_loss: 0.4006 - val_image_quality_output_loss: 0.8547 - val_age_output_loss: 1.3616 - val_weight_output_loss: 0.9619 - val_bag_output_loss: 0.8436 - val_footwear_output_loss: 0.8356 - val_pose_output_loss: 0.5448 - val_emotion_output_loss: 0.8567 - val_gender_output_acc: 0.8028 - val_image_quality_output_acc: 0.5942 - val_age_output_acc: 0.3953 - val_weight_output_acc: 0.6190 - val_bag_output_acc: 0.6300 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.7753 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00029: val_loss improved from 6.74968 to 6.65951, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.029.h5\n",
            "Epoch 30/100\n",
            "360/360 [==============================] - 61s 171ms/step - loss: 6.7364 - gender_output_loss: 0.4498 - image_quality_output_loss: 0.8571 - age_output_loss: 1.3591 - weight_output_loss: 0.9311 - bag_output_loss: 0.8249 - footwear_output_loss: 0.8143 - pose_output_loss: 0.6287 - emotion_output_loss: 0.8715 - gender_output_acc: 0.7810 - image_quality_output_acc: 0.5912 - age_output_acc: 0.4102 - weight_output_acc: 0.6385 - bag_output_acc: 0.6283 - footwear_output_acc: 0.6303 - pose_output_acc: 0.7354 - emotion_output_acc: 0.7107 - val_loss: 6.6683 - val_gender_output_loss: 0.4026 - val_image_quality_output_loss: 0.8589 - val_age_output_loss: 1.3659 - val_weight_output_loss: 0.9736 - val_bag_output_loss: 0.8349 - val_footwear_output_loss: 0.8448 - val_pose_output_loss: 0.5302 - val_emotion_output_loss: 0.8575 - val_gender_output_acc: 0.8115 - val_image_quality_output_acc: 0.5938 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.6181 - val_footwear_output_acc: 0.6027 - val_pose_output_acc: 0.7852 - val_emotion_output_acc: 0.7128\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 6.65951\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00029: val_loss improved from 6.74968 to 6.65951, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.029.h5\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.7146 - gender_output_loss: 0.4449 - image_quality_output_loss: 0.8537 - age_output_loss: 1.3624 - weight_output_loss: 0.9303 - bag_output_loss: 0.8190 - footwear_output_loss: 0.8103 - pose_output_loss: 0.6226 - emotion_output_loss: 0.8714 - gender_output_acc: 0.7877 - image_quality_output_acc: 0.5933 - age_output_acc: 0.4111 - weight_output_acc: 0.6383 - bag_output_acc: 0.6338 - footwear_output_acc: 0.6328 - pose_output_acc: 0.7389 - emotion_output_acc: 0.7114 - val_loss: 6.6051 - val_gender_output_loss: 0.3924 - val_image_quality_output_loss: 0.8513 - val_age_output_loss: 1.3589 - val_weight_output_loss: 0.9576 - val_bag_output_loss: 0.8212 - val_footwear_output_loss: 0.8395 - val_pose_output_loss: 0.5213 - val_emotion_output_loss: 0.8629 - val_gender_output_acc: 0.8085 - val_image_quality_output_acc: 0.5878 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.6434 - val_footwear_output_acc: 0.6186 - val_pose_output_acc: 0.7922 - val_emotion_output_acc: 0.7123\n",
            "\n",
            "Epoch 00031: val_loss improved from 6.65951 to 6.60513, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.031.h5\n",
            "Epoch 32/100\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 6.7115 - gender_output_loss: 0.4459 - image_quality_output_loss: 0.8589 - age_output_loss: 1.3579 - weight_output_loss: 0.9294 - bag_output_loss: 0.8193 - footwear_output_loss: 0.8081 - pose_output_loss: 0.6213 - emotion_output_loss: 0.8707 - gender_output_acc: 0.7876 - image_quality_output_acc: 0.5874 - age_output_acc: 0.4103 - weight_output_acc: 0.6378 - bag_output_acc: 0.6304 - footwear_output_acc: 0.6424 - pose_output_acc: 0.7375 - emotion_output_acc: 0.7109 - val_loss: 6.7352 - val_gender_output_loss: 0.3858 - val_image_quality_output_loss: 0.9350 - val_age_output_loss: 1.3893 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.8306 - val_footwear_output_loss: 0.8492 - val_pose_output_loss: 0.5002 - val_emotion_output_loss: 0.8589 - val_gender_output_acc: 0.8266 - val_image_quality_output_acc: 0.5362 - val_age_output_acc: 0.3795 - val_weight_output_acc: 0.6146 - val_bag_output_acc: 0.6280 - val_footwear_output_acc: 0.6076 - val_pose_output_acc: 0.7897 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 6.60513\n",
            "Epoch 33/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.6859 - gender_output_loss: 0.4396 - image_quality_output_loss: 0.8533 - age_output_loss: 1.3606 - weight_output_loss: 0.9308 - bag_output_loss: 0.8175 - footwear_output_loss: 0.8026 - pose_output_loss: 0.6114 - emotion_output_loss: 0.8701 - gender_output_acc: 0.7876 - image_quality_output_acc: 0.5939 - age_output_acc: 0.4134 - weight_output_acc: 0.6391 - bag_output_acc: 0.6395 - footwear_output_acc: 0.6407 - pose_output_acc: 0.7440 - emotion_output_acc: 0.7111 - val_loss: 6.8809 - val_gender_output_loss: 0.4661 - val_image_quality_output_loss: 0.8863 - val_age_output_loss: 1.3570 - val_weight_output_loss: 0.9601 - val_bag_output_loss: 0.9821 - val_footwear_output_loss: 0.8366 - val_pose_output_loss: 0.5228 - val_emotion_output_loss: 0.8700 - val_gender_output_acc: 0.7778 - val_image_quality_output_acc: 0.5818 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6215 - val_bag_output_acc: 0.5823 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.7996 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 6.60513\n",
            "Epoch 34/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.6601 - gender_output_loss: 0.4344 - image_quality_output_loss: 0.8523 - age_output_loss: 1.3551 - weight_output_loss: 0.9228 - bag_output_loss: 0.8191 - footwear_output_loss: 0.8017 - pose_output_loss: 0.6076 - emotion_output_loss: 0.8671 - gender_output_acc: 0.7927 - image_quality_output_acc: 0.5946 - age_output_acc: 0.4092 - weight_output_acc: 0.6429 - bag_output_acc: 0.6385 - footwear_output_acc: 0.6409 - pose_output_acc: 0.7427 - emotion_output_acc: 0.7106 - val_loss: 6.5955 - val_gender_output_loss: 0.3833 - val_image_quality_output_loss: 0.8757 - val_age_output_loss: 1.3619 - val_weight_output_loss: 0.9628 - val_bag_output_loss: 0.8179 - val_footwear_output_loss: 0.8219 - val_pose_output_loss: 0.5172 - val_emotion_output_loss: 0.8548 - val_gender_output_acc: 0.8299 - val_image_quality_output_acc: 0.5863 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6250 - val_bag_output_acc: 0.6513 - val_footwear_output_acc: 0.6354 - val_pose_output_acc: 0.7877 - val_emotion_output_acc: 0.7143\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 6.60513\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: val_loss improved from 6.60513 to 6.59546, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.034.h5\n",
            "Epoch 35/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.6353 - gender_output_loss: 0.4325 - image_quality_output_loss: 0.8543 - age_output_loss: 1.3514 - weight_output_loss: 0.9250 - bag_output_loss: 0.8106 - footwear_output_loss: 0.7980 - pose_output_loss: 0.5956 - emotion_output_loss: 0.8677 - gender_output_acc: 0.7932 - image_quality_output_acc: 0.5905 - age_output_acc: 0.4163 - weight_output_acc: 0.6424 - bag_output_acc: 0.6398 - footwear_output_acc: 0.6445 - pose_output_acc: 0.7519 - emotion_output_acc: 0.7114 - val_loss: 6.6336 - val_gender_output_loss: 0.3979 - val_image_quality_output_loss: 0.9467 - val_age_output_loss: 1.3498 - val_weight_output_loss: 0.9573 - val_bag_output_loss: 0.8144 - val_footwear_output_loss: 0.8085 - val_pose_output_loss: 0.4955 - val_emotion_output_loss: 0.8636 - val_gender_output_acc: 0.8197 - val_image_quality_output_acc: 0.5437 - val_age_output_acc: 0.3993 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.6419 - val_footwear_output_acc: 0.6319 - val_pose_output_acc: 0.8006 - val_emotion_output_acc: 0.7143\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 6.59546\n",
            "Epoch 36/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.6102 - gender_output_loss: 0.4193 - image_quality_output_loss: 0.8524 - age_output_loss: 1.3537 - weight_output_loss: 0.9239 - bag_output_loss: 0.8108 - footwear_output_loss: 0.7963 - pose_output_loss: 0.5869 - emotion_output_loss: 0.8669 - gender_output_acc: 0.8006 - image_quality_output_acc: 0.5931 - age_output_acc: 0.4076 - weight_output_acc: 0.6391 - bag_output_acc: 0.6403 - footwear_output_acc: 0.6399 - pose_output_acc: 0.7560 - emotion_output_acc: 0.7102 - val_loss: 6.6476 - val_gender_output_loss: 0.3865 - val_image_quality_output_loss: 0.8900 - val_age_output_loss: 1.4005 - val_weight_output_loss: 0.9730 - val_bag_output_loss: 0.8209 - val_footwear_output_loss: 0.8166 - val_pose_output_loss: 0.5012 - val_emotion_output_loss: 0.8589 - val_gender_output_acc: 0.8251 - val_image_quality_output_acc: 0.5699 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6438 - val_footwear_output_acc: 0.6245 - val_pose_output_acc: 0.7877 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 6.59546\n",
            "Epoch 37/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.5822 - gender_output_loss: 0.4146 - image_quality_output_loss: 0.8508 - age_output_loss: 1.3489 - weight_output_loss: 0.9188 - bag_output_loss: 0.8041 - footwear_output_loss: 0.7976 - pose_output_loss: 0.5833 - emotion_output_loss: 0.8641 - gender_output_acc: 0.8077 - image_quality_output_acc: 0.5950 - age_output_acc: 0.4127 - weight_output_acc: 0.6417 - bag_output_acc: 0.6435 - footwear_output_acc: 0.6389 - pose_output_acc: 0.7555 - emotion_output_acc: 0.7104 - val_loss: 6.5983 - val_gender_output_loss: 0.3972 - val_image_quality_output_loss: 0.9155 - val_age_output_loss: 1.3474 - val_weight_output_loss: 0.9566 - val_bag_output_loss: 0.8251 - val_footwear_output_loss: 0.8162 - val_pose_output_loss: 0.4878 - val_emotion_output_loss: 0.8525 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.3919 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6275 - val_footwear_output_acc: 0.6260 - val_pose_output_acc: 0.8070 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 6.59546\n",
            "Epoch 38/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.5812 - gender_output_loss: 0.4147 - image_quality_output_loss: 0.8498 - age_output_loss: 1.3480 - weight_output_loss: 0.9200 - bag_output_loss: 0.8056 - footwear_output_loss: 0.7953 - pose_output_loss: 0.5839 - emotion_output_loss: 0.8638 - gender_output_acc: 0.8080 - image_quality_output_acc: 0.5989 - age_output_acc: 0.4182 - weight_output_acc: 0.6428 - bag_output_acc: 0.6402 - footwear_output_acc: 0.6447 - pose_output_acc: 0.7551 - emotion_output_acc: 0.7106 - val_loss: 6.5646 - val_gender_output_loss: 0.3733 - val_image_quality_output_loss: 0.8605 - val_age_output_loss: 1.3554 - val_weight_output_loss: 0.9639 - val_bag_output_loss: 0.8093 - val_footwear_output_loss: 0.8378 - val_pose_output_loss: 0.5099 - val_emotion_output_loss: 0.8544 - val_gender_output_acc: 0.8229 - val_image_quality_output_acc: 0.5863 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.6523 - val_footwear_output_acc: 0.6086 - val_pose_output_acc: 0.7986 - val_emotion_output_acc: 0.7143\n",
            "\n",
            "Epoch 00038: val_loss improved from 6.59546 to 6.56462, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.038.h5\n",
            "Epoch 39/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.5607 - gender_output_loss: 0.4118 - image_quality_output_loss: 0.8514 - age_output_loss: 1.3430 - weight_output_loss: 0.9158 - bag_output_loss: 0.8021 - footwear_output_loss: 0.7923 - pose_output_loss: 0.5795 - emotion_output_loss: 0.8649 - gender_output_acc: 0.8053 - image_quality_output_acc: 0.5938 - age_output_acc: 0.4225 - weight_output_acc: 0.6413 - bag_output_acc: 0.6440 - footwear_output_acc: 0.6419 - pose_output_acc: 0.7609 - emotion_output_acc: 0.7101 - val_loss: 6.5889 - val_gender_output_loss: 0.3956 - val_image_quality_output_loss: 0.8589 - val_age_output_loss: 1.3572 - val_weight_output_loss: 0.9739 - val_bag_output_loss: 0.8177 - val_footwear_output_loss: 0.8348 - val_pose_output_loss: 0.4786 - val_emotion_output_loss: 0.8723 - val_gender_output_acc: 0.8182 - val_image_quality_output_acc: 0.5977 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.6399 - val_footwear_output_acc: 0.6329 - val_pose_output_acc: 0.8145 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 6.56462\n",
            "Epoch 40/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.5263 - gender_output_loss: 0.4098 - image_quality_output_loss: 0.8493 - age_output_loss: 1.3432 - weight_output_loss: 0.9147 - bag_output_loss: 0.7997 - footwear_output_loss: 0.7865 - pose_output_loss: 0.5602 - emotion_output_loss: 0.8629 - gender_output_acc: 0.8062 - image_quality_output_acc: 0.5949 - age_output_acc: 0.4203 - weight_output_acc: 0.6411 - bag_output_acc: 0.6441 - footwear_output_acc: 0.6501 - pose_output_acc: 0.7684 - emotion_output_acc: 0.7111 - val_loss: 6.7213 - val_gender_output_loss: 0.4336 - val_image_quality_output_loss: 0.8973 - val_age_output_loss: 1.3487 - val_weight_output_loss: 0.9520 - val_bag_output_loss: 0.9125 - val_footwear_output_loss: 0.8275 - val_pose_output_loss: 0.4896 - val_emotion_output_loss: 0.8602 - val_gender_output_acc: 0.7991 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6334 - val_bag_output_acc: 0.5933 - val_footwear_output_acc: 0.6230 - val_pose_output_acc: 0.8095 - val_emotion_output_acc: 0.7083\n",
            "\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 6.56462\n",
            "Epoch 41/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 6.5162 - gender_output_loss: 0.4059 - image_quality_output_loss: 0.8459 - age_output_loss: 1.3383 - weight_output_loss: 0.9153 - bag_output_loss: 0.7946 - footwear_output_loss: 0.7854 - pose_output_loss: 0.5673 - emotion_output_loss: 0.8635 - gender_output_acc: 0.8104 - image_quality_output_acc: 0.5880 - age_output_acc: 0.4206 - weight_output_acc: 0.6415 - bag_output_acc: 0.6518 - footwear_output_acc: 0.6493 - pose_output_acc: 0.7668 - emotion_output_acc: 0.7110 - val_loss: 6.5392 - val_gender_output_loss: 0.3746 - val_image_quality_output_loss: 0.8548 - val_age_output_loss: 1.3558 - val_weight_output_loss: 0.9706 - val_bag_output_loss: 0.8007 - val_footwear_output_loss: 0.8316 - val_pose_output_loss: 0.4891 - val_emotion_output_loss: 0.8621 - val_gender_output_acc: 0.8232 - val_image_quality_output_acc: 0.5947 - val_age_output_acc: 0.3948 - val_weight_output_acc: 0.6265 - val_bag_output_acc: 0.6538 - val_footwear_output_acc: 0.6166 - val_pose_output_acc: 0.8070 - val_emotion_output_acc: 0.7113\n",
            "\n",
            "Epoch 00041: val_loss improved from 6.56462 to 6.53923, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.041.h5\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 6.56462\n",
            "Epoch 41/100\n",
            "Epoch 42/100\n",
            "360/360 [==============================] - 61s 169ms/step - loss: 6.4913 - gender_output_loss: 0.4003 - image_quality_output_loss: 0.8419 - age_output_loss: 1.3357 - weight_output_loss: 0.9129 - bag_output_loss: 0.7944 - footwear_output_loss: 0.7848 - pose_output_loss: 0.5603 - emotion_output_loss: 0.8610 - gender_output_acc: 0.8142 - image_quality_output_acc: 0.5970 - age_output_acc: 0.4220 - weight_output_acc: 0.6402 - bag_output_acc: 0.6510 - footwear_output_acc: 0.6499 - pose_output_acc: 0.7701 - emotion_output_acc: 0.7105 - val_loss: 6.6595 - val_gender_output_loss: 0.3641 - val_image_quality_output_loss: 0.9744 - val_age_output_loss: 1.3534 - val_weight_output_loss: 0.9594 - val_bag_output_loss: 0.8270 - val_footwear_output_loss: 0.8182 - val_pose_output_loss: 0.4939 - val_emotion_output_loss: 0.8690 - val_gender_output_acc: 0.8358 - val_image_quality_output_acc: 0.5263 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.6290 - val_bag_output_acc: 0.6359 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.8026 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 6.53923\n",
            "Epoch 43/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 6.4713 - gender_output_loss: 0.4020 - image_quality_output_loss: 0.8401 - age_output_loss: 1.3367 - weight_output_loss: 0.9078 - bag_output_loss: 0.7889 - footwear_output_loss: 0.7838 - pose_output_loss: 0.5498 - emotion_output_loss: 0.8622 - gender_output_acc: 0.8094 - image_quality_output_acc: 0.6060 - age_output_acc: 0.4212 - weight_output_acc: 0.6417 - bag_output_acc: 0.6582 - footwear_output_acc: 0.6496 - pose_output_acc: 0.7709 - emotion_output_acc: 0.7102 - val_loss: 6.7320 - val_gender_output_loss: 0.4015 - val_image_quality_output_loss: 0.8771 - val_age_output_loss: 1.3521 - val_weight_output_loss: 0.9818 - val_bag_output_loss: 0.8768 - val_footwear_output_loss: 0.8881 - val_pose_output_loss: 0.5000 - val_emotion_output_loss: 0.8546 - val_gender_output_acc: 0.8304 - val_image_quality_output_acc: 0.5873 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6210 - val_bag_output_acc: 0.6200 - val_footwear_output_acc: 0.6066 - val_pose_output_acc: 0.8135 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 6.53923\n",
            "Epoch 44/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 6.4577 - gender_output_loss: 0.3935 - image_quality_output_loss: 0.8440 - age_output_loss: 1.3379 - weight_output_loss: 0.9104 - bag_output_loss: 0.7910 - footwear_output_loss: 0.7758 - pose_output_loss: 0.5465 - emotion_output_loss: 0.8587 - gender_output_acc: 0.8181 - image_quality_output_acc: 0.5979 - age_output_acc: 0.4154 - weight_output_acc: 0.6385 - bag_output_acc: 0.6553 - footwear_output_acc: 0.6532 - pose_output_acc: 0.7722 - emotion_output_acc: 0.7108 - val_loss: 6.5720 - val_gender_output_loss: 0.3563 - val_image_quality_output_loss: 0.8910 - val_age_output_loss: 1.3674 - val_weight_output_loss: 0.9678 - val_bag_output_loss: 0.8209 - val_footwear_output_loss: 0.8222 - val_pose_output_loss: 0.4855 - val_emotion_output_loss: 0.8610 - val_gender_output_acc: 0.8447 - val_image_quality_output_acc: 0.5813 - val_age_output_acc: 0.3790 - val_weight_output_acc: 0.6126 - val_bag_output_acc: 0.6379 - val_footwear_output_acc: 0.6181 - val_pose_output_acc: 0.7996 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 6.53923\n",
            "Epoch 45/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.4332 - gender_output_loss: 0.3878 - image_quality_output_loss: 0.8352 - age_output_loss: 1.3309 - weight_output_loss: 0.9019 - bag_output_loss: 0.7918 - footwear_output_loss: 0.7780 - pose_output_loss: 0.5488 - emotion_output_loss: 0.8589 - gender_output_acc: 0.8243 - image_quality_output_acc: 0.6046 - age_output_acc: 0.4229 - weight_output_acc: 0.6461 - bag_output_acc: 0.6544 - footwear_output_acc: 0.6543 - pose_output_acc: 0.7774 - emotion_output_acc: 0.7109 - val_loss: 6.5361 - val_gender_output_loss: 0.3387 - val_image_quality_output_loss: 0.8591 - val_age_output_loss: 1.3860 - val_weight_output_loss: 1.0056 - val_bag_output_loss: 0.8003 - val_footwear_output_loss: 0.8091 - val_pose_output_loss: 0.4763 - val_emotion_output_loss: 0.8610 - val_gender_output_acc: 0.8490 - val_image_quality_output_acc: 0.6002 - val_age_output_acc: 0.3834 - val_weight_output_acc: 0.5838 - val_bag_output_acc: 0.6533 - val_footwear_output_acc: 0.6394 - val_pose_output_acc: 0.8170 - val_emotion_output_acc: 0.7118\n",
            "\n",
            "Epoch 00045: val_loss improved from 6.53923 to 6.53606, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.045.h5\n",
            "Epoch 46/100\n",
            "360/360 [==============================] - 61s 169ms/step - loss: 6.4165 - gender_output_loss: 0.3933 - image_quality_output_loss: 0.8439 - age_output_loss: 1.3298 - weight_output_loss: 0.9050 - bag_output_loss: 0.7837 - footwear_output_loss: 0.7704 - pose_output_loss: 0.5355 - emotion_output_loss: 0.8551 - gender_output_acc: 0.8182 - image_quality_output_acc: 0.5997 - age_output_acc: 0.4232 - weight_output_acc: 0.6426 - bag_output_acc: 0.6558 - footwear_output_acc: 0.6523 - pose_output_acc: 0.7858 - emotion_output_acc: 0.7117 - val_loss: 6.6517 - val_gender_output_loss: 0.3824 - val_image_quality_output_loss: 0.8920 - val_age_output_loss: 1.3528 - val_weight_output_loss: 0.9682 - val_bag_output_loss: 0.8825 - val_footwear_output_loss: 0.8170 - val_pose_output_loss: 0.4921 - val_emotion_output_loss: 0.8647 - val_gender_output_acc: 0.8259 - val_image_quality_output_acc: 0.5660 - val_age_output_acc: 0.3869 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.6086 - val_footwear_output_acc: 0.6394 - val_pose_output_acc: 0.8031 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 6.53606\n",
            "Epoch 47/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.3903 - gender_output_loss: 0.3890 - image_quality_output_loss: 0.8393 - age_output_loss: 1.3253 - weight_output_loss: 0.8981 - bag_output_loss: 0.7804 - footwear_output_loss: 0.7688 - pose_output_loss: 0.5325 - emotion_output_loss: 0.8569 - gender_output_acc: 0.8196 - image_quality_output_acc: 0.6023 - age_output_acc: 0.4203 - weight_output_acc: 0.6473 - bag_output_acc: 0.6576 - footwear_output_acc: 0.6569 - pose_output_acc: 0.7835 - emotion_output_acc: 0.7102 - val_loss: 6.5786 - val_gender_output_loss: 0.3624 - val_image_quality_output_loss: 0.8749 - val_age_output_loss: 1.3385 - val_weight_output_loss: 0.9520 - val_bag_output_loss: 0.8133 - val_footwear_output_loss: 0.8268 - val_pose_output_loss: 0.5471 - val_emotion_output_loss: 0.8637 - val_gender_output_acc: 0.8380 - val_image_quality_output_acc: 0.5972 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6409 - val_footwear_output_acc: 0.6290 - val_pose_output_acc: 0.7867 - val_emotion_output_acc: 0.7158\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 6.53606\n",
            "Epoch 48/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.3814 - gender_output_loss: 0.3835 - image_quality_output_loss: 0.8401 - age_output_loss: 1.3227 - weight_output_loss: 0.9024 - bag_output_loss: 0.7774 - footwear_output_loss: 0.7702 - pose_output_loss: 0.5302 - emotion_output_loss: 0.8549 - gender_output_acc: 0.8235 - image_quality_output_acc: 0.6046 - age_output_acc: 0.4287 - weight_output_acc: 0.6432 - bag_output_acc: 0.6661 - footwear_output_acc: 0.6578 - pose_output_acc: 0.7862 - emotion_output_acc: 0.7099 - val_loss: 6.4467 - val_gender_output_loss: 0.3495 - val_image_quality_output_loss: 0.8558 - val_age_output_loss: 1.3440 - val_weight_output_loss: 0.9571 - val_bag_output_loss: 0.8168 - val_footwear_output_loss: 0.8034 - val_pose_output_loss: 0.4620 - val_emotion_output_loss: 0.8579 - val_gender_output_acc: 0.8418 - val_image_quality_output_acc: 0.5957 - val_age_output_acc: 0.3924 - val_weight_output_acc: 0.6374 - val_bag_output_acc: 0.6528 - val_footwear_output_acc: 0.6399 - val_pose_output_acc: 0.8185 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00048: val_loss improved from 6.53606 to 6.44670, saving model to /content/gdrive/My Drive/EIP_4_Computer_vision/Week_5/saved_models/v2_person_attr%s_model.048.h5\n",
            "Epoch 49/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.3505 - gender_output_loss: 0.3786 - image_quality_output_loss: 0.8376 - age_output_loss: 1.3213 - weight_output_loss: 0.8952 - bag_output_loss: 0.7725 - footwear_output_loss: 0.7644 - pose_output_loss: 0.5272 - emotion_output_loss: 0.8537 - gender_output_acc: 0.8223 - image_quality_output_acc: 0.6022 - age_output_acc: 0.4199 - weight_output_acc: 0.6467 - bag_output_acc: 0.6642 - footwear_output_acc: 0.6580 - pose_output_acc: 0.7892 - emotion_output_acc: 0.7109 - val_loss: 6.4988 - val_gender_output_loss: 0.3576 - val_image_quality_output_loss: 0.8556 - val_age_output_loss: 1.3701 - val_weight_output_loss: 0.9719 - val_bag_output_loss: 0.8116 - val_footwear_output_loss: 0.8118 - val_pose_output_loss: 0.4669 - val_emotion_output_loss: 0.8532 - val_gender_output_acc: 0.8423 - val_image_quality_output_acc: 0.5908 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6156 - val_bag_output_acc: 0.6453 - val_footwear_output_acc: 0.6334 - val_pose_output_acc: 0.8155 - val_emotion_output_acc: 0.7093\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 6.44670\n",
            "Epoch 50/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 6.3390 - gender_output_loss: 0.3774 - image_quality_output_loss: 0.8379 - age_output_loss: 1.3221 - weight_output_loss: 0.8932 - bag_output_loss: 0.7726 - footwear_output_loss: 0.7626 - pose_output_loss: 0.5206 - emotion_output_loss: 0.8525 - gender_output_acc: 0.8231 - image_quality_output_acc: 0.6024 - age_output_acc: 0.4257 - weight_output_acc: 0.6451 - bag_output_acc: 0.6624 - footwear_output_acc: 0.6580 - pose_output_acc: 0.7882 - emotion_output_acc: 0.7112 - val_loss: 6.7152 - val_gender_output_loss: 0.4676 - val_image_quality_output_loss: 0.8494 - val_age_output_loss: 1.3490 - val_weight_output_loss: 0.9564 - val_bag_output_loss: 0.8781 - val_footwear_output_loss: 0.8153 - val_pose_output_loss: 0.5389 - val_emotion_output_loss: 0.8605 - val_gender_output_acc: 0.7929 - val_image_quality_output_acc: 0.5888 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6166 - val_bag_output_acc: 0.6290 - val_footwear_output_acc: 0.6349 - val_pose_output_acc: 0.7753 - val_emotion_output_acc: 0.7093\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 6.44670\n",
            "Epoch 51/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.3187 - gender_output_loss: 0.3719 - image_quality_output_loss: 0.8359 - age_output_loss: 1.3150 - weight_output_loss: 0.8915 - bag_output_loss: 0.7745 - footwear_output_loss: 0.7640 - pose_output_loss: 0.5134 - emotion_output_loss: 0.8526 - gender_output_acc: 0.8266 - image_quality_output_acc: 0.6025 - age_output_acc: 0.4253 - weight_output_acc: 0.6457 - bag_output_acc: 0.6642 - footwear_output_acc: 0.6573 - pose_output_acc: 0.7892 - emotion_output_acc: 0.7103 - val_loss: 6.4947 - val_gender_output_loss: 0.3433 - val_image_quality_output_loss: 0.8878 - val_age_output_loss: 1.3393 - val_weight_output_loss: 0.9635 - val_bag_output_loss: 0.8239 - val_footwear_output_loss: 0.8093 - val_pose_output_loss: 0.4708 - val_emotion_output_loss: 0.8567 - val_gender_output_acc: 0.8400 - val_image_quality_output_acc: 0.5779 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6424 - val_footwear_output_acc: 0.6399 - val_pose_output_acc: 0.8194 - val_emotion_output_acc: 0.7098\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 6.44670\n",
            "Epoch 52/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.2918 - gender_output_loss: 0.3642 - image_quality_output_loss: 0.8331 - age_output_loss: 1.3128 - weight_output_loss: 0.8929 - bag_output_loss: 0.7729 - footwear_output_loss: 0.7602 - pose_output_loss: 0.5060 - emotion_output_loss: 0.8498 - gender_output_acc: 0.8377 - image_quality_output_acc: 0.6073 - age_output_acc: 0.4352 - weight_output_acc: 0.6455 - bag_output_acc: 0.6663 - footwear_output_acc: 0.6596 - pose_output_acc: 0.7921 - emotion_output_acc: 0.7111 - val_loss: 6.7181 - val_gender_output_loss: 0.5450 - val_image_quality_output_loss: 0.8632 - val_age_output_loss: 1.3389 - val_weight_output_loss: 0.9840 - val_bag_output_loss: 0.8424 - val_footwear_output_loss: 0.8172 - val_pose_output_loss: 0.4749 - val_emotion_output_loss: 0.8523 - val_gender_output_acc: 0.7631 - val_image_quality_output_acc: 0.5898 - val_age_output_acc: 0.4112 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.6334 - val_footwear_output_acc: 0.6319 - val_pose_output_acc: 0.8170 - val_emotion_output_acc: 0.7083\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 6.44670\n",
            "Epoch 53/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.2813 - gender_output_loss: 0.3613 - image_quality_output_loss: 0.8278 - age_output_loss: 1.3103 - weight_output_loss: 0.8866 - bag_output_loss: 0.7687 - footwear_output_loss: 0.7620 - pose_output_loss: 0.5136 - emotion_output_loss: 0.8510 - gender_output_acc: 0.8369 - image_quality_output_acc: 0.6033 - age_output_acc: 0.4316 - weight_output_acc: 0.6470 - bag_output_acc: 0.6676 - footwear_output_acc: 0.6570 - pose_output_acc: 0.7903 - emotion_output_acc: 0.7103 - val_loss: 6.5045 - val_gender_output_loss: 0.3599 - val_image_quality_output_loss: 0.8612 - val_age_output_loss: 1.3571 - val_weight_output_loss: 0.9665 - val_bag_output_loss: 0.8168 - val_footwear_output_loss: 0.8209 - val_pose_output_loss: 0.4589 - val_emotion_output_loss: 0.8631 - val_gender_output_acc: 0.8418 - val_image_quality_output_acc: 0.5853 - val_age_output_acc: 0.3983 - val_weight_output_acc: 0.6275 - val_bag_output_acc: 0.6434 - val_footwear_output_acc: 0.6379 - val_pose_output_acc: 0.8244 - val_emotion_output_acc: 0.7083\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 6.44670\n",
            "Epoch 54/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.2722 - gender_output_loss: 0.3561 - image_quality_output_loss: 0.8322 - age_output_loss: 1.3123 - weight_output_loss: 0.8948 - bag_output_loss: 0.7640 - footwear_output_loss: 0.7545 - pose_output_loss: 0.5107 - emotion_output_loss: 0.8476 - gender_output_acc: 0.8372 - image_quality_output_acc: 0.6081 - age_output_acc: 0.4292 - weight_output_acc: 0.6457 - bag_output_acc: 0.6669 - footwear_output_acc: 0.6647 - pose_output_acc: 0.7944 - emotion_output_acc: 0.7115 - val_loss: 6.6372 - val_gender_output_loss: 0.3599 - val_image_quality_output_loss: 0.9042 - val_age_output_loss: 1.3708 - val_weight_output_loss: 0.9845 - val_bag_output_loss: 0.8202 - val_footwear_output_loss: 0.8470 - val_pose_output_loss: 0.4756 - val_emotion_output_loss: 0.8750 - val_gender_output_acc: 0.8326 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6086 - val_bag_output_acc: 0.6463 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.8160 - val_emotion_output_acc: 0.7113\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 6.44670\n",
            "Epoch 55/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.2565 - gender_output_loss: 0.3680 - image_quality_output_loss: 0.8293 - age_output_loss: 1.3048 - weight_output_loss: 0.8842 - bag_output_loss: 0.7612 - footwear_output_loss: 0.7551 - pose_output_loss: 0.5076 - emotion_output_loss: 0.8464 - gender_output_acc: 0.8306 - image_quality_output_acc: 0.6094 - age_output_acc: 0.4306 - weight_output_acc: 0.6482 - bag_output_acc: 0.6693 - footwear_output_acc: 0.6678 - pose_output_acc: 0.7932 - emotion_output_acc: 0.7128 - val_loss: 6.8094 - val_gender_output_loss: 0.3545 - val_image_quality_output_loss: 0.9581 - val_age_output_loss: 1.3315 - val_weight_output_loss: 0.9528 - val_bag_output_loss: 0.8008 - val_footwear_output_loss: 0.9919 - val_pose_output_loss: 0.5461 - val_emotion_output_loss: 0.8738 - val_gender_output_acc: 0.8440 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.4142 - val_weight_output_acc: 0.6290 - val_bag_output_acc: 0.6488 - val_footwear_output_acc: 0.5670 - val_pose_output_acc: 0.7783 - val_emotion_output_acc: 0.7168\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 6.44670\n",
            "Epoch 56/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.2469 - gender_output_loss: 0.3610 - image_quality_output_loss: 0.8289 - age_output_loss: 1.3032 - weight_output_loss: 0.8840 - bag_output_loss: 0.7622 - footwear_output_loss: 0.7527 - pose_output_loss: 0.5056 - emotion_output_loss: 0.8493 - gender_output_acc: 0.8355 - image_quality_output_acc: 0.6020 - age_output_acc: 0.4347 - weight_output_acc: 0.6482 - bag_output_acc: 0.6721 - footwear_output_acc: 0.6622 - pose_output_acc: 0.7944 - emotion_output_acc: 0.7110 - val_loss: 6.5233 - val_gender_output_loss: 0.3532 - val_image_quality_output_loss: 0.8647 - val_age_output_loss: 1.3466 - val_weight_output_loss: 0.9814 - val_bag_output_loss: 0.8100 - val_footwear_output_loss: 0.8130 - val_pose_output_loss: 0.5001 - val_emotion_output_loss: 0.8541 - val_gender_output_acc: 0.8438 - val_image_quality_output_acc: 0.5754 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6235 - val_bag_output_acc: 0.6463 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.8080 - val_emotion_output_acc: 0.7123\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 6.44670\n",
            "Epoch 57/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.2018 - gender_output_loss: 0.3500 - image_quality_output_loss: 0.8258 - age_output_loss: 1.2978 - weight_output_loss: 0.8778 - bag_output_loss: 0.7618 - footwear_output_loss: 0.7497 - pose_output_loss: 0.4911 - emotion_output_loss: 0.8477 - gender_output_acc: 0.8420 - image_quality_output_acc: 0.6069 - age_output_acc: 0.4330 - weight_output_acc: 0.6478 - bag_output_acc: 0.6663 - footwear_output_acc: 0.6700 - pose_output_acc: 0.8004 - emotion_output_acc: 0.7127 - val_loss: 6.5819 - val_gender_output_loss: 0.3705 - val_image_quality_output_loss: 0.8608 - val_age_output_loss: 1.3709 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.8056 - val_footwear_output_loss: 0.8100 - val_pose_output_loss: 0.5295 - val_emotion_output_loss: 0.8577 - val_gender_output_acc: 0.8346 - val_image_quality_output_acc: 0.6002 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6245 - val_bag_output_acc: 0.6508 - val_footwear_output_acc: 0.6359 - val_pose_output_acc: 0.8041 - val_emotion_output_acc: 0.7059\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 6.44670\n",
            "Epoch 58/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.2020 - gender_output_loss: 0.3501 - image_quality_output_loss: 0.8287 - age_output_loss: 1.2993 - weight_output_loss: 0.8852 - bag_output_loss: 0.7513 - footwear_output_loss: 0.7534 - pose_output_loss: 0.4874 - emotion_output_loss: 0.8467 - gender_output_acc: 0.8474 - image_quality_output_acc: 0.6017 - age_output_acc: 0.4372 - weight_output_acc: 0.6470 - bag_output_acc: 0.6775 - footwear_output_acc: 0.6628 - pose_output_acc: 0.8023 - emotion_output_acc: 0.7101 - val_loss: 6.5862 - val_gender_output_loss: 0.3534 - val_image_quality_output_loss: 0.8687 - val_age_output_loss: 1.3896 - val_weight_output_loss: 0.9627 - val_bag_output_loss: 0.8089 - val_footwear_output_loss: 0.8206 - val_pose_output_loss: 0.5207 - val_emotion_output_loss: 0.8616 - val_gender_output_acc: 0.8435 - val_image_quality_output_acc: 0.5918 - val_age_output_acc: 0.3730 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6642 - val_footwear_output_acc: 0.6339 - val_pose_output_acc: 0.7932 - val_emotion_output_acc: 0.7123\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 6.44670\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 6.44670\n",
            "Epoch 58/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.1868 - gender_output_loss: 0.3498 - image_quality_output_loss: 0.8273 - age_output_loss: 1.2964 - weight_output_loss: 0.8755 - bag_output_loss: 0.7533 - footwear_output_loss: 0.7430 - pose_output_loss: 0.4978 - emotion_output_loss: 0.8438 - gender_output_acc: 0.8405 - image_quality_output_acc: 0.6087 - age_output_acc: 0.4333 - weight_output_acc: 0.6497 - bag_output_acc: 0.6768 - footwear_output_acc: 0.6674 - pose_output_acc: 0.7979 - emotion_output_acc: 0.7116 - val_loss: 6.7128 - val_gender_output_loss: 0.3762 - val_image_quality_output_loss: 0.9195 - val_age_output_loss: 1.3481 - val_weight_output_loss: 0.9701 - val_bag_output_loss: 0.8324 - val_footwear_output_loss: 0.8962 - val_pose_output_loss: 0.5084 - val_emotion_output_loss: 0.8619 - val_gender_output_acc: 0.8304 - val_image_quality_output_acc: 0.5387 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6493 - val_footwear_output_acc: 0.5898 - val_pose_output_acc: 0.8031 - val_emotion_output_acc: 0.7143\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 6.44670\n",
            "Epoch 60/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.1431 - gender_output_loss: 0.3419 - image_quality_output_loss: 0.8262 - age_output_loss: 1.2938 - weight_output_loss: 0.8730 - bag_output_loss: 0.7524 - footwear_output_loss: 0.7422 - pose_output_loss: 0.4734 - emotion_output_loss: 0.8403 - gender_output_acc: 0.8473 - image_quality_output_acc: 0.6076 - age_output_acc: 0.4416 - weight_output_acc: 0.6472 - bag_output_acc: 0.6720 - footwear_output_acc: 0.6723 - pose_output_acc: 0.8089 - emotion_output_acc: 0.7131 - val_loss: 6.4819 - val_gender_output_loss: 0.3491 - val_image_quality_output_loss: 0.8552 - val_age_output_loss: 1.3449 - val_weight_output_loss: 0.9651 - val_bag_output_loss: 0.8098 - val_footwear_output_loss: 0.8241 - val_pose_output_loss: 0.4695 - val_emotion_output_loss: 0.8642 - val_gender_output_acc: 0.8455 - val_image_quality_output_acc: 0.5878 - val_age_output_acc: 0.4053 - val_weight_output_acc: 0.6285 - val_bag_output_acc: 0.6592 - val_footwear_output_acc: 0.6250 - val_pose_output_acc: 0.8239 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 6.44670\n",
            "Epoch 61/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 6.1540 - gender_output_loss: 0.3444 - image_quality_output_loss: 0.8233 - age_output_loss: 1.2921 - weight_output_loss: 0.8749 - bag_output_loss: 0.7543 - footwear_output_loss: 0.7409 - pose_output_loss: 0.4829 - emotion_output_loss: 0.8412 - gender_output_acc: 0.8468 - image_quality_output_acc: 0.6076 - age_output_acc: 0.4409 - weight_output_acc: 0.6490 - bag_output_acc: 0.6747 - footwear_output_acc: 0.6729 - pose_output_acc: 0.8062 - emotion_output_acc: 0.7116 - val_loss: 6.5660 - val_gender_output_loss: 0.3764 - val_image_quality_output_loss: 0.9003 - val_age_output_loss: 1.3228 - val_weight_output_loss: 0.9631 - val_bag_output_loss: 0.8054 - val_footwear_output_loss: 0.8329 - val_pose_output_loss: 0.5006 - val_emotion_output_loss: 0.8644 - val_gender_output_acc: 0.8338 - val_image_quality_output_acc: 0.5680 - val_age_output_acc: 0.4132 - val_weight_output_acc: 0.6230 - val_bag_output_acc: 0.6493 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 6.44670\n",
            "Epoch 62/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.1237 - gender_output_loss: 0.3396 - image_quality_output_loss: 0.8222 - age_output_loss: 1.2865 - weight_output_loss: 0.8751 - bag_output_loss: 0.7441 - footwear_output_loss: 0.7417 - pose_output_loss: 0.4729 - emotion_output_loss: 0.8416 - gender_output_acc: 0.8475 - image_quality_output_acc: 0.6120 - age_output_acc: 0.4368 - weight_output_acc: 0.6511 - bag_output_acc: 0.6798 - footwear_output_acc: 0.6689 - pose_output_acc: 0.8062 - emotion_output_acc: 0.7112 - val_loss: 6.6225 - val_gender_output_loss: 0.3557 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.3401 - val_weight_output_loss: 0.9634 - val_bag_output_loss: 0.8026 - val_footwear_output_loss: 0.8298 - val_pose_output_loss: 0.4791 - val_emotion_output_loss: 0.8740 - val_gender_output_acc: 0.8393 - val_image_quality_output_acc: 0.5213 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6240 - val_bag_output_acc: 0.6493 - val_footwear_output_acc: 0.6245 - val_pose_output_acc: 0.8175 - val_emotion_output_acc: 0.7153\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 6.44670\n",
            "Epoch 63/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.0962 - gender_output_loss: 0.3372 - image_quality_output_loss: 0.8224 - age_output_loss: 1.2758 - weight_output_loss: 0.8726 - bag_output_loss: 0.7420 - footwear_output_loss: 0.7361 - pose_output_loss: 0.4697 - emotion_output_loss: 0.8402 - gender_output_acc: 0.8475 - image_quality_output_acc: 0.6127 - age_output_acc: 0.4409 - weight_output_acc: 0.6551 - bag_output_acc: 0.6807 - footwear_output_acc: 0.6688 - pose_output_acc: 0.8128 - emotion_output_acc: 0.7115 - val_loss: 6.5939 - val_gender_output_loss: 0.3461 - val_image_quality_output_loss: 0.8720 - val_age_output_loss: 1.3388 - val_weight_output_loss: 0.9876 - val_bag_output_loss: 0.8784 - val_footwear_output_loss: 0.8143 - val_pose_output_loss: 0.4841 - val_emotion_output_loss: 0.8726 - val_gender_output_acc: 0.8460 - val_image_quality_output_acc: 0.5903 - val_age_output_acc: 0.4092 - val_weight_output_acc: 0.6057 - val_bag_output_acc: 0.6300 - val_footwear_output_acc: 0.6290 - val_pose_output_acc: 0.8125 - val_emotion_output_acc: 0.7098\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 6.44670\n",
            "Epoch 64/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.0701 - gender_output_loss: 0.3389 - image_quality_output_loss: 0.8154 - age_output_loss: 1.2768 - weight_output_loss: 0.8632 - bag_output_loss: 0.7401 - footwear_output_loss: 0.7314 - pose_output_loss: 0.4698 - emotion_output_loss: 0.8343 - gender_output_acc: 0.8483 - image_quality_output_acc: 0.6153 - age_output_acc: 0.4452 - weight_output_acc: 0.6545 - bag_output_acc: 0.6806 - footwear_output_acc: 0.6793 - pose_output_acc: 0.8094 - emotion_output_acc: 0.7126 - val_loss: 6.6757 - val_gender_output_loss: 0.3737 - val_image_quality_output_loss: 0.8714 - val_age_output_loss: 1.3615 - val_weight_output_loss: 0.9899 - val_bag_output_loss: 0.8305 - val_footwear_output_loss: 0.8704 - val_pose_output_loss: 0.4970 - val_emotion_output_loss: 0.8814 - val_gender_output_acc: 0.8326 - val_image_quality_output_acc: 0.5913 - val_age_output_acc: 0.3859 - val_weight_output_acc: 0.6042 - val_bag_output_acc: 0.6523 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.8036 - val_emotion_output_acc: 0.7113\n",
            "360/360 [==============================]\n",
            "Epoch 00064: val_loss did not improve from 6.44670\n",
            "Epoch 65/100\n",
            "360/360 [==============================] - 61s 169ms/step - loss: 6.0566 - gender_output_loss: 0.3330 - image_quality_output_loss: 0.8153 - age_output_loss: 1.2778 - weight_output_loss: 0.8611 - bag_output_loss: 0.7355 - footwear_output_loss: 0.7320 - pose_output_loss: 0.4678 - emotion_output_loss: 0.8342 - gender_output_acc: 0.8503 - image_quality_output_acc: 0.6131 - age_output_acc: 0.4448 - weight_output_acc: 0.6561 - bag_output_acc: 0.6815 - footwear_output_acc: 0.6759 - pose_output_acc: 0.8084 - emotion_output_acc: 0.7125 - val_loss: 6.5619 - val_gender_output_loss: 0.3683 - val_image_quality_output_loss: 0.8713 - val_age_output_loss: 1.3295 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.8090 - val_footwear_output_loss: 0.8524 - val_pose_output_loss: 0.4767 - val_emotion_output_loss: 0.8757 - val_gender_output_acc: 0.8361 - val_image_quality_output_acc: 0.5908 - val_age_output_acc: 0.4008 - val_weight_output_acc: 0.6116 - val_bag_output_acc: 0.6647 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.8140 - val_emotion_output_acc: 0.7103\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 6.44670\n",
            "Epoch 66/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 6.0289 - gender_output_loss: 0.3258 - image_quality_output_loss: 0.8177 - age_output_loss: 1.2695 - weight_output_loss: 0.8622 - bag_output_loss: 0.7378 - footwear_output_loss: 0.7236 - pose_output_loss: 0.4568 - emotion_output_loss: 0.8355 - gender_output_acc: 0.8564 - image_quality_output_acc: 0.6138 - age_output_acc: 0.4455 - weight_output_acc: 0.6576 - bag_output_acc: 0.6861 - footwear_output_acc: 0.6767 - pose_output_acc: 0.8159 - emotion_output_acc: 0.7122 - val_loss: 6.8227 - val_gender_output_loss: 0.4130 - val_image_quality_output_loss: 0.8837 - val_age_output_loss: 1.4043 - val_weight_output_loss: 1.0079 - val_bag_output_loss: 0.8420 - val_footwear_output_loss: 0.8691 - val_pose_output_loss: 0.5303 - val_emotion_output_loss: 0.8722 - val_gender_output_acc: 0.8175 - val_image_quality_output_acc: 0.5833 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.5873 - val_bag_output_acc: 0.6339 - val_footwear_output_acc: 0.6275 - val_pose_output_acc: 0.7971 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 6.44670\n",
            "Epoch 67/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 6.0099 - gender_output_loss: 0.3282 - image_quality_output_loss: 0.8162 - age_output_loss: 1.2674 - weight_output_loss: 0.8509 - bag_output_loss: 0.7288 - footwear_output_loss: 0.7264 - pose_output_loss: 0.4589 - emotion_output_loss: 0.8331 - gender_output_acc: 0.8548 - image_quality_output_acc: 0.6141 - age_output_acc: 0.4461 - weight_output_acc: 0.6572 - bag_output_acc: 0.6920 - footwear_output_acc: 0.6761 - pose_output_acc: 0.8160 - emotion_output_acc: 0.7110 - val_loss: 6.5748 - val_gender_output_loss: 0.3404 - val_image_quality_output_loss: 0.8667 - val_age_output_loss: 1.3288 - val_weight_output_loss: 0.9826 - val_bag_output_loss: 0.8180 - val_footwear_output_loss: 0.8374 - val_pose_output_loss: 0.5004 - val_emotion_output_loss: 0.9005 - val_gender_output_acc: 0.8519 - val_image_quality_output_acc: 0.5823 - val_age_output_acc: 0.4087 - val_weight_output_acc: 0.6260 - val_bag_output_acc: 0.6404 - val_footwear_output_acc: 0.6131 - val_pose_output_acc: 0.8095 - val_emotion_output_acc: 0.7133\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 6.44670\n",
            "Epoch 68/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 6.0239 - gender_output_loss: 0.3244 - image_quality_output_loss: 0.8137 - age_output_loss: 1.2682 - weight_output_loss: 0.8613 - bag_output_loss: 0.7377 - footwear_output_loss: 0.7280 - pose_output_loss: 0.4559 - emotion_output_loss: 0.8348 - gender_output_acc: 0.8588 - image_quality_output_acc: 0.6114 - age_output_acc: 0.4469 - weight_output_acc: 0.6524 - bag_output_acc: 0.6831 - footwear_output_acc: 0.6755 - pose_output_acc: 0.8138 - emotion_output_acc: 0.7121 - val_loss: 6.5269 - val_gender_output_loss: 0.3381 - val_image_quality_output_loss: 0.8841 - val_age_output_loss: 1.3538 - val_weight_output_loss: 0.9706 - val_bag_output_loss: 0.8196 - val_footwear_output_loss: 0.8262 - val_pose_output_loss: 0.4738 - val_emotion_output_loss: 0.8608 - val_gender_output_acc: 0.8519 - val_image_quality_output_acc: 0.5823 - val_age_output_acc: 0.4067 - val_weight_output_acc: 0.6205 - val_bag_output_acc: 0.6414 - val_footwear_output_acc: 0.6275 - val_pose_output_acc: 0.8150 - val_emotion_output_acc: 0.7128\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 6.44670\n",
            "Epoch 69/100\n",
            "360/360 [==============================] - 61s 170ms/step - loss: 5.9524 - gender_output_loss: 0.3113 - image_quality_output_loss: 0.8086 - age_output_loss: 1.2645 - weight_output_loss: 0.8539 - bag_output_loss: 0.7236 - footwear_output_loss: 0.7139 - pose_output_loss: 0.4431 - emotion_output_loss: 0.8335 - gender_output_acc: 0.8628 - image_quality_output_acc: 0.6221 - age_output_acc: 0.4497 - weight_output_acc: 0.6575 - bag_output_acc: 0.6941 - footwear_output_acc: 0.6868 - pose_output_acc: 0.8236 - emotion_output_acc: 0.7113 - val_loss: 6.5784 - val_gender_output_loss: 0.3296 - val_image_quality_output_loss: 0.8871 - val_age_output_loss: 1.3193 - val_weight_output_loss: 0.9756 - val_bag_output_loss: 0.8209 - val_footwear_output_loss: 0.8300 - val_pose_output_loss: 0.5466 - val_emotion_output_loss: 0.8691 - val_gender_output_acc: 0.8609 - val_image_quality_output_acc: 0.5913 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.6562 - val_footwear_output_acc: 0.6285 - val_pose_output_acc: 0.8031 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 6.44670\n",
            "Epoch 70/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 5.9863 - gender_output_loss: 0.3263 - image_quality_output_loss: 0.8114 - age_output_loss: 1.2675 - weight_output_loss: 0.8556 - bag_output_loss: 0.7241 - footwear_output_loss: 0.7221 - pose_output_loss: 0.4489 - emotion_output_loss: 0.8305 - gender_output_acc: 0.8560 - image_quality_output_acc: 0.6178 - age_output_acc: 0.4490 - weight_output_acc: 0.6543 - bag_output_acc: 0.6922 - footwear_output_acc: 0.6792 - pose_output_acc: 0.8213 - emotion_output_acc: 0.7130 - val_loss: 6.5267 - val_gender_output_loss: 0.3457 - val_image_quality_output_loss: 0.8733 - val_age_output_loss: 1.3307 - val_weight_output_loss: 0.9750 - val_bag_output_loss: 0.8325 - val_footwear_output_loss: 0.8250 - val_pose_output_loss: 0.4750 - val_emotion_output_loss: 0.8694 - val_gender_output_acc: 0.8465 - val_image_quality_output_acc: 0.5729 - val_age_output_acc: 0.4072 - val_weight_output_acc: 0.6255 - val_bag_output_acc: 0.6508 - val_footwear_output_acc: 0.6290 - val_pose_output_acc: 0.8214 - val_emotion_output_acc: 0.7108\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 6.44670\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 6.44670\n",
            "Epoch 70/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 5.9616 - gender_output_loss: 0.3173 - image_quality_output_loss: 0.8076 - age_output_loss: 1.2587 - weight_output_loss: 0.8492 - bag_output_loss: 0.7262 - footwear_output_loss: 0.7270 - pose_output_loss: 0.4470 - emotion_output_loss: 0.8285 - gender_output_acc: 0.8610 - image_quality_output_acc: 0.6216 - age_output_acc: 0.4528 - weight_output_acc: 0.6570 - bag_output_acc: 0.6922 - footwear_output_acc: 0.6743 - pose_output_acc: 0.8213 - emotion_output_acc: 0.7143 - val_loss: 6.6031 - val_gender_output_loss: 0.3455 - val_image_quality_output_loss: 0.9144 - val_age_output_loss: 1.3603 - val_weight_output_loss: 0.9733 - val_bag_output_loss: 0.8213 - val_footwear_output_loss: 0.8518 - val_pose_output_loss: 0.4628 - val_emotion_output_loss: 0.8736 - val_gender_output_acc: 0.8475 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6553 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.8214 - val_emotion_output_acc: 0.7093\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 6.44670\n",
            "Epoch 72/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 5.9305 - gender_output_loss: 0.3129 - image_quality_output_loss: 0.8089 - age_output_loss: 1.2607 - weight_output_loss: 0.8500 - bag_output_loss: 0.7193 - footwear_output_loss: 0.7088 - pose_output_loss: 0.4373 - emotion_output_loss: 0.8326 - gender_output_acc: 0.8635 - image_quality_output_acc: 0.6182 - age_output_acc: 0.4501 - weight_output_acc: 0.6574 - bag_output_acc: 0.6927 - footwear_output_acc: 0.6831 - pose_output_acc: 0.8241 - emotion_output_acc: 0.7133 - val_loss: 6.6797 - val_gender_output_loss: 0.3565 - val_image_quality_output_loss: 0.9432 - val_age_output_loss: 1.3427 - val_weight_output_loss: 0.9817 - val_bag_output_loss: 0.8126 - val_footwear_output_loss: 0.8614 - val_pose_output_loss: 0.4833 - val_emotion_output_loss: 0.8983 - val_gender_output_acc: 0.8410 - val_image_quality_output_acc: 0.5397 - val_age_output_acc: 0.4122 - val_weight_output_acc: 0.6176 - val_bag_output_acc: 0.6597 - val_footwear_output_acc: 0.6270 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 6.44670\n",
            "Epoch 73/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 5.9106 - gender_output_loss: 0.3044 - image_quality_output_loss: 0.8046 - age_output_loss: 1.2596 - weight_output_loss: 0.8407 - bag_output_loss: 0.7182 - footwear_output_loss: 0.7105 - pose_output_loss: 0.4453 - emotion_output_loss: 0.8273 - gender_output_acc: 0.8668 - image_quality_output_acc: 0.6227 - age_output_acc: 0.4523 - weight_output_acc: 0.6602 - bag_output_acc: 0.6983 - footwear_output_acc: 0.6833 - pose_output_acc: 0.8181 - emotion_output_acc: 0.7122 - val_loss: 6.5337 - val_gender_output_loss: 0.3466 - val_image_quality_output_loss: 0.8749 - val_age_output_loss: 1.3237 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.8267 - val_footwear_output_loss: 0.8270 - val_pose_output_loss: 0.4922 - val_emotion_output_loss: 0.8660 - val_gender_output_acc: 0.8507 - val_image_quality_output_acc: 0.5744 - val_age_output_acc: 0.4018 - val_weight_output_acc: 0.6186 - val_bag_output_acc: 0.6508 - val_footwear_output_acc: 0.6354 - val_pose_output_acc: 0.8090 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 6.44670\n",
            "Epoch 74/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 5.9087 - gender_output_loss: 0.3131 - image_quality_output_loss: 0.8070 - age_output_loss: 1.2524 - weight_output_loss: 0.8425 - bag_output_loss: 0.7129 - footwear_output_loss: 0.7147 - pose_output_loss: 0.4402 - emotion_output_loss: 0.8260 - gender_output_acc: 0.8610 - image_quality_output_acc: 0.6203 - age_output_acc: 0.4577 - weight_output_acc: 0.6633 - bag_output_acc: 0.6969 - footwear_output_acc: 0.6842 - pose_output_acc: 0.8270 - emotion_output_acc: 0.7152 - val_loss: 6.6251 - val_gender_output_loss: 0.3744 - val_image_quality_output_loss: 0.8816 - val_age_output_loss: 1.3271 - val_weight_output_loss: 1.0008 - val_bag_output_loss: 0.8474 - val_footwear_output_loss: 0.8619 - val_pose_output_loss: 0.4699 - val_emotion_output_loss: 0.8620 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5858 - val_age_output_acc: 0.4058 - val_weight_output_acc: 0.5992 - val_bag_output_acc: 0.6379 - val_footwear_output_acc: 0.6295 - val_pose_output_acc: 0.8323 - val_emotion_output_acc: 0.7004\n",
            "\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 6.44670\n",
            "Epoch 75/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 5.8810 - gender_output_loss: 0.3103 - image_quality_output_loss: 0.8043 - age_output_loss: 1.2545 - weight_output_loss: 0.8402 - bag_output_loss: 0.7151 - footwear_output_loss: 0.7088 - pose_output_loss: 0.4262 - emotion_output_loss: 0.8214 - gender_output_acc: 0.8667 - image_quality_output_acc: 0.6249 - age_output_acc: 0.4507 - weight_output_acc: 0.6590 - bag_output_acc: 0.6924 - footwear_output_acc: 0.6857 - pose_output_acc: 0.8286 - emotion_output_acc: 0.7128 - val_loss: 6.5118 - val_gender_output_loss: 0.3273 - val_image_quality_output_loss: 0.9014 - val_age_output_loss: 1.3195 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.8020 - val_footwear_output_loss: 0.8338 - val_pose_output_loss: 0.4788 - val_emotion_output_loss: 0.8689 - val_gender_output_acc: 0.8599 - val_image_quality_output_acc: 0.5556 - val_age_output_acc: 0.4092 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.6607 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.8264 - val_emotion_output_acc: 0.7148\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 6.44670\n",
            "Epoch 76/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 5.8554 - gender_output_loss: 0.3062 - image_quality_output_loss: 0.7994 - age_output_loss: 1.2456 - weight_output_loss: 0.8378 - bag_output_loss: 0.7060 - footwear_output_loss: 0.7023 - pose_output_loss: 0.4383 - emotion_output_loss: 0.8196 - gender_output_acc: 0.8641 - image_quality_output_acc: 0.6190 - age_output_acc: 0.4556 - weight_output_acc: 0.6584 - bag_output_acc: 0.6995 - footwear_output_acc: 0.6879 - pose_output_acc: 0.8234 - emotion_output_acc: 0.7130 - val_loss: 6.6732 - val_gender_output_loss: 0.3402 - val_image_quality_output_loss: 0.9124 - val_age_output_loss: 1.3570 - val_weight_output_loss: 0.9911 - val_bag_output_loss: 0.8597 - val_footwear_output_loss: 0.8680 - val_pose_output_loss: 0.4709 - val_emotion_output_loss: 0.8739 - val_gender_output_acc: 0.8482 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.3909 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.6389 - val_footwear_output_acc: 0.6171 - val_pose_output_acc: 0.8279 - val_emotion_output_acc: 0.7054\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 6.44670\n",
            "Epoch 77/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 5.8555 - gender_output_loss: 0.3028 - image_quality_output_loss: 0.8010 - age_output_loss: 1.2465 - weight_output_loss: 0.8360 - bag_output_loss: 0.7014 - footwear_output_loss: 0.7117 - pose_output_loss: 0.4350 - emotion_output_loss: 0.8210 - gender_output_acc: 0.8660 - image_quality_output_acc: 0.6212 - age_output_acc: 0.4576 - weight_output_acc: 0.6643 - bag_output_acc: 0.7056 - footwear_output_acc: 0.6853 - pose_output_acc: 0.8312 - emotion_output_acc: 0.7155 - val_loss: 6.8426 - val_gender_output_loss: 0.4991 - val_image_quality_output_loss: 0.8824 - val_age_output_loss: 1.3510 - val_weight_output_loss: 0.9949 - val_bag_output_loss: 0.8622 - val_footwear_output_loss: 0.8762 - val_pose_output_loss: 0.4893 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.8053 - val_image_quality_output_acc: 0.5784 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6081 - val_bag_output_acc: 0.6483 - val_footwear_output_acc: 0.6190 - val_pose_output_acc: 0.8254 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 6.44670\n",
            "Epoch 78/100\n",
            "360/360 [==============================] - 63s 174ms/step - loss: 5.8374 - gender_output_loss: 0.3050 - image_quality_output_loss: 0.7973 - age_output_loss: 1.2418 - weight_output_loss: 0.8360 - bag_output_loss: 0.7092 - footwear_output_loss: 0.7030 - pose_output_loss: 0.4279 - emotion_output_loss: 0.8173 - gender_output_acc: 0.8641 - image_quality_output_acc: 0.6232 - age_output_acc: 0.4593 - weight_output_acc: 0.6646 - bag_output_acc: 0.7030 - footwear_output_acc: 0.6906 - pose_output_acc: 0.8297 - emotion_output_acc: 0.7138 - val_loss: 6.7974 - val_gender_output_loss: 0.4482 - val_image_quality_output_loss: 0.8877 - val_age_output_loss: 1.3780 - val_weight_output_loss: 0.9783 - val_bag_output_loss: 0.8660 - val_footwear_output_loss: 0.8872 - val_pose_output_loss: 0.4835 - val_emotion_output_loss: 0.8686 - val_gender_output_acc: 0.8013 - val_image_quality_output_acc: 0.5898 - val_age_output_acc: 0.4048 - val_weight_output_acc: 0.6047 - val_bag_output_acc: 0.6314 - val_footwear_output_acc: 0.6086 - val_pose_output_acc: 0.8120 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 6.44670\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 6.44670\n",
            "Epoch 78/100\n",
            "360/360 [==============================] - 63s 174ms/step - loss: 5.8049 - gender_output_loss: 0.2986 - image_quality_output_loss: 0.7940 - age_output_loss: 1.2391 - weight_output_loss: 0.8235 - bag_output_loss: 0.7010 - footwear_output_loss: 0.7040 - pose_output_loss: 0.4260 - emotion_output_loss: 0.8186 - gender_output_acc: 0.8681 - image_quality_output_acc: 0.6231 - age_output_acc: 0.4604 - weight_output_acc: 0.6671 - bag_output_acc: 0.7022 - footwear_output_acc: 0.6883 - pose_output_acc: 0.8323 - emotion_output_acc: 0.7155 - val_loss: 6.6553 - val_gender_output_loss: 0.3468 - val_image_quality_output_loss: 0.8955 - val_age_output_loss: 1.3491 - val_weight_output_loss: 0.9892 - val_bag_output_loss: 0.8773 - val_footwear_output_loss: 0.8412 - val_pose_output_loss: 0.4783 - val_emotion_output_loss: 0.8778 - val_gender_output_acc: 0.8519 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6111 - val_bag_output_acc: 0.6453 - val_footwear_output_acc: 0.6186 - val_pose_output_acc: 0.8264 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 6.44670\n",
            "Epoch 80/100\n",
            "360/360 [==============================] - 63s 175ms/step - loss: 5.8046 - gender_output_loss: 0.3024 - image_quality_output_loss: 0.8031 - age_output_loss: 1.2334 - weight_output_loss: 0.8268 - bag_output_loss: 0.7019 - footwear_output_loss: 0.7022 - pose_output_loss: 0.4186 - emotion_output_loss: 0.8162 - gender_output_acc: 0.8688 - image_quality_output_acc: 0.6224 - age_output_acc: 0.4683 - weight_output_acc: 0.6652 - bag_output_acc: 0.7016 - footwear_output_acc: 0.6850 - pose_output_acc: 0.8362 - emotion_output_acc: 0.7153 - val_loss: 6.9745 - val_gender_output_loss: 0.3911 - val_image_quality_output_loss: 0.8817 - val_age_output_loss: 1.4334 - val_weight_output_loss: 1.0729 - val_bag_output_loss: 0.9253 - val_footwear_output_loss: 0.9120 - val_pose_output_loss: 0.4666 - val_emotion_output_loss: 0.8915 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.5704 - val_age_output_acc: 0.3646 - val_weight_output_acc: 0.5486 - val_bag_output_acc: 0.6200 - val_footwear_output_acc: 0.6071 - val_pose_output_acc: 0.8289 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 6.44670\n",
            "Epoch 81/100\n",
            "360/360 [==============================] - 63s 175ms/step - loss: 5.7799 - gender_output_loss: 0.3004 - image_quality_output_loss: 0.7937 - age_output_loss: 1.2312 - weight_output_loss: 0.8299 - bag_output_loss: 0.6960 - footwear_output_loss: 0.6925 - pose_output_loss: 0.4196 - emotion_output_loss: 0.8165 - gender_output_acc: 0.8677 - image_quality_output_acc: 0.6280 - age_output_acc: 0.4669 - weight_output_acc: 0.6656 - bag_output_acc: 0.7047 - footwear_output_acc: 0.6947 - pose_output_acc: 0.8284 - emotion_output_acc: 0.7136 - val_loss: 6.6928 - val_gender_output_loss: 0.3502 - val_image_quality_output_loss: 0.8745 - val_age_output_loss: 1.3497 - val_weight_output_loss: 1.0176 - val_bag_output_loss: 0.8262 - val_footwear_output_loss: 0.8373 - val_pose_output_loss: 0.5742 - val_emotion_output_loss: 0.8632 - val_gender_output_acc: 0.8485 - val_image_quality_output_acc: 0.5685 - val_age_output_acc: 0.4013 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6473 - val_footwear_output_acc: 0.6319 - val_pose_output_acc: 0.8011 - val_emotion_output_acc: 0.7068\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 6.44670\n",
            "Epoch 82/100\n",
            "360/360 [==============================] - 63s 175ms/step - loss: 5.7320 - gender_output_loss: 0.2938 - image_quality_output_loss: 0.7919 - age_output_loss: 1.2263 - weight_output_loss: 0.8158 - bag_output_loss: 0.6857 - footwear_output_loss: 0.6883 - pose_output_loss: 0.4157 - emotion_output_loss: 0.8145 - gender_output_acc: 0.8727 - image_quality_output_acc: 0.6296 - age_output_acc: 0.4606 - weight_output_acc: 0.6687 - bag_output_acc: 0.7093 - footwear_output_acc: 0.6950 - pose_output_acc: 0.8319 - emotion_output_acc: 0.7132 - val_loss: 6.5858 - val_gender_output_loss: 0.3461 - val_image_quality_output_loss: 0.8766 - val_age_output_loss: 1.3280 - val_weight_output_loss: 0.9967 - val_bag_output_loss: 0.8209 - val_footwear_output_loss: 0.8587 - val_pose_output_loss: 0.4757 - val_emotion_output_loss: 0.8832 - val_gender_output_acc: 0.8477 - val_image_quality_output_acc: 0.5928 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.5942 - val_bag_output_acc: 0.6553 - val_footwear_output_acc: 0.6181 - val_pose_output_acc: 0.8304 - val_emotion_output_acc: 0.7123\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 6.44670\n",
            "Epoch 83/100\n",
            "360/360 [==============================] - 63s 175ms/step - loss: 5.7311 - gender_output_loss: 0.2940 - image_quality_output_loss: 0.7898 - age_output_loss: 1.2194 - weight_output_loss: 0.8213 - bag_output_loss: 0.6919 - footwear_output_loss: 0.6955 - pose_output_loss: 0.4104 - emotion_output_loss: 0.8089 - gender_output_acc: 0.8737 - image_quality_output_acc: 0.6293 - age_output_acc: 0.4710 - weight_output_acc: 0.6747 - bag_output_acc: 0.7090 - footwear_output_acc: 0.6912 - pose_output_acc: 0.8340 - emotion_output_acc: 0.7186 - val_loss: 6.7466 - val_gender_output_loss: 0.3754 - val_image_quality_output_loss: 0.9223 - val_age_output_loss: 1.3417 - val_weight_output_loss: 1.0018 - val_bag_output_loss: 0.8756 - val_footwear_output_loss: 0.8644 - val_pose_output_loss: 0.4831 - val_emotion_output_loss: 0.8823 - val_gender_output_acc: 0.8405 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3968 - val_weight_output_acc: 0.6037 - val_bag_output_acc: 0.6513 - val_footwear_output_acc: 0.6285 - val_pose_output_acc: 0.8224 - val_emotion_output_acc: 0.7063\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 6.44670\n",
            "Epoch 84/100\n",
            "360/360 [==============================] - 63s 175ms/step - loss: 5.7186 - gender_output_loss: 0.2896 - image_quality_output_loss: 0.7880 - age_output_loss: 1.2186 - weight_output_loss: 0.8137 - bag_output_loss: 0.6884 - footwear_output_loss: 0.6914 - pose_output_loss: 0.4197 - emotion_output_loss: 0.8092 - gender_output_acc: 0.8737 - image_quality_output_acc: 0.6299 - age_output_acc: 0.4736 - weight_output_acc: 0.6711 - bag_output_acc: 0.7102 - footwear_output_acc: 0.6898 - pose_output_acc: 0.8311 - emotion_output_acc: 0.7148 - val_loss: 6.8188 - val_gender_output_loss: 0.3509 - val_image_quality_output_loss: 0.8738 - val_age_output_loss: 1.3771 - val_weight_output_loss: 1.0966 - val_bag_output_loss: 0.8570 - val_footwear_output_loss: 0.9197 - val_pose_output_loss: 0.4616 - val_emotion_output_loss: 0.8821 - val_gender_output_acc: 0.8475 - val_image_quality_output_acc: 0.5873 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.5228 - val_bag_output_acc: 0.6523 - val_footwear_output_acc: 0.5997 - val_pose_output_acc: 0.8264 - val_emotion_output_acc: 0.6910\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 6.44670\n",
            "Epoch 85/100\n",
            "360/360 [==============================] - 63s 175ms/step - loss: 5.7137 - gender_output_loss: 0.2898 - image_quality_output_loss: 0.7843 - age_output_loss: 1.2244 - weight_output_loss: 0.8218 - bag_output_loss: 0.6831 - footwear_output_loss: 0.6873 - pose_output_loss: 0.4158 - emotion_output_loss: 0.8073 - gender_output_acc: 0.8753 - image_quality_output_acc: 0.6355 - age_output_acc: 0.4672 - weight_output_acc: 0.6668 - bag_output_acc: 0.7122 - footwear_output_acc: 0.6967 - pose_output_acc: 0.8363 - emotion_output_acc: 0.7160 - val_loss: 6.7288 - val_gender_output_loss: 0.3453 - val_image_quality_output_loss: 0.8943 - val_age_output_loss: 1.3462 - val_weight_output_loss: 1.0382 - val_bag_output_loss: 0.8310 - val_footwear_output_loss: 0.8630 - val_pose_output_loss: 0.5194 - val_emotion_output_loss: 0.8913 - val_gender_output_acc: 0.8467 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.3978 - val_weight_output_acc: 0.5719 - val_bag_output_acc: 0.6706 - val_footwear_output_acc: 0.6230 - val_pose_output_acc: 0.8150 - val_emotion_output_acc: 0.6920\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 6.44670\n",
            "Epoch 86/100\n",
            "360/360 [==============================] - 64s 177ms/step - loss: 5.6669 - gender_output_loss: 0.2872 - image_quality_output_loss: 0.7825 - age_output_loss: 1.2126 - weight_output_loss: 0.8146 - bag_output_loss: 0.6801 - footwear_output_loss: 0.6851 - pose_output_loss: 0.3997 - emotion_output_loss: 0.8051 - gender_output_acc: 0.8738 - image_quality_output_acc: 0.6311 - age_output_acc: 0.4766 - weight_output_acc: 0.6713 - bag_output_acc: 0.7131 - footwear_output_acc: 0.6976 - pose_output_acc: 0.8370 - emotion_output_acc: 0.7177 - val_loss: 6.7130 - val_gender_output_loss: 0.3367 - val_image_quality_output_loss: 0.8807 - val_age_output_loss: 1.3712 - val_weight_output_loss: 0.9972 - val_bag_output_loss: 0.8215 - val_footwear_output_loss: 0.9225 - val_pose_output_loss: 0.5108 - val_emotion_output_loss: 0.8725 - val_gender_output_acc: 0.8589 - val_image_quality_output_acc: 0.5749 - val_age_output_acc: 0.3894 - val_weight_output_acc: 0.5972 - val_bag_output_acc: 0.6612 - val_footwear_output_acc: 0.6081 - val_pose_output_acc: 0.8046 - val_emotion_output_acc: 0.7098\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 6.44670\n",
            "Epoch 87/100\n",
            "360/360 [==============================] - 64s 177ms/step - loss: 5.6499 - gender_output_loss: 0.2821 - image_quality_output_loss: 0.7861 - age_output_loss: 1.2134 - weight_output_loss: 0.8097 - bag_output_loss: 0.6742 - footwear_output_loss: 0.6800 - pose_output_loss: 0.3992 - emotion_output_loss: 0.8053 - gender_output_acc: 0.8775 - image_quality_output_acc: 0.6293 - age_output_acc: 0.4762 - weight_output_acc: 0.6740 - bag_output_acc: 0.7148 - footwear_output_acc: 0.6969 - pose_output_acc: 0.8426 - emotion_output_acc: 0.7176 - val_loss: 6.8031 - val_gender_output_loss: 0.3392 - val_image_quality_output_loss: 0.9603 - val_age_output_loss: 1.3669 - val_weight_output_loss: 1.0144 - val_bag_output_loss: 0.8834 - val_footwear_output_loss: 0.8772 - val_pose_output_loss: 0.4786 - val_emotion_output_loss: 0.8831 - val_gender_output_acc: 0.8574 - val_image_quality_output_acc: 0.5372 - val_age_output_acc: 0.4157 - val_weight_output_acc: 0.6022 - val_bag_output_acc: 0.6473 - val_footwear_output_acc: 0.6186 - val_pose_output_acc: 0.8363 - val_emotion_output_acc: 0.7049\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 6.44670\n",
            "Epoch 88/100\n",
            "360/360 [==============================] - 64s 177ms/step - loss: 5.6416 - gender_output_loss: 0.2829 - image_quality_output_loss: 0.7824 - age_output_loss: 1.2112 - weight_output_loss: 0.8047 - bag_output_loss: 0.6830 - footwear_output_loss: 0.6741 - pose_output_loss: 0.3994 - emotion_output_loss: 0.8039 - gender_output_acc: 0.8765 - image_quality_output_acc: 0.6311 - age_output_acc: 0.4762 - weight_output_acc: 0.6771 - bag_output_acc: 0.7100 - footwear_output_acc: 0.7023 - pose_output_acc: 0.8370 - emotion_output_acc: 0.7186 - val_loss: 6.7138 - val_gender_output_loss: 0.3455 - val_image_quality_output_loss: 0.9212 - val_age_output_loss: 1.3313 - val_weight_output_loss: 0.9994 - val_bag_output_loss: 0.8441 - val_footwear_output_loss: 0.9096 - val_pose_output_loss: 0.4786 - val_emotion_output_loss: 0.8840 - val_gender_output_acc: 0.8554 - val_image_quality_output_acc: 0.5784 - val_age_output_acc: 0.4127 - val_weight_output_acc: 0.6017 - val_bag_output_acc: 0.6672 - val_footwear_output_acc: 0.6101 - val_pose_output_acc: 0.8279 - val_emotion_output_acc: 0.6979\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 6.44670\n",
            "Epoch 89/100\n",
            "360/360 [==============================] - 64s 177ms/step - loss: 5.6315 - gender_output_loss: 0.2871 - image_quality_output_loss: 0.7821 - age_output_loss: 1.2087 - weight_output_loss: 0.8061 - bag_output_loss: 0.6778 - footwear_output_loss: 0.6810 - pose_output_loss: 0.3876 - emotion_output_loss: 0.8010 - gender_output_acc: 0.8730 - image_quality_output_acc: 0.6335 - age_output_acc: 0.4821 - weight_output_acc: 0.6714 - bag_output_acc: 0.7140 - footwear_output_acc: 0.6990 - pose_output_acc: 0.8467 - emotion_output_acc: 0.7161 - val_loss: 6.8296 - val_gender_output_loss: 0.3711 - val_image_quality_output_loss: 0.9104 - val_age_output_loss: 1.4125 - val_weight_output_loss: 1.0316 - val_bag_output_loss: 0.8528 - val_footwear_output_loss: 0.8768 - val_pose_output_loss: 0.4987 - val_emotion_output_loss: 0.8759 - val_gender_output_acc: 0.8477 - val_image_quality_output_acc: 0.5739 - val_age_output_acc: 0.3661 - val_weight_output_acc: 0.5843 - val_bag_output_acc: 0.6513 - val_footwear_output_acc: 0.6141 - val_pose_output_acc: 0.8219 - val_emotion_output_acc: 0.7103\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 6.44670\n",
            "Epoch 90/100\n",
            "360/360 [==============================] - 63s 176ms/step - loss: 5.5950 - gender_output_loss: 0.2724 - image_quality_output_loss: 0.7812 - age_output_loss: 1.2038 - weight_output_loss: 0.7997 - bag_output_loss: 0.6686 - footwear_output_loss: 0.6754 - pose_output_loss: 0.3907 - emotion_output_loss: 0.8031 - gender_output_acc: 0.8829 - image_quality_output_acc: 0.6361 - age_output_acc: 0.4803 - weight_output_acc: 0.6795 - bag_output_acc: 0.7160 - footwear_output_acc: 0.6999 - pose_output_acc: 0.8452 - emotion_output_acc: 0.7181 - val_loss: 6.9706 - val_gender_output_loss: 0.3874 - val_image_quality_output_loss: 0.9088 - val_age_output_loss: 1.4823 - val_weight_output_loss: 1.0630 - val_bag_output_loss: 0.8592 - val_footwear_output_loss: 0.8754 - val_pose_output_loss: 0.5003 - val_emotion_output_loss: 0.8942 - val_gender_output_acc: 0.8487 - val_image_quality_output_acc: 0.5665 - val_age_output_acc: 0.3433 - val_weight_output_acc: 0.5541 - val_bag_output_acc: 0.6562 - val_footwear_output_acc: 0.6265 - val_pose_output_acc: 0.8219 - val_emotion_output_acc: 0.6825\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 6.44670\n",
            "Epoch 91/100\n",
            "360/360 [==============================] - 63s 176ms/step - loss: 5.5686 - gender_output_loss: 0.2781 - image_quality_output_loss: 0.7783 - age_output_loss: 1.1959 - weight_output_loss: 0.7990 - bag_output_loss: 0.6693 - footwear_output_loss: 0.6674 - pose_output_loss: 0.3822 - emotion_output_loss: 0.7984 - gender_output_acc: 0.8825 - image_quality_output_acc: 0.6367 - age_output_acc: 0.4789 - weight_output_acc: 0.6776 - bag_output_acc: 0.7173 - footwear_output_acc: 0.7068 - pose_output_acc: 0.8470 - emotion_output_acc: 0.7197 - val_loss: 6.8049 - val_gender_output_loss: 0.3442 - val_image_quality_output_loss: 0.8855 - val_age_output_loss: 1.3567 - val_weight_output_loss: 1.0212 - val_bag_output_loss: 0.9400 - val_footwear_output_loss: 0.8691 - val_pose_output_loss: 0.5089 - val_emotion_output_loss: 0.8794 - val_gender_output_acc: 0.8539 - val_image_quality_output_acc: 0.5967 - val_age_output_acc: 0.4033 - val_weight_output_acc: 0.6161 - val_bag_output_acc: 0.6314 - val_footwear_output_acc: 0.6215 - val_pose_output_acc: 0.8090 - val_emotion_output_acc: 0.6895\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 6.44670\n",
            "Epoch 92/100\n",
            "360/360 [==============================] - 63s 174ms/step - loss: 5.5668 - gender_output_loss: 0.2765 - image_quality_output_loss: 0.7778 - age_output_loss: 1.1923 - weight_output_loss: 0.7903 - bag_output_loss: 0.6685 - footwear_output_loss: 0.6702 - pose_output_loss: 0.3909 - emotion_output_loss: 0.8004 - gender_output_acc: 0.8763 - image_quality_output_acc: 0.6317 - age_output_acc: 0.4862 - weight_output_acc: 0.6781 - bag_output_acc: 0.7217 - footwear_output_acc: 0.7023 - pose_output_acc: 0.8419 - emotion_output_acc: 0.7185 - val_loss: 6.7845 - val_gender_output_loss: 0.3599 - val_image_quality_output_loss: 0.8916 - val_age_output_loss: 1.3817 - val_weight_output_loss: 1.0199 - val_bag_output_loss: 0.8552 - val_footwear_output_loss: 0.8874 - val_pose_output_loss: 0.4993 - val_emotion_output_loss: 0.8894 - val_gender_output_acc: 0.8512 - val_image_quality_output_acc: 0.5714 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.5967 - val_bag_output_acc: 0.6587 - val_footwear_output_acc: 0.6230 - val_pose_output_acc: 0.8234 - val_emotion_output_acc: 0.6949\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 6.44670\n",
            "Epoch 93/100\n",
            "360/360 [==============================] - 63s 174ms/step - loss: 5.5541 - gender_output_loss: 0.2745 - image_quality_output_loss: 0.7704 - age_output_loss: 1.1999 - weight_output_loss: 0.7935 - bag_output_loss: 0.6688 - footwear_output_loss: 0.6675 - pose_output_loss: 0.3877 - emotion_output_loss: 0.7919 - gender_output_acc: 0.8813 - image_quality_output_acc: 0.6393 - age_output_acc: 0.4799 - weight_output_acc: 0.6806 - bag_output_acc: 0.7181 - footwear_output_acc: 0.7006 - pose_output_acc: 0.8452 - emotion_output_acc: 0.7174 - val_loss: 7.2067 - val_gender_output_loss: 0.4608 - val_image_quality_output_loss: 0.9773 - val_age_output_loss: 1.3996 - val_weight_output_loss: 1.0416 - val_bag_output_loss: 0.9046 - val_footwear_output_loss: 0.9510 - val_pose_output_loss: 0.5455 - val_emotion_output_loss: 0.9263 - val_gender_output_acc: 0.8309 - val_image_quality_output_acc: 0.5466 - val_age_output_acc: 0.3998 - val_weight_output_acc: 0.5997 - val_bag_output_acc: 0.6503 - val_footwear_output_acc: 0.6240 - val_pose_output_acc: 0.8229 - val_emotion_output_acc: 0.7083\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 6.44670\n",
            "Epoch 94/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 5.5339 - gender_output_loss: 0.2732 - image_quality_output_loss: 0.7712 - age_output_loss: 1.1918 - weight_output_loss: 0.7919 - bag_output_loss: 0.6643 - footwear_output_loss: 0.6623 - pose_output_loss: 0.3870 - emotion_output_loss: 0.7923 - gender_output_acc: 0.8818 - image_quality_output_acc: 0.6363 - age_output_acc: 0.4819 - weight_output_acc: 0.6797 - bag_output_acc: 0.7233 - footwear_output_acc: 0.7037 - pose_output_acc: 0.8484 - emotion_output_acc: 0.7186 - val_loss: 6.8825 - val_gender_output_loss: 0.3794 - val_image_quality_output_loss: 0.9217 - val_age_output_loss: 1.3787 - val_weight_output_loss: 1.0325 - val_bag_output_loss: 0.8809 - val_footwear_output_loss: 0.9092 - val_pose_output_loss: 0.4999 - val_emotion_output_loss: 0.8802 - val_gender_output_acc: 0.8455 - val_image_quality_output_acc: 0.5650 - val_age_output_acc: 0.3819 - val_weight_output_acc: 0.5848 - val_bag_output_acc: 0.6374 - val_footwear_output_acc: 0.6116 - val_pose_output_acc: 0.8229 - val_emotion_output_acc: 0.6989\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 6.44670\n",
            "Epoch 95/100\n",
            "360/360 [==============================] - 62s 171ms/step - loss: 5.4937 - gender_output_loss: 0.2708 - image_quality_output_loss: 0.7651 - age_output_loss: 1.1874 - weight_output_loss: 0.7876 - bag_output_loss: 0.6472 - footwear_output_loss: 0.6622 - pose_output_loss: 0.3834 - emotion_output_loss: 0.7898 - gender_output_acc: 0.8791 - image_quality_output_acc: 0.6431 - age_output_acc: 0.4827 - weight_output_acc: 0.6796 - bag_output_acc: 0.7271 - footwear_output_acc: 0.7073 - pose_output_acc: 0.8467 - emotion_output_acc: 0.7176 - val_loss: 6.7736 - val_gender_output_loss: 0.3406 - val_image_quality_output_loss: 0.9137 - val_age_output_loss: 1.3794 - val_weight_output_loss: 1.0287 - val_bag_output_loss: 0.8501 - val_footwear_output_loss: 0.8740 - val_pose_output_loss: 0.5013 - val_emotion_output_loss: 0.8858 - val_gender_output_acc: 0.8504 - val_image_quality_output_acc: 0.5655 - val_age_output_acc: 0.4028 - val_weight_output_acc: 0.5942 - val_bag_output_acc: 0.6602 - val_footwear_output_acc: 0.6200 - val_pose_output_acc: 0.8145 - val_emotion_output_acc: 0.6880\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 6.44670\n",
            "Epoch 96/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 5.5158 - gender_output_loss: 0.2739 - image_quality_output_loss: 0.7692 - age_output_loss: 1.1808 - weight_output_loss: 0.7870 - bag_output_loss: 0.6617 - footwear_output_loss: 0.6657 - pose_output_loss: 0.3828 - emotion_output_loss: 0.7947 - gender_output_acc: 0.8786 - image_quality_output_acc: 0.6431 - age_output_acc: 0.4915 - weight_output_acc: 0.6819 - bag_output_acc: 0.7233 - footwear_output_acc: 0.7039 - pose_output_acc: 0.8430 - emotion_output_acc: 0.7202 - val_loss: 6.9729 - val_gender_output_loss: 0.3692 - val_image_quality_output_loss: 0.9218 - val_age_output_loss: 1.3699 - val_weight_output_loss: 1.0355 - val_bag_output_loss: 0.9380 - val_footwear_output_loss: 0.9516 - val_pose_output_loss: 0.5004 - val_emotion_output_loss: 0.8866 - val_gender_output_acc: 0.8383 - val_image_quality_output_acc: 0.5531 - val_age_output_acc: 0.3814 - val_weight_output_acc: 0.5957 - val_bag_output_acc: 0.6414 - val_footwear_output_acc: 0.5957 - val_pose_output_acc: 0.8175 - val_emotion_output_acc: 0.7024\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 6.44670\n",
            "Epoch 97/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 5.4774 - gender_output_loss: 0.2748 - image_quality_output_loss: 0.7653 - age_output_loss: 1.1791 - weight_output_loss: 0.7739 - bag_output_loss: 0.6612 - footwear_output_loss: 0.6539 - pose_output_loss: 0.3794 - emotion_output_loss: 0.7898 - gender_output_acc: 0.8789 - image_quality_output_acc: 0.6410 - age_output_acc: 0.4943 - weight_output_acc: 0.6867 - bag_output_acc: 0.7217 - footwear_output_acc: 0.7120 - pose_output_acc: 0.8456 - emotion_output_acc: 0.7180 - val_loss: 6.8963 - val_gender_output_loss: 0.3523 - val_image_quality_output_loss: 0.9117 - val_age_output_loss: 1.3861 - val_weight_output_loss: 1.0619 - val_bag_output_loss: 0.8403 - val_footwear_output_loss: 0.9322 - val_pose_output_loss: 0.5184 - val_emotion_output_loss: 0.8934 - val_gender_output_acc: 0.8405 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.4117 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.6592 - val_footwear_output_acc: 0.6002 - val_pose_output_acc: 0.8046 - val_emotion_output_acc: 0.7039\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 6.44670\n",
            "Epoch 98/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 5.4342 - gender_output_loss: 0.2675 - image_quality_output_loss: 0.7626 - age_output_loss: 1.1747 - weight_output_loss: 0.7748 - bag_output_loss: 0.6442 - footwear_output_loss: 0.6565 - pose_output_loss: 0.3692 - emotion_output_loss: 0.7848 - gender_output_acc: 0.8822 - image_quality_output_acc: 0.6356 - age_output_acc: 0.4944 - weight_output_acc: 0.6832 - bag_output_acc: 0.7299 - footwear_output_acc: 0.7129 - pose_output_acc: 0.8554 - emotion_output_acc: 0.7208 - val_loss: 6.9556 - val_gender_output_loss: 0.4387 - val_image_quality_output_loss: 0.9094 - val_age_output_loss: 1.3787 - val_weight_output_loss: 1.0583 - val_bag_output_loss: 0.8557 - val_footwear_output_loss: 0.9307 - val_pose_output_loss: 0.5027 - val_emotion_output_loss: 0.8813 - val_gender_output_acc: 0.8279 - val_image_quality_output_acc: 0.5799 - val_age_output_acc: 0.4092 - val_weight_output_acc: 0.6136 - val_bag_output_acc: 0.6592 - val_footwear_output_acc: 0.6190 - val_pose_output_acc: 0.8224 - val_emotion_output_acc: 0.7073\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 6.44670\n",
            "Epoch 99/100\n",
            "360/360 [==============================] - 62s 172ms/step - loss: 5.4326 - gender_output_loss: 0.2690 - image_quality_output_loss: 0.7662 - age_output_loss: 1.1690 - weight_output_loss: 0.7751 - bag_output_loss: 0.6463 - footwear_output_loss: 0.6542 - pose_output_loss: 0.3683 - emotion_output_loss: 0.7845 - gender_output_acc: 0.8843 - image_quality_output_acc: 0.6416 - age_output_acc: 0.4962 - weight_output_acc: 0.6852 - bag_output_acc: 0.7277 - footwear_output_acc: 0.7128 - pose_output_acc: 0.8550 - emotion_output_acc: 0.7194 - val_loss: 6.9101 - val_gender_output_loss: 0.3614 - val_image_quality_output_loss: 0.9383 - val_age_output_loss: 1.4213 - val_weight_output_loss: 1.0354 - val_bag_output_loss: 0.8422 - val_footwear_output_loss: 0.8972 - val_pose_output_loss: 0.5234 - val_emotion_output_loss: 0.8908 - val_gender_output_acc: 0.8485 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3810 - val_weight_output_acc: 0.5918 - val_bag_output_acc: 0.6548 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.7986 - val_emotion_output_acc: 0.7044\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 6.44670\n",
            "Epoch 100/100\n",
            "360/360 [==============================] - 62s 173ms/step - loss: 5.4364 - gender_output_loss: 0.2608 - image_quality_output_loss: 0.7631 - age_output_loss: 1.1719 - weight_output_loss: 0.7715 - bag_output_loss: 0.6561 - footwear_output_loss: 0.6465 - pose_output_loss: 0.3816 - emotion_output_loss: 0.7848 - gender_output_acc: 0.8872 - image_quality_output_acc: 0.6429 - age_output_acc: 0.4930 - weight_output_acc: 0.6838 - bag_output_acc: 0.7234 - footwear_output_acc: 0.7150 - pose_output_acc: 0.8439 - emotion_output_acc: 0.7200 - val_loss: 7.0639 - val_gender_output_loss: 0.4114 - val_image_quality_output_loss: 0.9121 - val_age_output_loss: 1.3768 - val_weight_output_loss: 1.0356 - val_bag_output_loss: 0.9242 - val_footwear_output_loss: 0.9886 - val_pose_output_loss: 0.5212 - val_emotion_output_loss: 0.8939 - val_gender_output_acc: 0.8353 - val_image_quality_output_acc: 0.5685 - val_age_output_acc: 0.3884 - val_weight_output_acc: 0.6027 - val_bag_output_acc: 0.6399 - val_footwear_output_acc: 0.6042 - val_pose_output_acc: 0.8165 - val_emotion_output_acc: 0.6930\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 6.44670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0d32a51dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6bcb7f48-dced-4828-9d1d-263d1394d801"
      },
      "source": [
        "res = model.evaluate_generator(valid_gen,verbose=1)\n",
        "dict(zip(model.metrics_names,res))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 4s 71ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.38839285714285715,\n",
              " 'age_output_loss': 1.3767806178047544,\n",
              " 'bag_output_acc': 0.6398809523809523,\n",
              " 'bag_output_loss': 0.9241989586088393,\n",
              " 'emotion_output_acc': 0.6929563492063492,\n",
              " 'emotion_output_loss': 0.8938961653482347,\n",
              " 'footwear_output_acc': 0.6041666666666666,\n",
              " 'footwear_output_loss': 0.9886386621566046,\n",
              " 'gender_output_acc': 0.8353174603174603,\n",
              " 'gender_output_loss': 0.4114305093174889,\n",
              " 'image_quality_output_acc': 0.5684523809523809,\n",
              " 'image_quality_output_loss': 0.912076921690078,\n",
              " 'loss': 7.0638669861687555,\n",
              " 'pose_output_acc': 0.816468253968254,\n",
              " 'pose_output_loss': 0.5212062987543288,\n",
              " 'weight_output_acc': 0.6026785714285714,\n",
              " 'weight_output_loss': 1.0356388593476915}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMcGHHcoOGVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}